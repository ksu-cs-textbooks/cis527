




	
	
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
		

	
	
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		
		

	
	
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		

	
	
		

	
	
		
		
		
		
		
		

	
	
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		

	
	
		
		
var relearn_search_index = [
  {
    "content": "Course Introductory Material for CIS 527 Online\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/0-introduction/index.html"
  },
  {
    "content": " Ed Discussion This video was recorded before I decided to switch to Ed Discussion instead of using Discord. All discussions of “Discord” can be applied to “Ed Discussion” instead. Sorry for the confusion! - Russ\nYouTube Video Resources Slides Syllabus Video Script Welcome to CIS 527 - Enterprise System Administration and CC 510 - Computer Systems Administration. Even though these are two different courses in the catalog, they teach the same content and will use the same Canvas course. So, anywhere you see CIS 527 in this course, you can also mentally substitute CC 510 in its place.\nMy name is Russell Feldhausen, and I’ll be your instructor for this course. My contact information is shown here, and is also listed on the syllabus, and on the home page of the course on K-State Canvas. My email address is russfeld@ksu.edu, and it is the official method of communication for matters outside of this course, since it allows me to have a record of our conversations and respond when I’m available. However, I’ll also be available via the K-State CS Discord server, so you can easily chat with me there. There is a channel already created for this course that I encourage you to make use of throughout this course.\nWe also have one teaching assistant this semester - Matt Schwartz. He’ll also be available to answer questions, help with lab assignments, and will be doing some of the lab grading as well.\nFor communication in this course, there are two basic methods that I recommend. For general questions about the course, content, getting help with labs, and any other items relevant to the course, I encourage you to use the course channel on Discord. This allows all of us to communicate in a single space, and it also means that any questions I answer will immediately be available for the whole class. For personal issues, grading questions, or if you have something that is a “to-do” item for me, please email the course email, which is cis527-help@ksuemailprod.onmicrosoft.com. While I will strive to check Discord often, I’ve found that sometimes tasks can get lost in the discussion, so having an email in my inbox to prompt me to follow up is very helpful.\nI am working remotely out of my home in Kansas City, so I won’t be available on campus very often during the semester. I do have an office, and I will be there usually on Mondays to meet with folks on campus and hold office hours. I’ll be sure to announce those times as they are scheduled.\nIn addition, I must give credit to several people for helping me develop the content in this course. First and foremost is Seth Galitzer, the CS system administrator, as well as several of my former students and teaching assistants.\nFor a brief overview of the course, there are 7 lab modules, plus a final project, that you’ll be responsible for completing. The modules are configured in K-State Canvas as gated modules, meaning that you must complete each item in the module in order before continuing. There will be one module due each week, and you may work ahead at your own pace. When you are ready to have a lab graded, you’ll schedule a time to meet interactively with me either in person or remotely. Finally, all work in this course must be completed and all labs graded by no later than the Friday of finals week.\nLooking ahead to the rest of this first module, you’ll see that there are a few more items to be completed before you can move on. In the next video, I’ll discuss a bit more information about navigating through this course on Canvas and using the videos posted on YouTube.\nOne thing I highly encourage each of you to do is read the syllabus for this course in its entirety, and let me know if you have any questions. My view is that the syllabus is a contact between me as your teacher and you as a student, defining how each of us should treat each other and what we should expect from each other. I have made a few changes to my standard syllabus template for this course, and those changes are clearly highlighted. Finally, the syllabus itself is subject to change as needed as we adapt to this new course layout and format, and all changes will be clearly communicated to everyone before they take effect.\nThe grading in this course is very simple. Each of the 7 lab assignments will be worth 10% of your grade, for a total of 70%. Likewise, there are 15 quizzes, each one worth .66% for a total of 10%. We’ll also have a few guided discussions throughout the class, and your participation in each discussion will be 2% of your grade for a total of 10%. Finally, the final project is worth another 10% of your grade. There will be some extra credit points available, mainly through the Bug Bounty assignment, which you will review as part of this module. Lastly, the standard “90-80-70-60” grading scale will apply, though I reserve the right to curve grades up to a higher grade level at my discretion. Therefore, you will never be required to get higher than 90% for an A, but you may get an A if you score slightly below 90% if I choose to curve the grades.\nSince this is a completely online course, you may be asking yourself what is different about this course. First off, you can work ahead at your own pace, and turn in work whenever you like before the due date. However, as discussed before, you must do all the readings and assignments in order before moving on, so you cannot skip ahead.\nIn addition, due to the flexible online format of this class, there won’t be any long lecture videos to watch. Instead, each module will consist of several short videos, each focused on a particular topic or task. Likewise, there won’t be any textbooks formally used, but you’ll be directed to a bevy of online resources for additional information.\nWhat hasn’t changed, though, is the basic concept of a college course. You’ll still be expected to watch or read about 6 hours of content to complete each module. In addition to that, each lab assignment may require anywhere from 1 to 6 hours of work to complete. If you plan on doing a module every week, that roughly equates to 6 hours of content and 6 hours of homework each week, which is the expected workload from a 3 credit hour college course during the summer.\nAlso, while some of the quizzes will be graded automatically, much of the grading will still be done directly by me. This includes the lab assignments. For each lab assignment, you’ll schedule a time to meet with us either in person or remotely to review your work for that lab. Finally, we’ll still be available to meet with you online for virtual office hours as needed. Of course, you can always contact me via email if you have any questions. In fact, since I regularly work from home anyway, I’ll probably be easier to contact than some of my fellow faculty!\nFor this class, each student is required to have access to a personal computer. It should have plenty of RAM and CPU power to host several virtual machines concurrently. In addition, you’ll need some virtual machine software, which is available for free to K-State CS students. I’ll discuss how to acquire and install that software in the first lab module. Finally, you’ll also need access to a high-speed connection capable of web conferencing with video. If you have any concerns about meeting these requirements, please contact me ASAP! We may have options available through some on-campus resources to help you out.\nFinally, as you are aware, this course is always subject to change. While we have taught this class several times before, there may be a few hiccups as we get started due to new software and situations. The best advice I have is to look upon this graphic with the words “Don’t Panic” written in large, friendly letters, and remember that it’ll all work out in the end as long as you know where your towel is.\nSo, to complete this module, there are a few other things that you’ll need to do. The next step is to watch the video on navigating Canvas and using the YouTube videos, which will give you a good idea of how to most effectively work through the content in this course.\nTo get to that video, click the “Next” button at the bottom right of this page.\n",
    "description": "",
    "tags": null,
    "title": "Course Introduction",
    "uri": "/0-introduction/01-course-introduction/index.html"
  },
  {
    "content": " YouTube Video Video Transcript Congratulations! You made it through CIS 527. I hope that you’ve enjoyed your time working on this course, and that you’ve found the material I covered to be as interesting and useful as I have. I’ve really enjoyed putting it all together for you.\nNow that the class is over, there are a few last-minute things that you’ll want to take care of. First, please complete the TEVAL survey as soon as you receive it in your email. It is your chance to give honest, anonymous feedback about the course, content, and my teaching style. I welcome any comments or suggestions submitted via that survey, and will always be updating my classes to make them better over time.\nSecondly, I’d like you to complete a second survey via Qualtrics, which is linked below this video in the Canvas modules. That survey asks some more specific questions about your experience taking this course online, and will be used to inform future revisions to this course as well as other online courses taught in the K-State CS department.\nAlso, if you have any remaining corrections or updates to the course material that you’d like to have considered toward your Bug Bounty assignment, please submit them ASAP so I can get them entered before final grades are due.\nFinally, if you have any direct comments, corrections, or feedback to me, please feel free to contact me anytime. I’d be happy to hear from you.\nOnce you’ve completed the course, there are a few things that you should do to clear up some space and make sure you aren’t charged for resources you won’t be using.\nFirst, you are welcome to delete the virtual machines we created to free up some disk space on your computer. By the end of the semester, you may have nearly 100 GB of VMs on your system, so it is definitely worth trying to remove the ones you aren’t planning on using. However, you are welcome to keep them for as long as you like if you plan on using them again. Similarly, you can delete some of the old snapshots in order to save space on your system.\nNext, if you don’t plan on using your DigitalOcean droplets, I recommend logging in to the DigitalOcean control panel and destroying them so you won’t continue to be charged to use them. If you are interested in working with the cloud or hosting your own website, however, you might consider keeping the server labelled FRONTEND for that purpose. It is already configured and set up to host websites quickly and easily. If you do, you will definitely want to either remove or secure any websites we set up for this class that you won’t be using.\nLikewise, you may want to clean up the DNS settings for your domain name through Namecheap or whatever registrar you chose to use for this class. I highly recommend building your own website and continuing to use that domain name, but do make sure that it is properly configured before using it.\nLastly, if there are any other online services you worked with in this course, either as part of the lab assignments or your final project, don’t forget to review each of those and make sure they are properly configured or disabled.\nThat should cover it! The material for this class will always be available on my website outside of Canvas, and you are welcome to refer to it at any time. I wish you all the best of luck in the future, and encourage you to keep your system administration skills sharp going forward. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Course Wrap Up",
    "uri": "/9-wrap-up/01-course-wrap-up/index.html"
  },
  {
    "content": " Ed Discussion This video was recorded before I decided to switch to Ed Discussion instead of using Discord. All discussions of “Discord” can be applied to “Ed Discussion” instead. Sorry for the confusion! - Russ\nYouTube Video Resources Slides Video Script Hello, and welcome to the week one announcements for CIS 527 and CC 510. In Fall 2023, my name is Russ Feldhausen. I’ll be your instructor for this semester, my contact information can be found here. It’s also on the syllabus. It’s also on the cannabis page. Feel free to reach out to me if you have any questions or concerns at all in this class. I also have one teaching assistant this semester, Matt Schwartz, Matt is going to be working with me to help you if you have any questions on the labs, he’s also going to be helping me with doing some of the grading Matt did this class last year, I believe. So he’s pretty familiar with a lot of the content. And he’s going to be working right alongside you to check all the labs for me. So if you have any questions, feel free to reach out to either of us, we’re happy to help. In this course, just like a lot of courses, it’s built with gated modules on canvas. There’s one module due every other week in this class, so please check the due dates very carefully. Each module includes a lab assignment, that lab assignment requires a live grading portion, which means you’ll work with either Matt or myself via zoom, so that we can grade your work, it means that this is a very labor intensive class. So please be aware of that. We’re also going to have some discussions throughout the semester. So you’ll either have a recorded video to watch, or I’ll bring in a guest speaker and you’ll have the opportunity to join a live zoom and speak with one of our guest speakers. And then finally, at the end of the semester, we’ll have a final project.\nSo for discussion of this course, I’ve set up a channel on the CIS Discord server. So I please encourage you to use discord. One of the early assignments is to go to discord and introduce yourself there so that you can find the channel and make sure you’re in the right place. So please make sure you do that. If you have any questions, personal issues, grading questions, etc. I’ve set up a help email for this class, which is cis527-help. If you’re in any of the case, Dave email systems, if you just type CIS 527 Dash, it should autocomplete email is kind of our official method for communication in this class, Discord is a little bit more flexible. In general, feel free to ask questions on Discord. But if it’s personal stuff, grading stuff, feel free to put that in an email, that cis 527 Help email goes to both Matt and I and whoever is the first one available, we’ll answer that for you as best we can. So success in this course, learning to be a system administrator is difficult. And it is a different way of thinking compared to a lot of the programming that you’ve done. Because there’s simultaneously one way to do things. And there’s also many way to do things in a lot of ways. So to be successful in this course, you really need to come at it with a growth mindset, it’s going to be tricky, you’re going to break stuff, you’re going to run into issues. And so you have to be willing to sit down and try and learn. But I guarantee you, you can learn how to do any of this if you just take the time to do it. The second big thing about being a successful sysadmin is don’t just read or watch content, but engage with it. There’s a lot of content where I’m just going to sit there and go line by line through a config file, or I’m going to discuss some concepts. And if you just read the video, read the text or watch the video and just go oh yeah, I got it. That may not be enough, you may really need to engage with it and follow along and do those examples yourself. So to really see and understand how it’s working. A lot of these labs involve multiple parts. So please work iteratively get one part working, make sure it’s working, then move on to the next part. One of the most frustrating things in this class is when somebody brings me a lab, it’s like, oh, yeah, I tried to do this and it didn’t work. So then I tried to do this and it didn’t work. And so then I tried to do this and it didn’t work. And so they’ve got like three or four broken things in their lab instead of just one that they can focus on. And it’s hard to tell where the error is. The other nice thing is we’re using VMware for all of this. So VMware has a very nice snapshot feature. So just like any other class, save early and save often make yourself snapshots. Anytime you get something working, I always hate that situation where a student breaks a VM. And then they don’t have any snapshots. And they have to start over. And it happens every semester. And I will guarantee it will happen this semester. So make sure you’re making snapshots early and often and using them all the time. It’s just like playing a video game, you’re walking into a boss fight, auto save before you going to the boss fight. The same thing happens in this course. Finally, don’t be afraid to ask for help. There are always situations in this course where something will not work, right or you will do something and it doesn’t work the same on your computer as it does on every other every other computer. Don’t assume that you’re stupid or that you’re having trouble. Please ask for help. It is very easy in this course to back yourself into a corner. And almost every semester there’s a student that has something happened on their system that even I can’t debug. And so don’t sit there and just spin your wheels. If you get stuck and you’re not making progress, ask for help. Let us know Matt and I are here to help. So don’t be afraid to reach out and let us know if you have any questions.\nSo like I talked about the lab grading this course there are seven labs. Most of them are graded interactively. I think there are two labs that we submit everything online. For the interactive grading, what you’ll do is you’ll schedule a grading time via Calendly, we recommend choosing a 15 or 20 minute time slot, please, for my own sanity, please check the TA schedule first, Matt gets paid to do this hourly. So the more times that he can grade stuff, the more money he makes. So please feel free to check that schedule first and see if you can get on his schedule. If not feel free to get on my schedule, you can schedule your lab time before you’re done with the lab. But the lab grading must happen before the due date. I think most of the labs are due on Friday. And so you should schedule your lab grading time before Friday at 5pm. There are 25 of you in this class, which means if we do for an hour, that still takes six hours, at least, to grade everything. So that’s like I said, it’s very labor intensive to do this class. So please be aware, you may not get that early time if you don’t schedule ahead. The other thing is my Calendly, especially requires anywhere from two to four hours notice. So if you get done with the lab at four o’clock on Friday and want to try and find a grading time between four o’clock and five o’clock on Friday, it’s not going to be there. So please be aware of that schedule your grading times in advance. You when we do grading, we’ll bring it up on Zoom, we’ll have you share your screen pointed at VMware, we will ask you to walk through things in your VMs and show us that it’s working correctly, we may ask you to run commands, we may ask you to show us config files, we may ask you to do certain things. That’s our way of confirming issues, your systems work the way they should. The last thing, once you start grading, you cannot make a change. For example, if you pull up a config file and realize that something is wrong, it’s too late, you can’t make that change. So please make sure that you have everything done before you do any grading. Because once you start that grading process, it is the way it is. And we have to take it as you submit it. We try and be pretty lenient with partial credit, but do your own testing, make sure things are working before you go for lab grading.\nSo this year, one of the things I want to do, again, is discussions a couple years ago, I brought in some guest speakers. Now I’ve got a big class this semester, I think it’d be fun to do that again. So I’ve got a survey, it’s on the first page of the discussions module, it’s a little when to meet survey, please respond to that survey by Friday, August 25. I’m going to take the most common overlap time or maybe one or two of those and make those into zoom discussion times. On most weeks, I’ll just have them zoom office hours, but a few times throughout the semester, I will try and bring in some guest speakers. In years past I had Seth Galitzer, I got K-State’s CIO, Kyle Hutson to talk about supercomputers. I’ve had a couple of folks from industry that have come in. So it’s a really great chance to learn more about being a system administrator, and all the different ways that that looks out in the real world. And so part of your grade will come from participating in those discussions, you’ll submit some questions beforehand that I can pick from, and then you’ll either participate in the live discussion on Zoom, or you’ll watch the video and write a response afterwards. But it’s a really good way to learn more about system administration in the field itself. So I think this is a really fun part of this class. So that’s really all I’ve got for this first week of announcements. As always, if you have any questions, join the discussion on Discord. I’ll be looking forward to seeing your announcements, your introduction posts there, and I’ll be pretty active on Discord throughout the semester. I also host Tea Time office hours, which are Mondays at 11 o’clock this year, you’ll see some information being posted about that shortly. The Tea Time officers are available on campus in person, but they’re also held on zoom so you can join either way. And then of course, both Matt and I will have some one on one office hours available via Calendly. So you can schedule some time there. But again, bear in mind, there’s 25 of you in this class. It’s a very labor intensive class. So please be mindful and try and do the best you can working with us on Discord and some other things. And then schedule the office hours if you need that time to work with us directly. Because we’re going to be really overloaded trying to keep up with 25 students in this class. So that’s all I got for this semester. I’m rooting for you. Good luck. I hope everything goes well. You’ll probably see me every other week. Usually since this is an every other week class. I usually post announcement videos every other week. So watch for an announcement video probably the day after Labor Day this year. If you have any questions or concerns, let us know otherwise, I look forward to interacting with you on Discord and I will see you in a couple of weeks.\n",
    "description": "",
    "tags": null,
    "title": "Fall '23 Week 1",
    "uri": "/y-announcements/week01/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Operating System on Wikipedia Types of Operating Systems on Geeks for Geeks Understanding Operating Systems on GCF LearnFree GNU/Linux Naming Controversy on Wikipedia The Great Debate: Is it Linux or GNU/Linux? on How-To Geek Video Script Welcome to the first module in this class! Module 1 is all about creating secure workstations. In this module, you’ll learn how to install virtual machine software, install the operating systems we’ll be using in the class, and then configure several aspects of the operating systems. You’ll create users, manage file permissions, install software, and secure those systems.\nBefore we begin, here is a short overview of one major concept in this module - the operating system. Every computer you use has some sort of operating system installed, even if you don’t realize it. For this module, we’ll be using the two most commonly used operating systems in industry today, Microsoft Windows 10 and Ubuntu Linux. I’m guessing that most of you have used at least one of these systems before, and in fact many of you are probably using one of them now.\nOperating systems make up the core of a modern computer. This diagram shows exactly where the operating system fits in a larger hierarchy. A computer consists of hardware, and the operating system is the program that runs directly on the hardware. It is responsible for interfacing with the hardware, and running the applications needed by the user. On the very first computers, each application was itself an operating system. This allowed the programs to directly interface with the hardware, but the major drawback was that only one program could run at a time, and the computer had to be restarted between each program. In addition, each program would need to be customized to match the hardware it was running on.\nBy using an operating system, applications can be much more generalized, and multiple applications can be running at the same time. Meanwhile, hardware can be changed, sometimes even while the computer is running, and the operating system will manage the necessary interfaces to use that hardware. It is a very efficient system.\nThe major part of the operating system is called the kernel. It is the part specifically responsible for creating the interface between user applications and the hardware on the system. In fact, most of what you may consider an operating system is in fact applications running on the kernel. The start menu, control panel, and registry are actually applications in Windows. Because of this, there is some disagreement over whether they are considered part of the operating system or not. The same discussion has been going on for years in the Linux community. See the resources section for links to the discussion of GNU vs. Linux.\nFor this course, anything referring to the kernel or programs typically bundled along with the kernel will be considered the operating system. This follows the typical convention in most system administrator resources online.\nIf you are interested in learning about how operating system are built and how the kernel functions, consider taking CIS 520: Operating Systems.\nNext, we’ll start discussing Windows 10 and Ubuntu Linux in detail.\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/1-secure-workstations/01-introduction/index.html"
  },
  {
    "content": " Puppet Learning VM Deprecated As of 2023, the Puppet Learning VM is no longer being maintained. The videos below demonstrate some of the features of Puppet, which can also be done on your Ubuntu VM after installing Puppet Agent. Unfortunately, it is not easily possible to simulate an enterprise Puppet setup without this VM, so I’ll keep these videos up for demonstration purposes. –Russ\nYouTube Video Resources Slides Video Script Welcome to Module 2! In this module, we’ll be discussing configuration management. The next video will define that concept in detail and give some additional background.\nThe lab assignment will have you work with Puppet to completely automate most of the tasks you performed in Lab 1. By doing so, hopefully you’ll see the incredible power of using configuration management tools in your workflow as a system administrator.\nFor the lab, I chose to use the Puppet configuration management tool for a couple of reasons. First, they have an open source, community edition that is very powerful and easy to use, so there is no cost to get started. Secondly, their documentation and support is very well done, so you’ll have plenty of ways to get help if you get stuck. Finally, they provide an easy to use learning VM that can walk you through the basics of Puppet quickly and easily.\nTo get started in this module, I recommend downloading the Puppet Learning VM at this URL. The link is also in the resources section below this video. It is a very large download, so I recommend starting it now so you are ready to go by the time you get to that point. A later video will discuss how to install and use that VM in detail.\nGood luck!\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/2-configuration-management/01-introduction/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Transcript Welcome to Module 3! In this module, you’ll learn all about how to link your systems up using a variety of networking tools and protocols. First, we’ll discuss the OSI 7-Layer Networking Model and how each layer impacts your system. Then, we’ll look at how to configure various networking options in both Windows and Linux.\nFollowing that, you’ll learn how to configure a DNS and DHCP server using Ubuntu and connect your VMs to those services to make sure they are working properly. Finally, you’ll gain some experience working with some network troubleshooting tools, and you’ll use those to explore a few networking protocols such as HTTP and SNMP.\nThe lab assignment will instruct you to configure remote access for the VMs you’ve already created, as well as set up a new Linux VM to act as a DNS and DHCP server for your growing enterprise. Finally, you’ll get some hands-on experience with the SNMP protocol and Wireshark while performing some simple network monitoring.\nThis module helps build the foundation needed for the next module, which will cover setting up centralized directory and authentication systems in both Windows and Ubuntu. A deep understanding of networking is also crucial to later modules dealing with the cloud, as the cloud itself is primarily accessed via a network connection.\nClick the next button to continue to the next page, which introduces the OSI 7-Layer Networking Model.\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/3-core-networking-services/01-introduction/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Transcript Welcome to Module 4! In this module, we’ll discuss how to use directory services to provide consistent user accounts and permissions across a number of systems. Specifically, we’ll look at the Active Directory Domain Services for Windows, and the OpenLDAP implementation of the Lightweight Directory Access Protocol (LDAP) for Ubuntu Linux.\nIn addition, we’ll see how to configure clients to connect to each of those servers. Finally, I’ll discuss some implementation concerns for interoperability, for instances where you’ll have a heterogeneous network environment.\nMany students have reported to me that this is one of the most difficult lab assignments in this course, as it can be very frustrating to get everything set up and working correctly the first time. As always, if you have any questions or run into issues, please post in the course discussion forums to get help. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/4-directory-services/01-introduction/index.html"
  },
  {
    "content": " YouTube Video Resources Slides GitHub Student Developer Pack Video Transcript Welcome to Module 5! In this module, we’ll learn all about one of the hottest topics in system administration today: the cloud! Many enterprises today are in the process of moving their operations to the cloud, or may have already completely done so. However, some organizations may not find it feasible to use the cloud for their operations, and we’ll discuss some of those limitations.\nFor the lab assignment, you’ll create a few cloud systems and learn how to configure them for security and usability, We’ll also start creating some cloud resources for an organization, which we’ll continue working with in Lab 6.\nAs a sidenote, this lab involves working with resources on the cloud, and will require you to sign up and pay for those services. In general, your total cost should be low, usually around $20 total. If you haven’t already, you can sign up for the GitHub Student Developer Pack, linked in the resources below this video, to get discounts on most of these items. If you have any concerns about using these services, please contact me to make alternative arrangements! While I highly recommend working with actual cloud services in this course, there are definitely other ways to complete this material.\nAs always, if you have any questions or run into issues, please post in the course discussion forums to get help. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/5-the-cloud/01-introduction/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Docker Tutorial for Beginners by TechWorld with Nana on YouTube Video Transcript Welcome to module 5 A - the first new module added to this course in several years. In this module, we’re going to explore one of the biggest trends in system administration - containerization. We’ll see how containers have quickly overtaken traditional virtual machines for running workloads in the cloud, and how tools such as Docker make using containers quick and easy. We’ll also learn how we can use containers in our own development workflows, making it much easier to build tools that will work as seamlessly in the cloud as they do on a local system. Finally, we’ll take a look at Kubernetes, which has quickly become the most talked about tool for orchestrating containers in the cloud.\nBefore we begin, I want to give a huge shout out to the TechWorld with Nana channel on YouTube, which is linked at the top of this page. Her comprehensive video courses on Docker and Kubernetes are the most well organized resources for those tools I’ve come across, and much of the structure of this module is inspired by her work. So, I must give credit where it is due, and I encourage you to check out her resources as well.\nWe’ll start by taking a look at the architecture of containers in general, and then we’ll dive in to using Docker on our own systems. Let’s get started!\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/5a-containers/01-introduction/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Transcript Welcome to Module 6! In this module, we’ll discuss the various types of servers that you may come across in a large organization. This could include file servers, web servers, application servers, database servers, and more!\nIn the lab assignment, you’ll set up a file server on both Windows and Ubuntu, and deal with automatically providing access to those resources for your users. You’ll also configure the IIS and Apache web servers, and learn about installing an application on each. Finally, we’ll discuss some of the considerations for configuring a database server, and you’ll work with setting up a database server on Ubuntu as well.\nThis lab is quite a bit more open-ended than previous assignments, as you’ll have the ability to work with a web application of your choice. However, that can also make the lab a bit trickier, since each student may go about completing it in a slightly different way. This very closely mirrors what you’d find in a real-world scenario, as every organization’s needs are different, too.\nAs always, if you have any questions or run into issues, please post in the course discussion forums to get help. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/6-application-servers/01-introduction/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Transcript Welcome to Module 7! In this module, we’ll discuss several additional topics related to system administration. Honestly, each of these items probably deserves a module unto themselves, if not an entire course. However, since I am limited in the amount of things I can reasonably cover in a single course, I’ve chosen to group these items together into this single module.\nFirst, we’ll discuss DevOps, or development operations, a relatively new field blending the skills of both system administrators and software developers into a single unit. We’ll also discuss a bit of the theory related to system administration, including ITIL, formerly the Information Technology Infrastructure Library, a very standard framework for system administrators to follow.\nThen, we’ll cover information related to monitoring your systems on your network and in the cloud, and how to configure some of those tools in our existing infrastructure from previous lab assignments. Finally, we’ll discuss backups, and you’ll perform a couple of backup and restore exercises as part of the lab assignment.\nAs always, if you have any questions or run into issues, please post in the course discussion forums to get help. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/7-backups-monitoring-devops/01-introduction/index.html"
  },
  {
    "content": " YouTube Video Resources How to Write a Proposal from wikiHow Standard Technical Paper Template from Dr. Goddard at University of Nebraska - Lincoln (via Dr. Andresen) SWOT Analysis on Wikipedia How to Write a Technical Paper by Dr. Ernst at University of Washington Tips for Writing Technical Papers by Dr. Widom at Stanford University What Is a SWOT Analysis? from Bplans SWOT Analysis from Investopedia How to Do a SWOT Analysis for Your Small Business from WordStream A Detailed SWOT Analysis Example (Applicable to All Industries) from ClearPoint Strategy How to Plan a Technical Presentation from Witt Communications 5 Tips for Giving Effective Technical Presentations by Patrick Fuller on LinkedIn Video Transcript Greetings! This video describes the final project in this class. You can also find the full assignment description on the next page of this module. I encourage you to read it as soon as possible and ask any questions you have via the course discussion forums. I anticipate this project taking several weeks to complete, so I recommend starting soon. You do not have to wait until you have competed the entire class to start on the final project, but I do expect you to have all the labs completed before submitting the project, since it should reflect the entirety of your learning in this class.\nAlso, you may choose to work with a partner on this assignment. If you do, please email me and let me know. Teams will be expected to produce roughly twice the quantity of work described here, so plan to scale the scope of your project accordingly.\nIn essence, the goal of this project is to demonstrate your knowledge and experience in system administration by proposing, prototyping, and analyzing some type of information technology project. The audience for your proposal will be the typical C-suite executives in your organization. So, some of them will have a technical background, and others may not. You’ll have to design and write about your project accordingly.\nThere are several types of projects I’ve highlighted for this assignment:\nA change to K-State’s IT infrastructure Building a new cloud-based startup in a particular domain area Building an effective computer support infrastructure for K-State Designing and configuring a devops stack for a programming project With all of those options, you may substitute K-State for another organization you are more familiar with. However, it should be of an appropriate size to make the project worthwhile. If you’d like to get some examples for each type of project, feel free to ask me to provide them.\nFor this assignment, you will create four parts.\nThe first is a 1-page proposal describing the background and your proposed change. You may also include a brief analysis and justification for the change, but it is not necessary at this point. The proposal should also be in a professional format. The linked article from wikiHow gives some great examples. You’ll submit the proposal online via Canvas, and I’ll approve it, usually within a working day. Once it is approved, you can start working on the project itself.\nThe second part, and bulk of the project, consists of designing your proposed infrastructure. As part of the design, you should select some portion that you believe should be prototyped, and build that prototype using either virtual machines or cloud resources, whichever is appropriate. For example, if building a new cloud startup, you might prototype what a server for that company would look like, or explore systems for automatically provisioning cloud resources to match the scaling needs of the company. You don’t have to build the whole thing, but I expect you to build some smaller portion that demonstrates your ability to use what you’ve learned in this class in some practical way.\nFor the third part, as you are developing your proposed infrastructure, I would also like you to spend some time analyzing it. The design should be consistent with what we’ve learned in the class, as well as industry best practices. You’ll probably need to do some outside research and reading for this part. Make sure you keep track of all the resources you use, since you’ll need to properly cite those in your report. I’d like you to perform a SWOT analysis of your proposed structure. A SWOT analysis looks at the internal strengths and weaknesses of a proposal, as well as the external opportunities and threats that are implied by that project. Many businesses use this type of analysis for self-reflection, and I feel that it is a very valuable skill to learn. As an aside, in previous semesters many students simply reported that their proposal didn’t have any weaknesses or threats, but that simply isn’t the case. Any proposal has trade-offs and weaknesses, and it is your job to find them.\nOnce you are comfortable with your proposed design, prototype, and analysis, you’ll need to write a technical paper describing what you did. I’ve included a link to the Standard Technical Paper Template, which I highly encourage you to use for your paper. I have not given any specific minimum or maximum length, but it should adequately describe your project without being so long that it is difficult to read. Feel free to include any helpful graphics or diagrams to help describe your project. In addition, make sure you properly cite any resources or references you used while working on this project.\nFollowing that, you should prepare a presentation to share your findings with your audience. The presentation should be 7-15 minutes in length. You may use any style of presentation you choose, as long as it would be appropriate for this project. Again, make sure you keep your intended audience in mind as you create the presentation. During your presentation, be prepared to describe your proposal, demonstrate the prototype, and present your analysis in order to convince your audience that it is a good proposal that should be implemented.\nWhen you are ready, you’ll submit your write-up and presentation materials via Canvas. At that time, you’ll also contact me to schedule a time to give your presentation, either in-person or via Zoom. You won’t submit your prototype, but will demonstrate it during your presentation.\nThat’s all! Hopefully it all makes sense, but if not, feel free to post your questions on the course discussion forums on Canvas.\nFinally, a note on deadlines. As with everything else in this course, there is no specific deadline for this project other than the end of the course. That said, you should start on this project well before that date. Specifically, you must submit your proposal no later than one week before the last date of the course. In addition, if you contact me on the final day trying to schedule a presentation time, it may already be too late to complete everything. So, don’t wait until the last minute to complete this project!\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/8-final-project/01-introduction/index.html"
  },
  {
    "content": " YouTube Video Video Transcript Greetings everyone! This module contains additional materials and helpful information for system administrators. You are not required to view all of these videos, but I encourage you to do so if there are any topics here you find interesting.\nAlso, feel free to suggest any topics you’d like to see covered in this section as you go through the course. Any suggestions may be considered toward the extra credit assignment in this course.\nEnjoy!\n",
    "description": "",
    "tags": null,
    "title": "Introduction",
    "uri": "/x-extras/01-introduction/index.html"
  },
  {
    "content": " Install VMware (2) Create Windows 10 VM (3) HDD 60 GB or more RAM 1 GB or more Configure Windows 10 (10) Computer Name includes eID Users (cis527, AdminUser, NormalUser, GuestUser, EvilUser) Groups (Administrators, Users, Files) Software (VMware Tools, Firefox, Thunderbird, Notepad++, BGInfo) Windows Updates Access IIS from Ubuntu (firewall and IIS config) Windows 10 Files (10) Run Get-ChildItem -Recurse | Get-Acl | Format-List Check for groups and user on each folder (compare with screenshot) Create Ubuntu VM (3) HDD 20 GB or more RAM 1 GB or more Configure Ubuntu (10) Computer Name (terminal) User Accounts (cat /etc/passwd) Groups (cat /etc/group) Software (vm tools, Synaptic, GUFW, ClamAV) Firewall (enabled?) Access Apache from Windows (firewall and Apache config) Updates (apt update \u0026 apt upgrade) Configure Automatic Updates Ubuntu Files (10) Run ls -lR Check for group and user on each folder Snapshots (2) Windows Ubuntu ",
    "description": "",
    "tags": null,
    "title": "Lab 1 Grading Checklist",
    "uri": "/z-instructor-resources/01-lab-1-grading-checklist/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello, and welcome to the week one Announcements video for system administration during summer 2022. My name is Russell Feldhausen. I’ll be your instructor for this course, my contact information is shown here. You can also find it on the syllabus and on the homepage on K-State canvas, it’s by far the best way to get a hold of me is to just email me. But as I discussed in the intro video, and as I’ll discuss here, for anything related to the course, please use discord, it’s a great place for us to chat and you should get some very quick responses from you there. So a little bit about the structure in the course.\nThis course uses gated Modules in Canvas, there is about one module due per week. So if you look at the Modules tab, you can already see the first few modules published there. Please make sure you check the due dates on all of the assignments in the modules. The due dates are kind of spread out throughout the week sometimes. So make sure that you’re aware of the due dates that are upcoming, especially for the lab assignments. For these lab assignments, we’re going to do live grading, which means that you need to schedule a time to meet with me via zoom, and then I’ll be able to watch your screen using screen share. Have you walked through the lab live with me watching. And that’s how we’ll do the grading, you need to not only schedule your grading time, but actually have the grading time before the due date. So if a lab is due Friday at five, you need to schedule and meet with me before Friday at five to get full credit on that lab.\nThere’s also some discussions those are separated out in their own module, the discussions have their own due dates that are a little bit different than the rest of the weeks. So make sure you check those out. Because we have a small enrollment this semester, I didn’t go to the effort of actually bringing in new discussion speakers. So we’re going to watch the videos from last summer that I still feel are very relevant. And you’ll have a chance to respond to those and ask some more questions that I will then pass on to those speakers and see if I can get some answers. Also, this course has a final project, the final project module is upfront, so you can check it out right away. Be thinking about your final project. As you work through these first few labs, you don’t actually have to have a full proposal for your final project until toward the end of class. But the sooner you can come up with an idea and start thinking about it, the easier it will be to do down the road.\nSo as I said, for course communication, I prefer to use discord for any course questions or discussions. So if you have a question on a lab, if you’re not sure how to do something, if you want to chat about anything, Discord would probably be your best place to start. I’ll be there all of the other students will be there, it’s a great place to get questions answered, you might actually get answers from folks that are outside of this class if they pop in and help out a little bit with that. So Discord is definitely your first place to go. Email is the official form of communication here. Mainly it’s for personal issues, grading questions to dues. But if you ask a question on Discord, and you don’t get an answer, feel free to email it to me. And I will guarantee that you’ll get a response from me within one business day. So like I said, email is official. But Discord is a bit more flexible for communications. That’s why I’m pointing most things to discord. But email is still there for official.\nSo to be successful in this course, first and foremost, you need to come into this course with a growth mindset, you can learn how to do this, it is going to be difficult, and especially some of these labs are meant to be a little bit of a struggle. If you haven’t done some of these things before, it’s going to feel kind of frustrating to do it the first time, part of the point is working through that frustration and learning how to get past it because the second time you do it, it will be much easier. Another big thing I really encourage you to do, as you’re working through the content in this course, don’t just mindlessly watch or read the content, but actually try and engage with the content. A lot of the content that you’re going to see in the lab, in the lab textbooks in the videos that I do in all of the activities that you see, really is helping you work toward the lab. So if you’ve gone through that content, and really engaged in it and tried to learn it, the labs should be much, much easier. For the labs themselves, I really encourage you to try and work iteratively, pick one task in the lab and try and get that working as best you can before you move on to the next task. It makes it much easier to make sure that you’ve got things working in debug problems before you move on. Another big thing to really keep in mind is to save your work early and save it often VMware the virtualization tool that we’re going to use support snapshots, you definitely have to create a snapshot before the end of lab one. And most students learned the hard way that it’s much, much easier to take a snapshot before you try and install it, configure something and then roll back to that snapshot. If you screw it up versus trying to uninstall and undo all the configuration you’ve done snapshots work really great and VMware, please take advantage of that. And then finally, don’t be afraid to ask for help. You know, like I said, part of this class is exploring on your own and reading documentation. But if you get stuck and you’re not making progress for about a half hour or so, that’s a good sign that you need to take a step back and come and ask for help. I really hate to see students spinning their wheels on something in this class and then later on finding out that it wasn’t something they could have solved anyway. So if you get stuck, try and get past it if you can, but if not, don’t be afraid to ask for help.\nSo I talked about this a little bit earlier. But for the lab grading each lab, you’re going to schedule a time with me via Calendly for grading. The two exceptions are lab two, you don’t have to do live grading. And I believe right now lab seven is set up that way, but I’m changing some parts of lab seven. So that may change toward the end of the semester, you can schedule your time for grading before the lab is complete, as long as the time that we will meet is after the lab is complete. So feel free to go in on Monday and schedule a time for Friday afternoon, if you are sure that you’re going to have the lab done by Friday afternoon, totally fine. Calendly does require you to give me four hours notice. So if you try and go online and Friday at two and try and schedule a time for Friday at four, it’s not going to be available. So you need to make sure you get your lab schedules taken care of ahead of time, you can even go ahead and just ask me for a weekly timeslot. And I’m happy to set that up for you. When we do the lab grading, we’ll connect via zoom will use screen sharing so I can see what you’re seeing and have you walked me through different things on your virtual machines. And then the other big thing about lab grading is once we start grading, you cannot go through and change things. So if we’re halfway through grading, and you realize you did something wrong, it’s already too late. Because we started the grading process, it’s already been submitted as it were. And so bear in mind that once you start the grading process, you can’t change anything, even if you notice that it’s wrong.\nSo I also touched on this a little bit earlier, I’m normally in this class, I try and bring in guest speakers for discussions. However, we’ve got a very small group this semester. And because I had some really good discussions last summer, I think I’m just going to reuse those videos from last summer. So for the discussions, this time, you’ll get a chance to watch the video and write a reaction to the video. And then you’re also going to come up with a few questions for that speaker. And once I get all those submissions, I will actually send those on to that speaker and see if I can get a written response back that I can share with the class. So even though you may not get to interact directly with these speakers, they’re more than willing to chat with you offline via email. So I’m happy to pass questions around based on those discussions. So that’s really everything you need to know to get started in this class.\nLike I said, please feel free to keep in touch. The first module has you do a quick introduction on Discord. So make sure you get that done soon. You can join us on Discord for chatting anytime I will be there I’ll probably be posting news and articles throughout the summer just to keep the engagement up. Another thing you can do to keep in touch is several of the computer science faculty host what we call t time office hours. Those are Tuesdays at 3:30 and Fridays at 10:30. Throughout the summer. Tea Time is meant to be an open office hours time where you can come in and chat about anything except classwork that you’re working on. So life university if you need advice, if you just need to hang out with people, a lot of times it’s just me and the faculty hanging out and having a lot of fun while we work on other things. So feel free to join us for tea time. I think it’s a really great opportunity to be able to get to know some folks in industry and some faculty. Sometimes we have alumni folks dropped by. So check out tee time. And then of course, if you need any help, you can always schedule one on one office hours through Calendly. It’s the same link that you use for grading. So feel free to grab a time if you need some one on one help from me as well.\nSo that’s all I’ve got for this week’s announcements. You’re welcome to get started on the class. I wish you the best of luck throughout the semester. I’ll try and post a weekly announcements video usually sometime on Monday. That gives you some updates on the next week’s work. And so keep an eye out for that. You’ll also see me post things on Discord every once in a while but best of luck to you this summer and I look forward to working with you throughout the semester. Good luck\n",
    "description": "",
    "tags": null,
    "title": "Summer '22 Week 1",
    "uri": "/y-announcements/old/summer2022/week01/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello and welcome to the week two announcements video for CIS 527 and CC 510 in Fall 2023. I’ll try and post announcements videos like this about every other week in this class. Usually these announcements videos are pretty informal, so you’ll notice I don’t have my background set up or anything, but it’s just a chance for me to talk through some things that are on my mind, things that I would normally announce at the beginning of class if we were meeting in person. But instead, I think it’s much easier to do these quick videos is.\nSo first things first, you should be working on lab one and you should be hopefully getting to the point where you’re about ready to get it graded. So what you’ll need to do to get your labs graded is you’ll need to schedule a time slot to meet with either Matt or myself. My preference is that you check Matt’s schedule first. The only reason I say that is because there are 26 of you and there’s only one of me, and I’m currently teaching three classes officially and three classes unofficially. So my schedule is very packed this semester and so I’m really relying on Matt to cover a lot of the grading load, but I’m available as well. So please schedule a time slot. Check matt’s calendar first. If Matt’s times don’t work for you, then check my calendar and pick a time with me. When you’re ready to get graded. Make sure you make snapshots in VMware so that you can roll back to those snapshots if anything breaks. And then when it’s time for grading, please have everything ready. That means have your VMs open, have them up and running. Have files open, whatever we ask you to do so that it goes really quickly. And then just be prepared to demonstrate your work for this first lab. We’re going to ask you to show us a few settings on each of your systems. And we’re going to ask you to run a couple of terminal commands that will show all the file permissions for the docs folders that you made. So be prepared to do some of that. Typically, lab grading only takes about ten minutes per lab if everything thing is working correctly. However, if things are not working correctly, it can take much longer as we try and diagnose problems and look for partial credit. Um. When we’re doing lab grading, we may not work with you long enough to fix those problems. You may want to schedule a follow up time to get some problems fixed, especially in labs three and four where those labs build on each other. This first lab, don’t worry about it. We’re going to delete these VMs as soon as we’re done with them, so don’t worry too much about it. This is just an introductory lab, but be aware that that’s what we’re doing with the lab grading, so let us know if you have any questions on that. But don’t be afraid to schedule your time slot. And to be clear, you can schedule your time slot now, as long as that time slot happens after you get the lab done. So if you want to get graded next Wednesday when it’s due, I believe it’s Tuesday or Wednesday when it’s due, go ahead and schedule the time slot that works for you. It’ll be on our calendars. And then make sure you have the lab done before that time slot. Do not wait until the due date to try and schedule your time slot because A, most of them will be full, and B, most, I believe Matt’s calendar does. And I know my calendar requires anywhere from two to 4 hours prior notice to schedule a time. So be aware of that.\nThe other thing that I’ve done is I’ve scheduled our weekly discussion times for Thursdays from one to 02:00 p.m.. That was the time slot that worked best for most people in this class and also fits my schedule. So what we’ll be doing with that is I’m going to reach out to a few friends of mine and try and get a few guest speakers to come in during those discussion times to tell you about their jobs in it and what they do. So I’ll be posting a schedule for that very soon. Some of these will be repeats from previous semesters, but I’m also going to try and get a couple of new people if I can find some, so be aware of that. So to earn points for the discussions, you’ll notice that there are five graded discussions throughout the semester. What you’ll do is you’ll submit two questions before the scheduled discussion time. So I will be updating those and posting those as soon as I get the schedule figured out. Then you can either attend the live discussion with our guest speaker Thursday from one to 02:00 P.m. That week. Or if you’re not available, you can watch the video afterwards. Either one. To earn points. If you attend live and participate by asking a question, you’ll get points. If you attend live and don’t ask a question or you watch a video, there will be a short response that I will have you write afterwards. So you don’t have to participate if you don’t want to. There are ways to earn points either way, and then for weeks where we don’t have a scheduled guest speaker, I will just use those times as office hours. Matt may also be available during those times, so I will hop in the Zoom session. If you have questions, join us and ask us any questions you have. If nobody shows up after about ten or 15 minutes, I will tend to drop out of the Zoom session. But those are kind of our open office hours times. If you have any questions for us on days that are not scheduled for a guest speaker.\nThe other big thing in this class if you need any help, there are a couple of different ways I recommend getting help. First and foremost, there’s the CIS 527 help email address that is posted on the Canvas website. That address goes to myself and Matt, so we can both answer those questions very quickly. We’ve also set up an Ed discussion board for this course that you can find on Canvas linked on the side. Ed Discussion is a great place to get questions answered in this class because it’s got a lot of features where we can post code and snippets and things like that. And any of your questions you post can be seen by others in the class. So it’s a great way where we only have to answer a question once, especially if it’s a popular question and someone’s confused on it. If either of those don’t work, you can also schedule a time via calendar with either myself or Matt to get some one on one help. And then of course, you’re also welcome to attend the office hours on Thursdays if we’re not having a guest speaker that week. So that’s all the real updates that I’ve got for this week. Hopefully things are going well with the first lab up. Hopefully by now you’ve gotten access to VMware and you’ve gotten access to the Microsoft Store so you can get VMware installed, get Windows installed. If you haven’t done that, I recommend doing that now. If you have any issues with that, let us know. It may take us a day or so to go through the process to get that fixed. So again, do not wait until the last minute to start on these labs. We really want you to be successful, and that means starting early. So, as always, if you have any questions, let us know. Otherwise, I will probably post another video next week. Good luck.\n",
    "description": "",
    "tags": null,
    "title": "Fall '23 Week 2",
    "uri": "/y-announcements/week02/index.html"
  },
  {
    "content": "Creating secure workstations for an enterprise.\n",
    "description": "",
    "tags": null,
    "title": "Secure Workstations",
    "uri": "/1-secure-workstations/index.html"
  },
  {
    "content": " YouTube Video Resources Slides 10 Tips for Successfully Implementing Enterprise Software from Liquid State An Analyst’s Guide to Evaluating Enterprise Software from Automation World 18 Enterprise Software Selection Risks and How to Avoid Them from CIO.com Video Transcript This module is all about application servers. So, before we begin talking about them in-depth, we should first discuss what an application server actually is.\nIn this class, I’ll be referring to an application server as any server that runs an enterprise application. So far, in this course we’ve dealt with setting up workstations, centralized authentication systems, network services, and cloud resources, but most of those are just the infrastructure for our enterprise. Most of the activity in our organization will happen on top of that infrastructure in a variety of application servers. They handle the backend processing and storage of data, as well as the frontend interface that our users will see, either on a website, mobile application, or custom software tool. In general, application servers, are the always on, always available resources that our infrastructure provides to our users.\nThere are many different types of application servers that your organization may use. These could include web servers such as Apache or IIS, database servers such as MySQL or Microsoft SQL Server, email servers, and even some file servers. Those servers could also run specialized application such as customer relationship management or CRM, inventory, accounting, and more. In this course, we’ll primarily be dealing with web servers, file servers, and database servers, as they are most commonly used in a variety of enterprises.\nOne of the major tasks that any system administrator may face is to help your organization evaluate and choose new software. The choice of enterprise software is a very important decision, and can have a major impact on your organization’s budget, flexibility, and success in the future. So, in this video, I’ll discuss some considerations you should have in mind if you are ever asked to evaluate software for your organization.\nThe first step is to perform an evaluation of the software packages available for your needs, before you ever download or install them. Some things you might look at in this phase are the source of the software itself: is it a reputable company, an open-source project, or an unknown entity. Likewise, you could look at the available support offerings, knowledgebase, and user community around the software. If the community is active and the support documentation is well written, it is generally a very good sign. You might also look at the company’s history and coding practices if you can find any information about either of those. If the company has a history of good software, or, conversely a history of major flaws and bugs, that should weigh into your decision as well. Depending on your needs, you might also explore options for extending the software and integrating it into your existing infrastructure. Finally, it’s always a good idea to read some reviews and seek recommendations from others. If your organization works in a unique field, it might be worth contacting some of your peers to see what they are using and what their experience has been. In many cases, they can direct you to the best option available, or let you avoid the mistakes they’ve made along the way. And all of this comes before you’ve even looked at the software itself!\nOnce you have a few candidates that you’d like to test, you should go through the process of installing or using each one. As you do so, you’ll want to make a note of any other software or tools that are needed to work with the application you are reviewing. You’ll also be on the lookout for system requirements, such as storage, RAM and CPU usage. In addition, you should look at how easy it is to get data in and out of the system. One of the major frustrations with enterprise software is “vendor lock-in,” which happens when all of your data is tied to a particular software program or vendor, and there is no easy way to move to a new system. Next, you’ll want to consider how easy it was to configure the software. Does it have all the options you need, or are there parts of the system that are difficult to work with for you as an administrator, or possibly for your users. Finally, as you work with the software, you’ll definitely want to use tools such as Wireshark to monitor any network traffic on your systems. While it is rare, some applications can have major security flaws or even be compromised before installation, so you should always be on the lookout for unusual network traffic when evaluating new software.\nLastly, you’ll need to extensively test the software for usability with your users and customers. Even if the software itself is high quality, if your users are unable to use it effectively, it could be worse than not having any software at all. As you work with a group of users to beta test the system, you’ll want to monitor many aspects of how the system performs, such as the system load and load balancing needed. You should also continue to monitor the network traffic, and ensure that any data sent across the network is properly encrypted. Remember our example from Lab 3 with Apache authentication - just because it is unreadable doesn’t mean that it is properly encrypted, so you may have to consult the documentation or a security expert if you aren’t sure.\nIn addition, you may also want to look at how easy it is to deploy updates to the system. Does it require a large amount of downtime, or is it relatively seamless? You could also look at features such as logging and accountability. Are you able to tell which users are accessing the system, and what they are doing? It could help you diagnose problems, but also detect when users are acting maliciously. In many cases, security issues are the result of insider threats, so having a good idea of what your users are doing is very important. Finally, you should always evaluate the accessibility of the software. Does it work well with assistive devices such as a screen reader or alternative input devices? Would colorblind users have issues interacting with the software? Sometimes this can be as simple as having red and green buttons or icons with no text on them - a colorblind user would not be able to tell the difference between them.\nOf course, there are many, many more things you should consider when choosing software for any organization. My hope is that this will give you at least some idea of what that process looks like. The rest of this module will deal primarily with how to configure and work with a variety of application servers, including file servers, web servers, and database servers.\n",
    "description": "",
    "tags": null,
    "title": "Application Servers Overview",
    "uri": "/6-application-servers/02-application-servers-overview/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Docker Tutorial for Beginners by TechWorld with Nana on YouTube Docker Overivew from Docker Container Report from Datadog GitHub Codespaces Overview from GitHub Virtual Machine-Based Features in Codio from Codio Video Transcript Up to this point, we’ve been working with traditional virtual machines in our labs. To use a traditional virtual machine, we first install a hypervisor that allows us to run multiple VMs on a single system. That hypervisor may be installed directly on the hardware, as it is in most enterprise settings, or we may install the hypervisor on top of an existing operating system, like we’ve done with VMWare on our own computers. Then, inside of the virtual machine, we install another entire operating system, as well as any of the software we need to run our applications. If we want to have multiple VMs that all run the same operating system, we’ll have to install a full copy of that operating system in each VM, in addition to the operating system that may be installed outside of our hypervisor. Can you spot a possible problem here?\nIn traditional VM architectures, we may end up wasting a lot of our time and resources duplicating the functions of an operating system within each of our virtual machines. This can lead to very bloated virtual machines, making them difficult to move between systems. In addition, we have to manage an entire operating system, complete with security updates, configuration, and more, all within each virtual machine. While there are many tools to help us streamline and automate all of this work, in many ways it can seem redundant.\nThis is where containers come in. Instead of a hypervisor, we install a container engine directly on our host operating system. The container engine creates an interface between the containers and the host operating system, allowing them to share many of the same features, so we don’t need to install a full operating system inside of a container. Instead, we just include the few parts we need, such as the basic libraries and command-line tools. Then, we can add our application directly on top of that, and we’re good to go. Containers are often many times smaller than a comparable virtual machine, making them much easier to move between systems, or we can just choose to run multiple instances of the same container to provide additional redundancy. In addition, as we’ll see later, containers are built from read-only images, so we don’t have to worry about any unwanted changes being made to the container itself - a quick restart and we’re back to where we started! With all of those advantages, hopefully you can see why containers have quickly become one of the dominant technologies for virtualization in industry today.\nIn this course, we’re going to take a deep dive into using Docker to create and manage our containers. Docker is by far the most commonly used container platform, and learning how to work with Docker is quickly becoming a must-have skill for any system administrator, as well as most software developers. So, learning how to use Docker is a great way to build your resume, even if you aren’t planning to work in system administration!\nDocker itself consists of three major parts. First, we have the Docker client, which is either the Docker command-line tool or a graphical interface such as Docker Desktop. We use the client to interface with the rest of the system to create and manage our containers. Behind the scenes, we have the Docker engine, also known as the Docker daemon, which handles actually running our containers as well as managing things such as images, networks, and storage volumes. We’ll spend the next several parts of this module exploring the Docker client and engine. Finally, Docker images themselves are stored on a registry such as Docker Hub, allowing us to quickly find and use images for a wide variety of uses. Many code repositories such as GitHub and GitLab also can act as registries for container images, and you can even host your own!\nNext, let’s look at the structure of a typical container. A container is a running instance of an image, so we can think of a container like an object in object-oriented programming, which is instantiated from an image, acting like a class in this example. An image consists of several layers, which are used to build the file system of the image. Each time we make a change to the contents of an image, such as installing a piece of software or a library, we generate a new layer of the image. This allows us to share identical layers between many images! For example, if we have multiple images that are build using the same Ubuntu base image, we only have to store one copy of the layers of the Ubuntu base image, and then each individual image just includes the layers that were added on top of the base image! This makes it easy to store a large number of similar images without taking up much storage space, and we can even take advantage of this structure to cache layers as we build our own images!\nWhen we create a container from an image, we add a small, temporary read/write layer on top of the image layers. This is what allows us to instantiate the container and have it act like a running system. As soon as we stop the container, we discard the read/write layer, which means that any data added or changed in the container is lost, unless we make arrangements to have it stored elsewhere. So, this helps us maintain a level of confidence that any container started from an image will always be the same, and we can restart a container anytime to revert back to that state.\nSo, as we saw earlier, we can easily instantiate multiple copies of the same image into separate containers. Each of those containers will have their own read/write layer, but they’ll share the same base image. This makes it extremely easy to deploy multiple instances of the same container on a single system, allowing us to quickly build in redundancy and additional capacity quickly.\nWe can also use this to build really interesting infrastructures! Consider developer tools such as GitHub Codespaces or educational tools like Codio - behind the scenes, those platforms are simply instantiating containers based on a shared image that contains all of the development tools and information that the user needs, and once the user is done with their work, the container can be either paused to save computing resources, or it can be destroyed if no longer needed by the user.\nSo, in summary, why should we consider using containers in our infrastructure instead of the virtual machines we’ve been learning about so far? Well, for starters, we know that images for containers are typically much smaller than traditional virtual machines, and the use of layers and de-duplication allows us to further reduce the storage needed by images that share a common ancestor. In addition, since the image itself is somewhat decoupled from the underlying operating system, it becomes very easy to decouple our applications from the operating system it runs on. We’ve also discussed how containers and images are very portable and scalable, so it is very easy to start and run a container just about anywhere it is needed. Finally, as we’ll see later in this module, there are many tools available to help us orchestrate our containers across many systems, such as Kubernetes.\nOne topic we really haven’t talked about yet is sandboxing. This is another great feature of containers that really can’t be overlooked. Consider a traditional web-hosting scenario, where a single server has a webserver such as Apache installed, as well as a database, email server, and more. Since all of those applications are installed on the same operating system, a vulnerability in any one of them can allow an attacker access to the entire system! Instead, if each of those applications is running in a separate container, we can easily isolate them and control how they communicate with each other, and therefore prevent a vulnerability in one application from affecting the others! Of course, this depends on us properly configuring our container engine itself, and as we’ll see later in this lab, it is very easy to unintentionally introduce security concerns unless we truly understand what we are doing.\nAs we’ve already heard in this video, containers are quickly becoming one of the most used tools in system administration today. According to Docker, as of April 2021, there have been over 300 billion individual image downloads from Docker Hub, with 10% of those coming just in the last quarter! There are also over 8.3 million individual image repositories hosted on Docker Hub, meaning that nearly any self-hosted piece of software can probably be found on Docker Hub in some form.\nDatadog also publishes their list of the top technologies running in Docker among their clients. Looking here, there aren’t too many surprises - we see that the Nginx webserver, Redis key-value database, and Postgres relational database are the top three images, with many other enterprise tools and data storage platforms close behind.\nHopefully this gives you a good overview of the architecture of containers and some of the technology behind the scenes. In the next few parts of this lab, we’re going to dive directly into using Docker on our own systems.\n",
    "description": "",
    "tags": null,
    "title": "Architecture",
    "uri": "/5a-containers/02-architecture/index.html"
  },
  {
    "content": " Warning This is the assignment page for Lab 1. It is placed before the rest of the module’s content so you may begin working on it as you review the content. Click Next below to continue to the rest of the module.\nLab 1 - Secure Workstations Instructions Create two virtual machines meeting the specifications given below. The best way to accomplish this is to treat this assignment like a checklist and check things off as you complete them.\nIf you have any questions about these items or are unsure what they mean, please contact the instructor. Remember that part of being a system administrator (and a software developer in general) is working within vague specifications to provide what your client is requesting, so eliciting additional information is a very necessary skill.\nNote To be more blunt - this specification may be purposefully designed to be vague, and it is your responsibility to ask questions about any vagaries you find. Once you begin the grading process, you cannot go back and change things, so be sure that your machines meet the expected specification regardless of what is written here. –Russ\nAlso, to complete many of these items, you may need to refer to additional materials and references not included in this document. System administrators must learn how to make use of available resources, so this is a good first step toward that. Of course, there’s always Google !\nTime Expectation This lab may take anywhere from 1 - 6 hours to complete, depending on your previous experience working with these tools and the speed of the hardware you are using. Installing virtual machines and operating systems is very time-consuming the first time through the process, but it will be much more familiar by the end of this course.\nSoftware This lab is written with the expectation that most students will be using VMware Workstation or VMware Fusion to complete the assignment. That software is available free of charge on the VMware Store open to all K-State CS students, and it is highly recommended for students who are new to working with virtual machines, since most of the assignments in this class are tailored to the use of that platform.\nIf you are using another virtualization platform, you may have to adapt these instructions to fit. If you are unsure about any specification and how it applies to your setup, please contact the instructor.\nYou will also need installation media for the following operating systems:\nWindows 10 Version 22H2 or later - See the Azure Dev Tools page on the CS Support Wiki for instructions. Look for Windows 10 Education, version 22H2 on the list of software available on the Azure Dev Tools site. File Name: en-us_windows_10_consumer_editions_version_22h2_x64_dvd_8da72ab3.iso SHA 256 Hash: f41ba37aa02dcb552dc61cef5c644e55b5d35a8ebdfac346e70f80321343b506 Your file may vary as Microsoft constantly updates these installers. You may choose to upgrade to a later version of Windows 10 while installing updates. You may choose to use Windows 11 for this course. The instructions shown in the course may not exactly match Windows 11 so adaptation may be necessary. Contact the instructor if you have any questions or run into issues. Ubuntu 22.04 LTS (Jammy Jellyfish) or later - Download from Ubuntu or the K-State CS Mirror File Name: ubuntu-22.04.3-desktop-amd64.iso SHA 256 Hash: a435f6f393dda581172490eda9f683c32e495158a780b5a1de422ee77d98e909 If a point release is available (ex: 22.04.4), feel free to us that version. Do not upgrade to a newer LTS or non-LTS release such as Ubuntu 23.04, as those versions may have significant changes that are not covered in these assignments. Note The original course materials were developed for Windows 10 Version 1803 and Ubuntu 18.04 LTS. Some course materials may still show the older versions. Students should use the software versions listed in bold above if at all possible, as these assignments have been verified using those versions. If not, please contact the instructor for alternative options. If you find any errors or issues using the updated versions of these systems, please contact the instructor.\nTask 0: Install Virtualization Software Install the virtualization software platform of your choice. It must support using Windows 10 and Ubuntu 22.04 as a guest OS. In general, you’ll need the latest version of the software.\nVMware Workstation or VMware Fusion is recommended and available free of charge on the VMware Store open to all K-State CS students.\nYou may need to install the latest version available for download and then update it within the software to get to the absolute latest version that supports the latest guest OS versions.\ninfo-123 Unfortunately, the process for getting VMWare licenses changed in 2022, so we have to manually request access for a class. If you have issues accessing this site and getting a license, it is likely that I forgot to send in that request. Please email me and remind me so I can get that done! - Russ\nTask 1: Create a Windows 10 Virtual Machine Create a new virtual machine for Windows 10. It should have 60 GB of storage available. If given the option, do not pre-allocate the storage, but do allow it to be separated into multiple files. This will make the VM easier to work with down the road. It should also have at least 2 GB of RAM. You may allocate more RAM if desired. You may also allocate additional CPU cores for better performance if desired.\nInstall Windows 10 in that virtual machine to a single partition. You may use the express settings when configuring Windows. Do not use a Microsoft account to sign in! Instead, create a local (non-Microsoft) account as defined below. You may also be asked to set the computer name, which is given below.\nNote Windows 10 Version 1903 has made it more difficult to create a local account when installing. See the video later in this module for instructions or refer to this guide from How-To Geek Task 2: Configure Windows 10 Configure the Windows 10 Virtual Machine as specified below.\nComputer Name: cis527w-\u003cyour eID\u003e (example: cis527w-russfeld) Info This is very important, as it allows us to track your virtual machine on the K-State network in case something goes wrong in a later lab. By including both the class and your eID, support staff will know who to contact. A majority of students have missed this step in previous semesters, so don’t forget! The computer name must be changed after the Windows installation is complete –Russ\nPrimary User Account: Username: cis527 | Password: cis527_windows (Member of Administrators \u0026 Users groups) Other User Accounts: AdminUser | AdminPassword123 (Administrators \u0026 Users group) NormalUser | NormalPassword123 (Users group) GuestUser | GuestPassword123 (Guests group only) EvilUser | EvilPassword123 (Users group) Install Software VMware Tools Mozilla Firefox Mozilla Thunderbird IIS Web Server Notepad++ BGInfo Download the Bginfo.exe file and place it on the cis527 user’s desktop. It does not have an installation program. Run it once to see what it does! Verify Windows Defender is running. It should be installed by default. Configure Firewall Make sure Windows Firewall is enabled Allow all incoming connections to port 80 (for IIS) Tip You can test this by accessing the Windows VM IP Address from Firefox running on your Ubuntu VM, provided they are on the same virtual network.\nInstall Windows Updates: Run Windows Update and reboot as necessary until all available updates are installed. Automatic Updates: Make sure the system is set to download and install security updates automatically. Note Even though you may have installed a particular version of Windows, such as 22H2, you should run updates repeatedly until there are no more updates available. You may end up installing at least one major update rollup. Keep going until you are sure there are no more updates to be found.\nTask 3: Windows Files \u0026 Permissions Warning Read the whole task before you start! You have been warned. –Russ\nCreate the folder C:\\docs. It should be owned by the cis527 account, but make sure all other users can read and write to that folder. Within C:\\docs, create a folder for each user created during task 2 except for cis527, with the folder name matching the user’s name. Make sure that each folder is owned by the user of the same name, and that that user has full permissions to its namesake folder. Create a group containing cis527 and AdminUser, and set permissions on C:\\docs for that group to have full access to each folder created in C:\\docs. Tip When you create a group and add a user to that group, it does not take effect until you reboot the computer.\nNo other user should be able to access any other user’s folder. For example, EvilUser cannot access GuestUser’s folder, but AdminUser and cis527 can, as well as GuestUser, who is also the owner of its own folder. In each subfolder of C:\\docs, create a text file. It should have the same access permissions as the folder it is contained in. The name and contents of the text file are up to you. Tip Use either the cis527 or AdminUser account to create these files, then modify the owner and permissions as needed. Verify that they can only be accessed by the correct users by logging in as each user and seeing what can and can’t be accessed by that user, or by using the permissions auditing tab. Many students neglect this step, leaving the file owner incorrect.\nDon’t remove the SYSTEM account or the built-in Administrator account’s access from any of these files. Usually this is as simple as not modifying their permissions from the defaults. See this screenshot and this screenshot for what these permissions should look like in PowerShell. This was created using the command Run Get-ChildItem -Recurse | Get-Acl | Format-List in PowerShell Task 4: Create an Ubuntu 22.04 Virtual Machine Create a new virtual machine for Ubuntu 22.04 Desktop. It should have 30 GB of storage available. If given the option, do not pre-allocate the storage, but do allow it to be separated into multiple files. This will make the VM easier to work with down the road. It should also have at least 2 GB of RAM. You may allocate more RAM if desired. You may also allocate additional CPU cores for better performance if desired.\nNote Ubuntu 22.04 seems to be really RAM hungry right now, so I recommend starting with 2 GB of RAM if you have 8 GB or more available on your system. The installer may freeze if you try to install with only 1 GB of RAM allocated. Once you have it installed, you may be able to reduce this at the expense of some performance if you are short on available RAM (as it will use swap space instead). In Ubuntu, swap should be enabled by default after you install it, but you can learn more about it and how to configure it here . When we get to Module 5 and discuss Ubuntu in the cloud, we’ll come back to this and discuss the performance trade-offs in that scenario. –Russ\nInstall Ubuntu 22.04 Desktop in that virtual machine to a single partition. You may choose to use a minimal install when prompted. You will be asked to create a user account and set the computer name. Use the information given below.\nTip The Ubuntu installation will sometimes hang when rebooting after installation in a VM. If that happens, wait about 30 seconds, then click VM \u003e Power \u003e Restart Guest in VMware (or similar) to force a restart. It should not harm the VM.\nTask 5: Configure Ubuntu 22.04 Configure the Ubuntu 22.04 Virtual Machine as specified below.\nComputer Name: cis527u-\u003cyour eID\u003e (example: cis527u-russfeld) Info This is very important, as it allows us to track your virtual machine on the K-State network in case something goes wrong in a later lab. By including both the class and your eID, support staff will know who to contact. A majority of students have missed this step in previous semesters, so don’t forget! You should be prompted for a computer name as part of the installation process, but it will try to auto-complete it based on the chosen username and must be changed. –Russ\nPrimary User Account: Username: cis527 | Password: cis527_linux (Account should have Administrator type or be in the sudo group) Other User Accounts: adminuser | AdminPassword123 (Administrator type or sudo group) normaluser | NormalPassword123 (Normal type) guestuser | GuestPassword123 (Normal type) eviluser | EvilPassword123 (Normal type) Install Software Open VM Tools (open-vm-tools-desktop) (recommended) -OR- VMware Tools (do not install both) Mozilla Firefox (firefox) Mozilla Thunderbird (thunderbird) Apache Web Server (apache2) Synaptic Package Manager (synaptic) GUFW Firewall Management Utility (gufw) ClamAV (clamav) Configure Firewall Make sure Ubuntu Firewall (use ufw, not iptables) is enabled Allow all incoming connections to port 80 (for Apache) Tip You can test this by accessing the Ubuntu VM IP Address from Firefox on your Windows VM, provided they are on the same virtual network.\nInstall Updates: Run system updates and reboot as necessary until all available updates are installed. Automatic Updates: Configure the system to download and install security updates automatically each day. Task 6: Ubuntu Files \u0026 Permissions Warning Read the whole task before you start! You have been warned. –Russ\nCreate a folder /docs (at the root of the system, not in a user’s home folder). Any user may read or write to this folder, and it should be owned by root:root (user: root; group: root). Within /docs, create a folder for each user created during task 5 except for cis527, with the folder name matching the user’s name. Make sure that each folder is owned by the user of the same name, and that that user has full permissions to its namesake folder. Create a group and set permissions on each folder using that group to allow both cis527 and adminuser to have full access to each folder created in /docs. Tip When you create a group and add a user to that group, it does not take effect until you reboot the computer.\nNo other user should be able to access any other user’s folder. For example, eviluser cannot access guestuser’s folder, but adminuser and cis527 can, as well as guestuser, who is also the owner of its own folder. In each subfolder of /docs, create a text file. It should have the same access permissions as the folder it is contained in. The name and contents of the text file are up to you. Tip Use either the cis527 or adminuser account to create these files, then modify the owner, group, and permissions as needed. Verify that they can only be accessed by the correct users by logging in as each user and seeing what can and can’t be accessed by that user, or by using the su command to become that user in the terminal. Many students neglect this step, leaving the file owner incorrect.\nSee this screenshot for what these permissions may look like in Terminal. This was created using the command ls -lR in the Linux terminal. Task 7: Make Snapshots For each of the virtual machines created above, create a snapshot labelled Lab 1 Submit in your virtualization software before you submit the assignment. The grading process may require making changes to the VMs, so this gives you a restore point before grading starts.\nTask 8: Schedule A Grading Time Contact the instructor and schedule a time for interactive grading. You may continue with the next module once grading has been completed.\n",
    "description": "",
    "tags": null,
    "title": "Assignment",
    "uri": "/1-secure-workstations/02-assignment/index.html"
  },
  {
    "content": "Final Project Instructions The final project in this course is to demonstrate your knowledge and experience in a variety of areas of system administration. In addition, you’ll gain experience communicating about those areas with non-technical users.\nAudience For this project, your intended audience is primarily C-suite executives (CEO, CIO, CFO, President, Vice-President etc.) of the organization. Some of them will have a technical background, some will not. So, you’ll have to write and present accordingly.\nStep 0: Teamwork If you so choose, you may team up with another student to complete this project. Please let the instructor know via email if you plan on working on a team for this project. Teams will be expected to produce roughly twice the quantity of work described below, so plan to scale the scope of your project accordingly.\nNote For example, a 15 minute presentation becomes a 30 minute presentation. Likewise, your proposal and report should be twice as long as it would be if only one person was working on it. –Russ\nStep 1: Proposal First, you’ll need to create and submit a proposal for your project. Your proposal should fit in one of the major topic areas below:\nHow would you change a particular aspect of K-State’s IT infrastructure? (You may substitute another organization you are more familiar with in place of K-State, such as your current employer or a past internship) How would you architect the technology resources for a new, cloud-based startup in the domain area of your choice? (That is, you must clearly describe what the company does as well as how you’d structure their resources.) How would you build an effective computer support infrastructure for K-State. (You may substitute another organization you are more familiar with in place of K-State, such as your current employer or a past internship. However, it should be of sufficient size to warrant a formal support infrastructure.) How would you develop an automated devops stack for a particular programming project or organization? Submit your 1 page proposal via K-State Canvas for approval. Once it is approved, you may start working on the project itself. Make sure you have appropriately scaled your proposal to the amount of work below; if the proposal is too simple or too complex it may be difficult to complete everything listed below.\nWarning Do not wait until the last minute to submit your proposal! To make that point clear, the proposal is due typically one to two weeks before the end of the course, giving you ample time to complete the project.\nStep 2: Design \u0026 Prototype The next step of the project is to design your proposed system and develop a small prototype showing some aspect of it in practice. In this phase, you’ll clearly describe the proposed solution in as much technical detail as you feel is needed. It should be very clear that you’ve carefully considered each element of the solution, and have taken steps to make sure it is clear and easy to understand.\nIf you do any research, make sure you include the relevant links, papers, and resources in your write-up described below. As with any academic work, any information you use that is not of your own creation should be appropriately cited.\nYou may develop your prototype using virtual machines, cloud resources, or any other infrastructure available to you. If you are unsure of the appropriate scope of the prototype, please contact the instructor to discuss your options.\nStep 3: SWOT Analysis In addition, you will perform a SWOT (Strengths, Weaknesses, Opportunities, Threats) analysis of your proposed solution and associated prototype. Be very blunt and honest in each section, as you should be able to show the level of depth of your technical expertise, the lessons learned through building the prototypes, and any analysis you have performed.\nTip There are NO proposals without several external threats and internal weaknesses, no matter how good it is. If you can’t find any, think harder! :^) –Russ\nStep 4: Technical Writeup Once you feel you have fully designed and analyzed your proposal and prototype, you will need to create a formal technical paper detailing your work. It should follow the Standard Technical Paper Template from Dr. Goddard at UNL. Hopefully the content in each section should be pretty self-explanatory. There is no minimum or maximum length specified; it should be long enough to include everything needed to describe your proposal and analysis, but short and succinct enough that it is still worth reading.\nNote The paper template uses bullets to show what belongs in each section of the paper, but your paper SHOULD NOT consist solely of bullet points.\nStep 5: Presentation Once you are ready, you will present your proposal, prototype, and analysis to your audience. The presentation will be a short video recorded using any tools you prefer. It should be no shorter than 7 minutes, and no longer than 15 minutes for a single presenter. You’ll need to clearly explain the relevant background, the proposal, the analysis, and your conclusions based on that analysis. In short, you’re trying to sell the audience on this proposal and convince them to consider adopting it, so you’ll need to not only focus on presenting the information, but persuading the audience to be in favor of the change. Any slides, graphics, or other elements to make the presentation more persuasive is encouraged.\nStep 6: Submit \u0026 Schedule Finally, submit your writeup and video via Canvas.\nAs with any assignment in this course, if you have any questions at all please contact the instructor or post in the course discussion channel.\n",
    "description": "",
    "tags": null,
    "title": "Assignment",
    "uri": "/8-final-project/02-assignment/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Configuration Management on Wikipedia Configuration Management from Puppet Modern Configuration Management: Configuration as Code from Chef Use Case: Configuration Management from Ansible Configuration Management from MITRE Systems Engineering Guide SaltStack Enterprise from SaltStack DevOps on Wikipedia What Is DevOps? From The Agile Admin What is this DevOps Thing, Anyway? From Just Enough Developed Infrastructure (JEDI) Periodic Table of DevOps Tools from XebiaLabs Video Script This module deals with the topic of configuration management. Before I introduce that topic directly, let’s look at what we’ve done so far in this class.\nIn Lab 1, you were asked to set up and configure a single operating system VM for both Windows and Linux. For many system administrators, that is exactly how they operate on real systems - each one is set up by hand, one at a time, manually. This process, of course, does have some benefits. For the sysadmin, they get hands-on management with each computer they support. In addition, they can easily customize each system to meet the individual needs of the user. For sysadmins with a low level of experience, or few resources, this can be a very easy way to at least help users configure their systems. And, in practice, this method works well for small groups of users. If you are only supporting a few users, maybe even up to several dozen, manually managing systems and configurations may be the most efficient and cost-effective way to do so.\nHowever, there are some major downsides to this as well. First and foremost, it is a very time consuming and labor intensive process. You probably experienced that while working on the first lab. Depending on your own experience and familiarity with those systems and the tasks at hand, it may have taken anywhere from one to several hours to complete. While you can, of course, become more proficient, it may still take quite a bit of hands-on time to configure an operating system. In addition, there is the high likelihood of machines being configured inconsistently, leading to additional support headaches down the road. I can confidently say from my time working as a sysadmin and from grading labs in this class, even with the same simple instructions, it is very difficult for any two people to configure a computer in exactly the same way. Also, as new software and operating system updates are released, you’ll find that you need to install them on a per-machine basis, or depend on the end users to do those updates themselves. Finally, as your groups get larger, this approach really doesn’t scale well at all.\nSo, that leads to the big question - how do we make this process scalable?\nOne way to do this is through the use of automation tools, such as GNU Make. While Make was developed to help with compiling large pieces of code, it shares many similarities with the tools we’ll look at in this module. Many system administrators also wrote scripts to automate part of their process, but in most cases those scripts were simply a list of steps or commands that the sysadmin would run manually, and they would have to be customized to fit each particular operating system or configuration.\nThere are also a couple of techniques that can be used to scale this process. One is through the use of system images. Much like the virtual machines we are using in this class, you can actually store the entire hard drive of a computer as a system image and copy it as many times as you’d like to machines. However, this process is also very time-consuming since the images are so big, and they can only be copied onto the same or similar hardware configurations. This is great when everyone has the same hardware, but in many organizations that simply isn’t the case. Lastly, you can also use tools to create custom operating system installers for both Windows and Linux, with much of the configuration and software pre-installed and configured along with the operating system. Again, this is very time consuming, and it must be redone and updated every time something changes. In addition, none of these methods really address how to handle updates and new software once the system is in the hands of the users. In short, we need something better.\nEnter the concept of a defined configuration. Imagine this: what if we could write out exactly how we would like our computers to be configured, and then direct our staff to make sure the system matched that configuration, regardless of the operating system or hardware? That configuration would be a list of configured items only, not a set of steps to accomplish that task. In essence, much of the first lab assignment could be thought of as a defined configuration. Those items are high level, system independent, and hopefully just enough information that any competent system administrator could build a system that meets those requirements. If that is the case, could we build a software tool to do the same?\nThis is the big concept behind configuration management. As a system administrator, all you have to do is define your desired configuration in a way that these tools can understand it, then they will do the rest. In that way, you can think of your system configuration as just another piece of “source code” and manage it just like any other code. If you need to deploy a new piece of software or update one, simply change the configuration file and every system using the tool will make the necessary changes to match. By doing so, this will greatly reduce downtime and errors and increase the consistency of all the systems you are managing. According to one estimate, 70% of all downtime in datacenters is due to human error, so the more we can take humans out of the equation, the better.\nThere are many tools available for configuration management. For this class, we’ll be using Puppet as it is one of the easiest tools to work with in my experience, and it is available for a variety of platforms and uses. If you are interested in using configuration management tools in your own work, I encourage you to review each of these platforms, as each one offers unique features and abilities.\nAs we discuss automation tools, I briefly want to bring up the term DevOps, since it is really one of the big things you’ll hear discussed on the internet today. DevOps is a shortened form of “development operations” and is, in essence, a very close collaboration between software development and system administration staff. A good way to think of this is the application of “agile” software development practices to the world of system administration, so that the systems can be as “agile” (pun intended) as the developers want them to be. This involves a high level of automation and monitoring, both in terms of configuration management but also deployment and testing. There are also the very short development cycles traditionally seen in agile development, with increased deployment frequency. We’ll spend a little bit of time talking about DevOps later in the course, but many of the concepts we are covering here in configuration management are very applicable there as well.\nFinally, here are a few tools from the DevOps world you may come across. Some of the biggest ones, Jenkins and Travis, are used for build and test automation, while tools such as Nagios and Icinga are helpful for system monitoring. Feel free to check out any of these tools on your own time, as you may find several of them very useful.\n",
    "description": "",
    "tags": null,
    "title": "Configuration Management Overview",
    "uri": "/2-configuration-management/02-configuration-management-overview/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Extras - Git CS GitLab DevOps on Wikipedia Infrastructure as Code on Wikipedia What is DevOps? from Amazon Web Services What Is DevOps? from The Agile Admin DevOps: Breaking the Development-Operations Barrier from Atlassian Periodic Table of DevOps Tools from XebiaLabs The Ultimate DevOps Tool Chest from XebiaLabs webhook on GitHub webhook Hook Examples on GitHub Webhooks on GitLab Documentation Video Transcript One of the hottest new terms in system administration and software development today is DevOps. DevOps is a shortened form of “Development Operations,” and encompasses many different ideas and techniques in a single term. I like to think of it as the application of the Agile software development process to system engineering. Just as software developers are focusing on more frequent releases and flexible processes, system administrators have to react to quickly changing needs and environments. In addition, DevOps engineers are responsible for automating much of the software development lifecycle, allowing developers to spend more time coding and less time running builds and tests manually. Finally, to accommodate many of these changes, DevOps engineers have increasingly adapted the concept of “infrastructure as code,” allowing them to define their system configurations and more using tools such as Puppet and Ansible in lieu of doing it manually. As we saw in Module 2, that approach can be quite powerful.\nAnother way to think of DevOps is the intersection of software development, quality assurance, and operations management. A DevOps engineer would work fluidly with all three of these areas, helping to automate their processes and integrate them tightly together.\nThere are many different ways to look at the DevOps toolchain. This graphic gives one, which goes through a process of “plan, create, verify, package, release, configure, and monitor.” The first four steps deal with the development process, as developers work to plan, test, and package their code for release. Then, the operations side takes over, as the engineers work to make that code widely available in the real world, dealing with issues related to configuration and monitoring.\nFor this lecture, I’ll look at a slightly modified process, consisting of these seven steps. Let’s take a look at each one in turn and discuss some of the concepts and tools related to each one.\nFirst is code. As software developers work to create the code for a project, DevOps engineers will be configuring many of the tools that they use. This could be managing a GitHub repository, or even your own GitLab instance. In addition, you could be involved in configuring their development and test environments using tools such as Vagrant and Puppet, among others. Basically, at this phase, the responsibility of a DevOps engineer is to allow developers to focus solely on creating code, without much worry about setting up the environments and tools needed to do their work.\nThe next steps, build and test, are some of the most critical for this process. As developers create their code, they should also be writing automated unit tests that can be run to verify that the code works properly. This is especially important later on, as bugfixes may inadvertently break code that was previously working. With a properly developed suite of tests, such problems can easily be detected. For a DevOps engineer, the most important part is automating this process, so that each piece of code is properly tested and verified before it is used. Generally, this involves setting up automated building and testing procedures using tools such as Travis and Jenkins. When code is pushed to a Git repository, these tools can automatically download the latest code, perform the build process, and run any automated tests without any user intervention. If the tests fail, they can automatically report that failure back to the developer.\nOnce the product is ready for release, the package and release processes begin. Again, this process can be fully automated, using tools in the cloud such as Heroku, AWS, and Azure to make the software available to the world. In addition, there are a variety of tools that can be used to make downloadable packages of the software automatically. For web-based applications, this process could even be chained directly after the test process, so that any build that passes the tests is automatically released to production.\nOf course, to make a package available in production requires creating an environment to host it from, and there are many automated configuration tools to help with that. We’ve already covered Puppet in this class, and Ansible and SaltStack can perform similar functions in the cloud. In addition, you may choose to use container tools such as Docker to make lightweight containers available, making deployment across a variety of systems quick and painless.\nLastly, once your software is widely available, you’ll want to continuously monitor it to make sure it is available and free of errors. There are many free and paid tools for performing this task, including Nagios, Zabbix, and Munin. As part of this lab’s assignment, you’ll get to set up one of these monitoring tools on your own cloud infrastructure, just to see how they work in practice.\nOf course, one of the major questions to ask is “Why go to all this trouble?” There are many reasons to consider adopting DevOps practices in any organization. First and foremost is to support a faster release cycle. Once those processes are automated, it becomes much easier to quickly build, test, and ship code. In addition, the whole process can be more flexible, as it is very easy to change a setting in the automation process to adapt to new needs. This supports one of the core tenets of the agile lifecycle, which focuses on working software and responding to change. In addition, DevOps allows you to take advantage of the latest developments in automation and virtualization, helping your organization stay on top of the quickly changing technology landscape. Finally, in order for your environment to be fluidly scalable, you’ll need to have a robust automation architecture, allowing you to provision resources without any human interaction. By adopting DevOps principles, you’ll be ready to make that leap as well.\nFinally, to give you some experience working with DevOps, let’s do a quick example. I’ve named this the “Hello World” of DevOps, as I feel it is a very good first step into that world, hopefully opening your eyes to what is possible. You’ll perform this activity as part of the assignment for this lab as well.\nFirst, you’ll need to create a project on the K-State CS GitLab server, and push your first commit to it. You can refer to the Git video in the Extras module for an overview of that process. I’m going to use an existing project, which is actually the source project for this course. We’ll be using the Webhooks feature of this server. When certain events happen in this project, we can configure the server to send a web request to a particular URL with the relevant data. We can then create a server to listen for those requests and act upon them.\nYou will also need to either configure SSH keys for your GitLab repository, or configure the repository to allow public access.\nNext, we’ll need to install and configure the webhook server on our DigitalOcean droplet. It is a simple server that allows you to listen for those incoming webhooks from GitHub and GitLab, and then run a script when they are received. Of course, in practice, many times you’ll be responsible for writing this particular piece yourself in your own environment, but this is a good example of what it might look like.\nOn my Ubuntu cloud server, I can install webhook using APT:\nsudo apt update sudo apt install webhook Next, I’ll need to create a file at /etc/webhook.conf and add the content needed to create the hook:\n[ { \"id\": \"cis527online\", \"execute-command\": \"/home/cis527/bin/cis527online.sh\", \"command-working-directory\": \"/var/www/bar/html/\", \"response-message\": \"Executing checkout script\", \"trigger-rule\": { \"match\": { \"type\": \"value\", \"value\": \"f0e1d2c3b4\", \"parameter\": { \"source\": \"header\", \"name\": \"X-Gitlab-Token\" } } } } ] You can find instructions and sample hook definitions on the webhook documentation linked in the resources section below this video. You’ll need to configure this file to match your environment. Also, you can use this file to filter the types of events that trigger the action. For example, you can look for pushes to specific branches or tags.\nSince this file defines a script to execute when the Webhook is received, I’ll need to create that script as well:\n#!/bin/bash git pull exit 0 This script is a simple script that uses the git pull command to get the latest updates from Git. I could also place additional commands here if needed for this project.\nOnce I create the script, I’ll need to modify the permissions to make sure it is executable by all users on the system:\nchmod a+x bin/cis527online.sh Once everything is configured, I can restart the webhook server using this command:\nsudo systemctl restart webhook Then, if everything is working correctly, I can test it using my cloud server’s external IP address on port 9000, with the correct path for my hook. For the one I created above, I would visit http://\u003cip_address\u003e:9000/hooks/cis527online. You should see the response Hook rules were not satisfied displayed. If so, your webhook server is up and running. Of course, you may need to modify your firewall configuration to allow that port.\nLastly, if my repository requires SSH keys, I’ll need to copy the public and private keys into the root user’s .ssh folder, which can be found at /root/.ssh/. Since webhook runs as a service, the Git commands will be run as that user, and it’ll need access to that key to log in to the repo. There are more advanced ways of doing this, but this is one way that works.\nI’ll also need to download a copy of the files from my Git repository onto this system in the folder I specified in webhook.conf\nsudo rm /var/www/bar/html/* sudo git checkout \u003crepository_url\u003e /var/www/bar/html Now, we can go back to the K-State CS GitLab server and configure our webhook. After you’ve opened the project, navigate to Settings and then Integrations in the menu on the left. There, you can enter the URL for your webhook, which we tested above. You’ll also need to provide the token you set in the webhook.conf file. Under the Trigger heading, I’m going to checkmark the “Push events” option so that all push events to the server will trigger this webhook. In addition, I’ll uncheck the option for “Enable SSL verification” since we have not configured an SSL certificate for our webhook server. Finally, I’ll click Add webhook to create it.\nOnce it is created, you’ll see a Test button below the list of existing webhooks. So, we can choose that option, and Select “Push events” to test the webhook. If all works correctly, it should give us a message stating that the hook executed successfully.\nHowever, the real test is to make a change to the repository, then commit and push that change. Once you do, it should automatically cause the webhook to fire, and within a few seconds you should see the change on your server. I encourage you to test this process for yourself to make sure it is working correctly.\nThere you go! That should give you a very basic idea of some of the tools and techniques available in the DevOps world. It is a quickly growing field, and it is very useful for both developers and system administrators to understand these concepts. If you are interested in learning more, I encourage you to read some of the materials linked in the resources section below this video, or explore some of the larger projects hosted on GitHub to see what they are doing to automate their processes.\n",
    "description": "",
    "tags": null,
    "title": "DevOps",
    "uri": "/7-backups-monitoring-devops/02-devops/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Active Directory on Wikipedia Lightweight Directory Access Protocol (LDAP) on Wikipedia Samba (Software) on Wikipedia NetIQ eDirectory on Wikipedia (previously Novell Directory Services) Overview of Novell Directory Services from Novell Kerberos (Protocol) on Wikipedia RFC 1510 - Kerberos from IETF Directories, Directory Services, and LDAP from Apache Understanding Workgroups and Domains from eTutorials.org Video Transcript This video presents a brief overview of directory services and related concepts. This information is very helpful in understanding the larger role that many of these services play in modern computer networks.\nAt its core, a directory service is simply a piece of software that can store and retrieve information. The information stored could take many forms, including user accounts, groups, system information, resources, security settings, and more. By providing a centralized location to store that information, it becomes much simpler to share that information across a large enterprise network.\nWe’ve already worked with one example of a directory service in this course. The modern Domain Name Service is, at its core, a directory service. It stores a directory of domain names matched to IP addresses, as well as additional information about a domain such as the location of the mail server. It can then provide that information to the systems on a large network, providing a level of consistency that is difficult to achieve if each computer has to store a local version of the same information.\nDirectory services for computers have also been around for quite a while. This page gives a brief timeline of events, starting in the late 1980s and culminating with the release of Microsoft’s Active Directory Domain Services in 1999. Let’s look at each of these items in detail.\nFirst, the X.500 set of standards was released in 1988, and was one of the first projects that aimed to provide a consistent directory of user account information. It was actually developed as a directory for the X.400 set of standards for email, which at that time was the dominant way that email was handled on the internet. Those standards were developed using the OSI networking protocols. However, with the rise of the TCP/IP set of internet protocols, these systems were quickly made obsolete. X.400 was replaced by the SMTP standards used today to send email between MTAs, while X.500 forms the basis of many later protocols. What is most notable about X.500 is that it actually defined several underlying protocols, including a Directory Access Protocol (DAP), as well as a Directory System Protocol (DSP) and others. Today, we primarily use protocols based on the original DAP defined by X.500.\nSoon after the release of the X.500 standards, work began on an implementation of the DAP protocol using the TCP/IP internet protocols. It was eventually named the Lightweight Directory Access Protocol, or LDAP. LDAP is the underlying protocol used by many of todays directory services, including Microsoft Active Directory.\nHowever, it is very important to understand that LDAP is not a complete implementation of the original X.500 DAP. Instead, it implements a core set of features, and then adds some additional features not present in the original protocol, while several parts of the original protocol are not implemented. The maintainers of the X.500 standard are very quick to point out this difference, but in practice most systems fully support LDAP as it stands today.\nLDAP also has uses beyond just a directory service for user accounts on workstations. For example, LDAP can be used to provide a user lookup service to an email client, or it can be linked with a website to provide user authentication from a central repository of users.\nLDAP, along with many other directory services, defines a tree structure for the data. In this way, the data can be organized in a hierarchical manner, closely resembling the real relationships between people, departments, and organizations in an enterprise. For example, in this diagram, you can clearly see nodes in the tree for the overarching organization, the organizational unit, and the individual user.\nLDAP uses several short abbreviations for different fields and types of data, and at times they can be a bit confusing to new system administrators. This slide lists a few of the most common ones, as well as a good way to remember them. For example, an ou in LDAP is an organizational unit, which could be a department, office, or some other collection of users and systems.\nYou can see these abbreviations clearly in this sample LDAP entry. Here, we can see the distinguished name includes the canonical name, as well as the domain components identifying its location in the tree. We can also see the common name and surname for this user, as well as the distinguished name for his supervisor. As you work with LDAP throughout this module, you’ll see data presented in this format several times.\nSwitching gears a bit, in the early 1990s, there were many different software programs developed to help share files and users between systems. One of the first was Samba. Samba is an open source project that originally reverse engineered the SMB (hence the name) and CIFS file sharing protocols used by Windows. By doing so, they were able to enable file sharing between Linux based systems, including Macs, and Windows. Eventually, Microsoft worked with the developers of the project to allow them to fully implement the protocol, and in more recent versions, a computer running Samba can even act as part of an Active Directory domain. We’ll work with Samba a bit later this semester when we deal with application servers.\nAround the same time, Novell released their Directory Services software. This was one of the first directory services targeted at large organizations, and for many years it was the dominant software in that area. It was built on top of the IPX/SPX internet protocols, so it required computers to run special software in order to use the directory service.\nMuch like the LDAP example earlier, NDS also supported a tree structure for storing information in the directory. It also had some novel features for assigning permissions across different users and ous, which made it a very powerful tool. Many older system administrators have probably worked with Novell in the past. I remember that both my high school, as well as some departments at K-State, used NDS until they switched to Microsoft Active Directory in the mid 2000s.\nBefore we discuss Active Directory, we should first discuss Windows Workgroups and Homegroups. Most home networks don’t need a full directory service, but it is very helpful to have a way to share files easily between a few computers connected to the same network. Prior to Windows 7, Windows used a “Workgroup” to share these files.\nComputers with the same Workgroup name would be able to discover each other across a local network, and any shared files or resources would be available. However, user accounts were not shared, so you wanted to access a resource shared from a particular computer, you’d have to know a username and password for that system as well. For smaller networks, this usually wasn’t a problem at all. However, as the network grows larger, it becomes more and more difficult to maintain consistent user accounts across all systems.\nThat all changed in 1999 with the introduction of Microsoft Active Directory. It coincided with the release of Windows 2000, one of the first major Windows releases directly targeted at large enterprises. Active Directory uses LDAP and Kerberos to handle authentication, and provides not only a central repository of user accounts and systems, but it can also be used to share security configurations across those systems as well. It was really ideal for large enterprise networks, and gave administrators the level of control they desired. Active Directory quickly became the dominant directory service, and most enterprises today use Active Directory to manage user accounts for their Windows systems. In fact, both K-State and K-State Computer Science primarily use Active Directory to manage their user accounts across a variety of systems and platforms.\nWith a shared domain, the user account information is stored on the central domain controller, instead of the individual workstations. In this way, any user who has a valid account on the domain and permissions to access a resource can do so, regardless of which system they are connected to. This is what allows you to use the same user account to log on to computers across campus, as well as online resources such as webmail and Canvas. One of the major activities in this lab will be to install and configure an Active Directory Domain, so you can get first-hand experience of how this works.\nLastly, no discussion of directory services would be complete without discussing Kerberos. Kerberos is named after Cerberus, the three-headed dog from ancient Greek mythology. It allows a client and a server to mutually authenticate each other’s identity through a third party, hence the three-headed mascot.\nKerberos was originally developed at MIT in the 1980s, and fully released in 1993 as RFC 1510. It is one of the primary ways that directory services today handle authentication, and it is used by many other types of security software on the internet today.\nThe Kerberos protocol is quite complex, and it uses many different cryptographic primitives to ensure the communication is properly secured and authenticated. However, this diagram gives a quick overview of the protocol and how it works.\nFirst, the client will communicate with the authentication server. The server responds with two messages, labelled A and B in this diagram. Message A is encrypted by the server using the client’s key. When the client receives that message, it attempts to decrypt it. If it is successful, it can use the contents inside the message to prove its identity. Then, it can send two new messages, C and D, to the ticket granting server, which in practice is usually the authentication server, but it doesn’t have to be. Message C includes the contents of message B along with information about what the client would like to access, and message D is the decrypted contents of the original message A. The ticket granting server then looks at those messages and confirms the user’s identity. If it is correct, it will then send two more messages, E and F, to the client. Message E is the ticket that can be given to the server containing the desired resource, and message F contains a key that can be used to communicate with that server. So, the client finally can contact the destination server, sending along message E, the ticket from earlier, as well as a new message G that is protected with the key contained in message F. Finally, the server decrypts message G and uses it to create and send back message H, which can be confirmed by the client to make sure the server is using the same key. If everything works correctly, both the client and server know they can trust each other, and they have established keys that allow them to communicate privately and share the requested information. Kerberos is a very powerful protocol, and if you’d like to learn more, I highly recommend reading the linked resources below this video. Kerberos is also discussed in several of the cybersecurity courses here at K-State.\nThat’s all the background information you’ll need for this module. Next, you’ll work on installing Microsoft Active Directory on Windows and OpenLDAP on Ubuntu, as well as configuring clients to use those systems for authentication.\n",
    "description": "",
    "tags": null,
    "title": "Directory Services Overview",
    "uri": "/4-directory-services/02-directory-services-overview/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello and welcome to the Week Three announcements video for CIS 527 and CC 510 in Fall 2023. So this week lab One is due tomorrow by 07:00 P.m., so make sure you get that done. Remember, for the lab grading, you need to schedule an interactive grading time with either Matt, My, UTA or myself via calendar. Both of our calendars are open. You can schedule a time anytime. However, please bear in mind that most of the time you have to schedule a meeting at least 2 hours in advance. So don’t wait until tomorrow afternoon because we may be all full. And then you’ll have to schedule for Thursday so make sure you get that scheduled ASAP. The lab is due before 07:00 P.m. Just to account for some late stuff in case people schedule for after work. Also, don’t forget that the Week Two quizzes are due next week. So on September 11 on Monday, make sure you get those done. And then lab Two is going to be due on September 20. We’re also shooting on having a discussion around that same time as well and I’ll briefly talk about that.\nSo the discussion sessions have been set for Thursdays from one to 02:00 p.m.. Most weeks those will just be office hours while I will sit in zoom for a while and answer any questions and that may also be there as well. I have sent out speaker invites to get a few speakers from industry to come in and I’m hoping to schedule our first speaker soon, probably within the next couple of weeks. And so as soon as I get the first few speakers nailed down, I will post that schedule, and then I will update the assignments on canvas, where you can go in and ask a few questions beforehand and then also have the assignment afterwards, where you can get participation credit if you’re not able to attend the live discussion. But for right now, the Thursdays are just office hours, so feel free to drop by anytime if you have any questions on the lab.\nSo you’re going to start working on lab two. Lab two is basically redoing lab One using Puppet. So what you’re going to do is create two new virtual machines, reinstall the operating system, you’re going to install the Puppet agent and you’re going to make a snapshot. After you’ve done all of that, I highly encourage you to reinstall the operating system and run all of the updates for the operating system, it may take several times in both Windows and Linux to get all of the updates. Then install Puppet and then make your snapshot. It’s really, really important that you remember to make your snapshot at that time before you’ve done any other setup so that you can write your Puppet Manifest and test it and then roll it back to that snapshot. Every semester I have somebody do this lab forget to make that snapshot, and then they ask me if there’s any way to undo it, and I have to tell them, no. You get to reinstall your VMs, so make sure you remember to make that snapshot the Puppet manifest. You’re just making a manifest file to create users files and do a few other things. Try and keep it very simple. In the videos, I show some ways that you can use Puppet Resource to query your system to see what the setup is currently. So, for example, you could manually add some of those users. Use Puppet Resource to see what that looks like and then trim down that output to create the user accounts that you want.\nI do have model solutions for these. The model solutions are less than 200 lines of code per operating system. It is possible to create one unified file that works for both operating systems, but in general, I recommend creating one for Ubuntu and one for Windows. They’re going to be similar, but they’ll have some differences, so it makes it a lot easier to keep track. In this lab, you can also use anything from the Puppet Library. There are a couple of times where I show you how to use different library things. If you do use a library from Puppet, make sure you put in comments at the top of your Puppet manifest. What library needs to be installed if you use the Utils Library or Standard Library or the Windows File Permissions Library, anything like that. So make sure you make note of that so that when we’re grading those, we know that we need to install those. And finally, for lab two, the grading for lab two is done offline. So all you have to do is submit your two Puppet Manifest files via Canvas, and then Matt and I will go through and grade those after. The lab is due in a couple of weeks. So you don’t have to schedule a grading time for lab two unless you want to. If you want us to go through it manually, we can definitely do that as well. So that’s really all I’ve got for this week. It’s a pretty short week.\nHopefully things are going well. As always, if you have any questions in this class, you can keep up with us by joining the discussions on Edstem. You can also join Office hours on Thursdays. There’s lots of different ways you can get in touch with both Matt and I. So just let us know if you have any questions. Otherwise, I hope things are going well. I know that it kind of feels like you have to do this lab again, but that’s really the point, is to show you the benefit of doing this manually versus the benefit of doing this in puppet. So hopefully things go well with lab two. Let us know if you have any questions and I will see you again in a couple of weeks.\n",
    "description": "",
    "tags": null,
    "title": "Fall '23 Week 3",
    "uri": "/y-announcements/week03/index.html"
  },
  {
    "content": "Greetings! Here is the information you’ll need to know to get your lab graded.\nGrading Date/Time: \u003ctime\u003e Contact me ASAP if that time will not work for you for any reason and I will work with you to reschedule. Zoom Link: \u003clink\u003e If you haven’t used Zoom before, I recommend going to https://ksu.zoom.us/ first to download the Zoom client software. Then, when the time comes, click the link above to join the Zoom session. Your Setup: Before you join the Zoom session for grading, please have the following steps completed on your end. This helps things go as smoothly as possible: Use the fastest internet connection available to you. If you are using a laptop with a wireless connection, you may want to plug in to a hard-wired connection if available. If you can work from campus, that may be best, depending on your home internet connection. Make sure your audio input device (microphone) is working in Zoom. You are not required to have a webcam, but you are welcome to use one if you like. I will be using a webcam and microphone on my end. Confirm that all the VMs for this lab are booted and running on your system. We’ll be using the desktop sharing features of Zoom so I can see your VMs. So, make sure you close any windows or programs that are running in the background that shouldn’t be shared with me. Operations: Here are a few operations you may be asked to demonstrate: PowerShell (in C:\\files) \u003e Get-ChildItem -Recurse | Get-Acl | Format-List Terminal (in /files) \u003e ls -lR Finality Once you begin the grading process, no changes may be made to your system to fix any errors encountered. The grade you receive will reflect the state of your VMs at the start of the grading process. This is to ensure that grades are fairly determined and that no special consideration is given. That should cover everything. Please let me know if you have any questions prior to your scheduled grading time.\nGood luck!\nRuss\n",
    "description": "",
    "tags": null,
    "title": "Lab 1 Grading Email",
    "uri": "/z-instructor-resources/02-lab-1-grading-email/index.html"
  },
  {
    "content": " YouTube Video Video Script This course makes extensive use of several features of Canvas which you may or may not have worked with before. To give you the best experience in this course, this video will briefly describe those features and the best way to access them.\nWhen you first access the course on Canvas, you will be shown this homepage, with my contact information and any important information about the course. This is a quick, easy reference for you if you ever need to get in touch with me.\nLet’s walk through the options in the main menu to the left. First, any course announcements will be posted in the Announcements section, which is available here. Those announcements will also be configured to send emails to all students when they are posted, though in your personal Canvas settings you can disable email notifications if you so choose. Please make sure you check here often for any updates to course information.\nThe next section is Modules, which is where you’ll primarily interact with the course. You’ll notice that I’ve disabled several of the common menu items in this course, such as Files and Assignments. This is to simplify things for you as students, so you remember that all the course content is available in one place.\nWhen you first arrive at the Modules section, you’ll see all of the content in the course laid out in order. If you like, you can minimize the modules you aren’t working on by clicking the arrow to the left of the module name.\nAs you look at each module, you’ll see that it gives quite a bit of information about the course. At the top of each module is an item telling you what parts of the module you must complete to continue. In this case, it says “Complete All Items.” Likewise, the following modules may list a prerequisite module, which you must complete before you can access it.\nWithin each module is a set of items, which must be completed in listed order. Under each item you’ll see information about what you must do in order to complete that item. For many of them, it will simply say “view,” which means you must view the item at least once to continue. Others may say “contribute,” “submit,” or give a minimum score required to continue. For assignments, it also helpfully gives the number of points available, and the due date.\nLet’s click on the first item, Course Introduction, to get started. You’ve already been to this page by this point. Most course pages will consist of an embedded video, followed by links to any resources used or referenced in the video, including the slides and a downloadable version of the video. Finally, a rough video script will be posted on the page for your quick reference.\nWhile I cannot force you to watch each video in its entirety, I highly recommend doing so. The script on the page may not accurately reflect all of the content in the video, nor can it show how to perform some tasks which are purely visual.\nWhen you are ready to move to the next step in a module, click the “Next” button at the bottom of the page. Canvas will automatically add “Next” and “Previous” buttons to each piece of content which is accessed through the Modules section, which makes it very easy to work through the course content. I’ll click through a couple of items here.\nAt any point, you may click on the Modules link in the menu to the left to return to the Modules section of the site. You’ll notice that I’ve viewed the first few items in the first module, so I can access more items here. This is handy if you want to go back and review the content you’ve already seen, or if you leave and want to resume where you left off. Canvas will put green checkmarks to the right of items you’ve completed.\nFinally, you’ll find the usual Canvas links to view your grades in the course, as well as a list of fellow students taking the course.\nLet’s go back to the Course Introduction page, and look at some of the features of the YouTube video player that may be useful to you as you go through this course.\nWhen you load the player, it gives you a control toolbar at the bottom with several options. From here, you can restart the video, control playback speed, adjust the volume, enable subtitled captions, see information about the video, open the video in a new window, and more.\nEvery video in this course will have subtitles available, though the subtitles may be slightly garbled as they are automatically transcribed from the audio. I will do my best to edit them to be as accurate as possible. This will make the videos accessible to all students. In addition, studies have shown that a large number of video viewers prefer to have subtitles available even if they do not have any hearing impairment. So, please feel free to enable subtitles at any time.\nIn addition, I will attempt to speak as clearly as possible when creating the videos. If you feel that I am going too fast, or that you can keep up with me going more quickly, feel free to adjust the playback speed as needed. I have been known to absorb lecture videos at 1.5x playback speed myself, depending on the content.\nFinally, you are welcome to download the videos from YouTube if desired, based on your internet connection or other factors. If you are interested in getting these videos in another format, please contact me.\nHopefully this gives you a good idea of the different resources available in this course. To complete the first module, click the next button below to continue with the next item.\n",
    "description": "",
    "tags": null,
    "title": "Navigating Canvas \u0026 YouTube",
    "uri": "/0-introduction/02-navigating-canvas-youtube/index.html"
  },
  {
    "content": " YouTube Video Resources Slides The TCP/IP Guide Packet Switching on Wikipedia Computer Network on Wikipedia Spanning Tree Protocol on Wikipedia Basics of Computer Networking from GeeksforGeeks Video Transcript Before we start working with networking on our virtual machines, let’s take a few minutes to discuss some fundamental concepts in computer networking.\nFor most people, a computer network represents a single entity connecting their personal computer to “The Internet,” and not much thought is given to how it works. In fact, many users believe that there is a direct line from their computer directly to the computer they are talking with. While that view isn’t incorrect, it is definitely incomplete.\nA computer network more closely resembles this diagram. Here we have three computers, connected by six different network devices. The devices themselves are also interconnected, giving this network a high level of redundancy. To get from Computer A to Computer B, there are several possible paths that could be taken, such as this one. If, at any time, one of those network links goes down, the network can use a different path to try and reach the desired endpoint.\nThis is all due to the fact that computer networks use a technology called “packet switching.” Instead of each computer talking directly with one another, as you do when you make a long-distance phone call, the messages sent between two computers can be broken into packets, and then distributed across the network. Each packet is able to make its way from the sender to the receiver, where they are reassembled into the correct message. A great analogy to this process is sending a postcard through the mail. The postal service uses the address on the postcard to get it from you to its destination, but the path taken may change each time. At each stop along the way, the post office determines what the best next step would be, hopefully getting your postcard closer to the correct destination each time. This allows your postcard to get where it needs to go, without anyone ever trying to determine the entire route beforehand.\nWhen we scale this up to the size of the internet, visualized here, it is really easy to see why this is important. By using packet switching, a message can get from one end of the internet to the other without needing to take the time to figure out the entire path beforehand. It can simply move from one router to another, each time taking the most logical step toward its destination.\nModern computer networks use a theoretical model called the Open Systems Interconnection (or OSI) model, commonly referred to as the OSI 7-Layer model, to determine how each part of the network functions. For system administrators, this model is very helpful as it allows us to understand what different parts of the network should be doing, without worrying too much about the underlying technologies making it happen. In this module, we’ll look at each layer of this model in detail.\nWhen an application wants to communicate across a network, it generates a data packet starting at layer 7, the application layer, on the computer it is sending from. Then, the packet moves downward through the layers, with each layer adding a bit of information to the packet. Once it reaches the bottom layer, it will be transmitted to the first hop on the network, usually your home router. The router will then examine the packet to determine where it needs to go. It can do so by peeling back the layers a bit, usually to layer 3, the network layer, which contains the IP address of the destination. Then, it will send the packet on its way to the next hop. Once it is received by the destination computer, it will move the packet back up the layers, with each one peeling off its little bit of information. Finally, the packet will be received by the destination application.\nAs we discussed, each layer adds a bit of information to the packet as it moves down from the application toward the physical layer. This is called encapsulation. To help understand this, we can return to the postal service analogy from earlier. Let’s say you’d like to send someone a letter. You can write the letter to represent the packet of data you’d like to send, then place it in a fancy envelope with the name of the recipient on it. Then, you’ll place that envelope in a mailing envelope, and put the mailing address of the recipient on the outside. This is effectively encapsulating your letter, with each layer adding information about where it is destined. Then, the postal service might add a barcode to your letter, and place it in a large box with other letters headed to the same destination, further encapsulating it. Once it reaches the destination, each layer will be removed, slowly revealing the letter inside.\nIn this video, we’ll discuss the bottom two layers of the model, the physical and data link layers. Later videos will discuss the other layers in much more detail.\nFirst, the physical layer. This layer represents the individual bytes being sent across the network, one bit at a time. Typically this is handled directly in hardware, usually by your network interface card or NIC. There are many different ways this can be done, and each type of network has a different way of doing it. For this class, we won’t be concerned with that level of detail. If you hear things such as “100BASE-T” or “1000BASE-T” or “gigabit,” those terms are typically referring to the physical layer.\nThe next layer up is the data link layer. At this layer, data consists of a frame, which is a standard sized chunk of data to be sent from one computer to another. A packet may fit inside of a frame, or a packet may be further divided into multiple frames, depending on the system and size of the packet. Some common technologies used at this layer are Ethernet, and the variety of 802.11 wireless protocols, among others.\nOne important concept at this layer is the media access control address, or MAC address. Each physical piece of hardware is assigned a unique, 48-bit identifier on the network. When it is written, it is usually presented as 6 pairs of 2 hexadecimal characters, separated by colons, such as the example seen here. The MAC address is used in the data link layer to identify the many different devices on the network. For most devices, the MAC address is set by the manufacturer of the device, and is usually stored in the hardware itself. It is intended to be permanent, and globally unique in the world. However, many modern systems allow you to change, or “spoof,” the MAC address. This can be really useful when dealing with technologies such as virtualization. However, it also can be used maliciously. By duplicating an existing MAC address on the network, you can essentially convince the router to forward all traffic meant for that system to you, allowing you to perform man-in-the-middle attacks very easily. For most uses, however, you won’t have to do anything to configure MAC addresses on your system, but you’ll want to be aware of what they are and how they are used.\nThe major networking item handled by the data link layer is routing. Routing is how each node in the network determines the best path to get from one point to another on the network. It is also very important in allowing the network to have redundant connections while preventing loops across those connections. To determine how to route a packet, most networks use some variant of a spanning tree algorithm to build the network map. Let’s see how that works.\nTo start, here is a simple network. There are 6 network segments, labeled a through f. There are also 7 network bridges connecting those segments, numbered with their ID on the network. To begin, the network bridge with the lowest ID is selected as the root bridge. Next, each bridge determines the least-cost path from itself to the root bridge. The port on that bridge in the direction of the root bridge is labelled as the root port, or RP in this diagram. Then, the shortest path from each network segment toward the root bridge is labelled as the designated port, or DP. Finally, any ports on a bridge not labelled as a root port or designated port are blocked, labelled BP on this diagram.\nNow, to get a message from network segment f to the root bridge, it can send a message toward its designated port, on bridge 4. Then, bridge 4 will send the packet out of its root port into segment c, which will pass it along its designated port on bridge 24. The process continues until the packet reaches the root bridge. In this way, any two network segments are able to find a path to the root bridge, which will allow them to communicate.\nIf, at any time a link is broken, the spanning tree algorithm can be run again to relabel the ports. So, in this instance, segment f would now send a message toward bridge 5, since the link between segment c and bridge 24 is broken.\nFinally, the other important concept at layer 2 is the use of virtual local area networks, or VLANs. A VLAN is simply a partition of a network at layer 2, isolating each network. In essence, what you are doing is marking certain ports of a router as part of one network or another, and telling the router to treat them as separate networks. In this way, you can simplify your network design by having systems grouped by function, and not by location.\nHere’s a great example. In this instance, we have a building with three floors. Traditionally, each floor would consist of its own network segment, since typically there would be one router per floor. Using VLANs, we can rearrange the network to allow all computers in the engineering department to be on the same network segment, even though they are spread across multiple floors of the building.\nIn the real world, VLANs are used extensively here at K-State. For example, all of the wireless access points are on the same VLAN, even though they are spread across campus. The same goes for any credit card terminals, since they must be protected from malicious users attempting to listen for credit card numbers on the network.\nMost enterprise-level network routers are able to create VLANs, but many home routers do not support that feature. Since we won’t be working with very large networks in this course, we won’t work with VLANs directly. However, they are very important to understand, since most enterprises make use of them.\nIn the following videos, we’ll discuss the next layers of the OSI model in more detail.\n",
    "description": "",
    "tags": null,
    "title": "Networking Overview",
    "uri": "/3-core-networking-services/02-networking-overview/index.html"
  },
  {
    "content": " YouTube Video Resources How Does SSH Work from Hostinger SSH Essentials: Working with SSH Servers, Clients and Keys from DigitalOcean How to Set Up SSH Keys on Ubuntu 20.04 from DigitalOcean Simplify Your Life With an SSH Config File from Nerderati Video Transcript In this video, I’ll discuss how to use the Secure Shell, or SSH, remote server and client. You’ll be using it throughout the semester to connect to various computers, and there are a few things you should know that will help make your experience a much more pleasant one.\nIf you’d like to know more about the technical details of how SSH works, refer to the links in the resources section below the video.\nFirst, let’s look at the SSH client program, available as the ssh command on most Linux systems. It is typically installed by default, but if not you can install it using the apt command:\nsudo apt install openssh-client Once it is installed, you can connect to any server you wish by simply using the ssh command followed by the host you’d like to connect to. Optionally, you may need to provide a different username, in which case you would put the username before the host, connecting the two with an @ symbol, much like an email address. If you’d like to try it yourself, you can try to connect to the CS Linux servers using a command similar to the following:\nssh russfeld@cslinux.cs.ksu.edu Obviously, you’ll need to replace russfeld with your own username.\nIf this is the first time you are connecting to a particular server, you’ll receive an error message similar to the following:\nThe authenticity of host ‘cislinux.cs.ksu.edu (129.130.10.43)’ can’t be established.\nECDSA key fingerprint is SHA256: \u003crandom_text\u003e.\nAre you sure you want to continue connecting (yes/no)?\nAs a security feature, the SSH client will remember the identity of each server you connect to using this fingerprint. Since this is the first time you have connected to this server, it doesn’t have a fingerprint stored, and it will ask you to confirm that the one presented is correct. If you wanted to, you could contact the owner of the server and ask them to verify that the fingerprint is correct before connecting, but in practice that is usually not necessary. However, once you’ve accepted the fingerprint, it will store it and check the stored fingerprint against the one presented on all future connections to that server.\nIf, at any point in the future, the fingerprints don’t match, you will be presented with a similar error message. In that case, however, you should be very cautious. It could be the case that the server was recently reset by its owner, in which case you can easily contact them to verify that fact. However, it could also be the case that someone is attempting to either listen to your connection or have you connect to a malicious server. In that case, you should terminate the connection immediately, or else they could possibly steal your credentials or worse.\nIn any case, since this is our first time connecting, just type yes to store the fingerprint and continue connecting. You’ll then be asked to provide your password. Note that when you enter your password here it will not show any indication that you are entering text, so you must type carefully. If the password is accepted, you’ll be given access to the system.\nOn that system, you can run any command that you normally would. You can even run graphical programs remotely via SSH, but that requires a bit more configuration and software which I won’t go into here. Once you are finished, you can type exit to close the remote session and return to your own computer.\nNext, let’s discuss the SSH server. It is available as open-source software for Linux, and can easily be installed on any system running Ubuntu Linux using the apt command:\nsudo apt install openssh-server Once it has been installed, you can verify that it is working by using the ssh command to connect to your own system:\nssh localhost If this is the first time you have done so, you should get the usual warning regarding host authenticity. You can simply type yes to continue connecting, then provide your password to log on to the system. Once you have verified that it is working, type exit to return to the local terminal. If your server is connected to a network, you can use ssh to connect to it just like you would any other server. Note that you may have to configure your firewall to allow incoming SSH connections, typically on port 22.\nThe SSH server has many options that you can configure. You can edit the configuration file using this command:\nsudo nano /etc/ssh/sshd_config There are many great resources online to help you understand this file. In general you can use the default settings without any problems, but there are a couple of lines you may want to change. Note that any line prefixed with a hash symbol # is a code comment, so you will need to remove that symbol for the change to be read. The commented lines reflect the default settings, which can be overridden by un-commenting the line and changing the value.\nThe first line you may want to change is:\n#Port 22 This line defines the port that SSH listens on. By default, SSH uses port 22, a well-known port for that service. However, since everyone knows that SSH uses port 22, it is very common for servers connected to the internet to receive thousands of malicious login attempts via that port in an attempt to discover weak passwords and usernames. By changing the SSH port to a different port, you can eliminate much of that traffic. Of course, if you change this port here, you may also need to update your firewall rules to allow the new port through the firewall.\nThe other line worth looking at is:\n#PasswordAuthentication yes By changing this line to no you can require that all users on your system use SSH keys to log in, which are generally much more secure than passwords. We’ll discuss SSH keys shortly. Remember to make sure your own SSH key is properly configured before changing this setting, or you may lock yourself out from your own system.\nIf you make any changes to the configuration file, you can restart the server using the following command:\nsudo systemctl restart ssh You can also always check the status of the server using this command:\nsudo systemctl status ssh To make your life a bit easier, let’s discuss SSH keys. Instead of providing a password each time you want to log on to a system with SSH, you can have your computer automatically provide a key to prove your identity. Not only is this simpler for you, but in many cases it is much more secure overall.\nTo use SSH keys, you must first generate one. You’ll need to perform this step on the computer you plan to connect from, not on the server you’ll connect to. In Terminal, type the following command to start the process:\nssh-keygen -t rsa -b 4096 This will generate a public \u0026 private RSA keypair with a 4096 bit size, generally regarded as a very secure key by most standards.\nThe command will first ask you where to store the key. It defaults to ~/.ssh/id_rsa which is the standard location, so just press enter to accept the default.\nNext, it will ask you to set a passphrase for your key. If you are setting up the key on a secure computer that only you would have access to, you can leave this blank to not set a passphrase on the key. In that way, you won’t have to enter any passwords to use SSH with this key. However, if anyone gains access to the key file itself, they can easily log in to any system as you with that key until you revoke access. If you want your keys to be more secure, you can set a passphrase to lock the key. The downside is that you’ll have to provide the passphrase for the key in order to use it, but it is still more secure than providing the password to the user account itself.\nOnce the key is created, it will give you some information about where it is stored. You can view those files by typing the following:\nls ~/.ssh/ There should be at least two files here. The first, id_rsa, is your private key. Do not share that file with anyone! You may choose to make a backup of that file for your own use, if desired. However, if the key was not protected with a passphrase when it was created, you should protect that file as closely as if it contained all of your passwords in plain text.\nThe second file, id_rsa.pub, is your public key. This is the file we’ll need to give to other servers so that we can log on using our private key. There are a couple of ways to do so. First, I’ll show you the automatic way, then I’ll discuss what it does so you could do it manually if needed.\nTo send your public key to a remote server, we’ll use the ssh-copy-id command. Its syntax is very similar to the normal SSH command. For example, to send my new public key to the CS Linux server, I would do the following:\nssh-copy-id russfeld@cslinux.cs.ksu.edu It will prompt you for your password, and if it succeeds it will install the key on that server. You can then verify that it worked by using the normal SSH command to connect. It should succeed without asking you to provide any password, except possibly the passphrase for your SSH key.\nOnce on that server, we can see where the key gets installed. It is usually placed in a file at ~/.ssh/authorized_keys. On some systems, it may be ~/.ssh/authorized_keys2 due to a security vulnerability, or both files may be present. This is configurable in the SSH Server configuration file.\nIn that authorized_keys file, you’ll see a copy of your public key on a single line. To add additional keys to the system, you can either use the ssh-copy-id command from the system containing the new key, or copy and paste the public keys directly into this file, one per line. To remove a key, simply delete the corresponding line from this file. Note that at the end of the line you can see the username and computer name from the machine where the key was created, which can be very helpful in identifying keys.\nFinally, there is one other trick you can use to make working with SSH servers much more useful, and that is through the use of an SSH config file. First, make sure you are back on your local computer and not on any remote servers. Check the command prompt carefully, or type exit a few times to make sure you have closed all SSH sessions. On your local computer, open a new SSH config file using the following command:\nnano ~/.ssh/config By convention, your SSH config file should reside in that location. In that file, you can configure a wide variety of host settings for the servers you connect to. For example, here is one entry from my own SSH config file:\nHost cs HostName cslinux.cs.ksu.edu Port 22 User russfeld IdentityFile ~/.ssh/id_rsa The first line gives the short name for the host. Then, all lines below it give configuration information for that host. For example, here I’ve given the hostname (you can also use an IP address), the port, the username, and the private key file that should be used when connecting to this host. These are all options that you’d normally have to provide on the command line when using the ssh command. By placing them here, you can just reference the host by its short name, and all of this information will automatically be used when you connect to that system.\nWith this in place, all I have to do to connect to the CS Linux servers is type:\nssh cs and all my other options are read from this configuration file. Pretty nifty, right?\nThere are many other options you can include in your SSH config file. I recommend checking out the links in the resources section for more information.\nI hope this video will be very useful to you as you work with SSH throughout this semester and in the future. I know much of this information has been very helpful to me in my career thus far.\n",
    "description": "",
    "tags": null,
    "title": "SSH",
    "uri": "/x-extras/02-ssh/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello and welcome to the week two announcements video for CIS 527 in summer 2022. So this week you should be finishing up lab one. Lab one grading is due tonight by 7pm. So if you haven’t scheduled a time to meet with me for grading, make sure you do that ASAP. I do have trainings on Monday afternoons from about one to three, I usually get done with those a little earlier. But it’s worth noting that at least half of my afternoon is taken up by another meeting. So make sure you schedule your schedule your grading times in advance so that you can get those in whenever they work for you. And of course, if you work from eight to five, let me know and I’ll make some time available in my calendar probably after 5pm. So around six or seven to get you taken care of coming up this week, you’ve got lab two quizzes, which are due on Friday. And then next week, you’ll be turning in lab two and the first discussion which is the week two discussion.\nSo the discussion for this week is Seth Galitzer. In years past, if you watch the first video, we used to bring in live speakers for this class. Unfortunately, this summer because of low enrollment, and I don’t have a ton of time, I’m reusing the videos from a previous semester. So our first video speaker is Seth Galitzer. He’s our computer science system administrator. He has been in that role since 2006. He originally has a bachelor’s degree in computer science from K State. And he basically is the person that manages all the Computer Science Systems and labs that we work with all of our research computing. For us as faculty, he helps us purchase and configure our own computer equipment. So Seth has a world of research in system administration and always loves to share his knowledge. So take a look at the video we recorded last year, answer the prompts about it and then come up with a couple of questions you’d like me to pass on to Seth. And then I will take care of passing those on, it might take a little bit for me to get back from him since he’s a little bit busy in the summer. But I will try and get some answers to those questions for you as soon as I can.\nSo this coming week, you’ll be working on lab two, the basic idea of lab two is to redo everything that you did in lab one using the Puppet automation tool to actually make it automated using code instead of doing it by hand. So to get started with lab two, you’re going to reinstall your operating system and a virtual machine install Puppet run all the updates on the operating system. And then very importantly, you need to make a snapshot of your operating system before you start working in Puppet. That way as you write your Puppet manifest file and test you can roll back to that snapshot before you did any Puppet work and use that for testing. Every semester I have one student forget that step where they forget to make a snapshot and then they have to reinstall the operating system and try again, because there is no way to rollback a Puppet manifest once you’ve applied it, you need to rollback the snapshot in the virtual machine. The other big hint I can tell you is try and keep it simple. This is not meant to be a very complex lab, most solutions are around 200 lines of code. And for a lot of this, you can simply configure it manually, then use puppet resource to query the system figure out what that looks like. And then the output of puppet resource is very similar to what you’ll put in your puppet manifest file. Don’t just copy paste the whole thing but put just the parts that are important into your manifest file. And it should work just fine.\nThat’s all I’ve got for this week. Other than that, feel free to keep in touch. We’ve got discussions on Discord that you’re welcome to chat with me anytime. I’m available for tea time office hours, Tuesdays at 330 and Fridays at 1030. If you want to come out and talk about just about anything in the world, or just hanging out with people while you continue to work on homework. And then of course you can always use my Calendly link to schedule a one on one office hours with me if you have any questions or concerns about the class. So I know that you might be thinking with lab one, you have to do it again in lab two, but it’s really worth it to see the difference between manually doing all this configuration work and then automating that using a tool like puppet. I hope you really start to see the value of automation and System Administration. That’s one of the big things we push for. As always, if you have any questions, let me know otherwise, good luck, and I will see you next week.\n",
    "description": "",
    "tags": null,
    "title": "Summer '22 Week 2",
    "uri": "/y-announcements/old/summer2022/week02/index.html"
  },
  {
    "content": " Note TODO As of 2020, both KSIS and HRIS have moved to the cloud, leaving very few K-State resources hosted directly on campus. This video will be updated in future semesters to reflect that change. -Russ\nYouTube Video Resources Slides Cloud Computing on Wikipedia The Cloud/Electric Generator Analogy by Roger Smith from InformationWeek The Big Switch: Rewiring the World from Editson to Google by Nicholas Carr Virtual Private Network on Wikipedia Service Oriented Architecture on Wikipedia Lots of Bits by Jeff Barr on AWS News Blog Animotos Facebook Scale-Up by Thorsten von Eicken from RightScale Cloud Forecast 2015: Skills, Security and Public Cloud Infrastructure from 2nd Watch Video Transcript To begin this module, let’s take a few minutes to discuss its major topic, the cloud. At this point, some of you may be wondering what exactly I’m talking about when I refer to the cloud. My hope is that, by the end of this video, you’ll have a much clearer mental picture of exactly what the cloud is and how it is related to all of the things we’ve been working with in this class so far.\nWithout giving too much of the answer away, I always like to refer to this comic strip from XKCD when defining the cloud. Take a minute to read it and ponder what it says before continuing with this video.\nTo start, one way you can think of cloud computing is the applications and resources that you are able to access in the cloud. It could be an email address, social media website, finance tracking application, or even a larger scale computing and storage resource. All of those could be part of the cloud. To access them, we use the devices available to us, such as our computers, laptops, tablets, phones, but even our TVs, game consoles, and media players all access and use the cloud.\nOne of the core concepts underlying the cloud is the centralization of resources. This is not a new idea by any stretch of the imagination. In fact, many comparisons have been made between the growth of large electric power plants and the growth of the cloud. Here, you can see both of those worlds colliding, as this picture shows Amazon’s founder, Jeff Bezos, standing in front of an old electric generator from the 1890s at a beer brewery in Belgium.\nIn the early days of electricity, if a company wanted to use electricity in their factory, they usually had to purchase a power generator such as this one and produce their own. Companies really liked this approach, as they could be completely in control of their costs and production, and didn’t have to worry about failures and problems that they couldn’t resolve themselves.\nGenerating electricity is also a very inefficient process, but it can be made much more efficient by scaling up. A single company may not use all of the electricity they generate all the time, and any issues with the generator could cause the whole operation to slow down until it is resolved. But, if many companies could band together and share their electrical systems, the process can be much more efficient and the power supply more secure than each company could provide on their own.\nIt was during this time that several companies were formed to do just that: provide a large scale power generation and distribution network. Many companies were initially hesitant to give up their generators, fearing that they would be at the mercy of someone else for the power to run their operations, and that such a large scale system wasn’t sustainable in the long run. However, their fears proved to be false, and within a few decades nearly all of them had switched to the growing power grid.\nFor K-State, most of our electricity comes from Jeffrey Energy Center near Wamego, KS. It is capable of generating over 2 gigawatts of power, enough to power over one million homes. For most of us, we don’t even take the time to think about where our electricity comes from, we just take it for granted that it will be there when we need it, and that there is more than enough of it to go around.\nThe cloud was developed for many of those same reasons. This story is detailed in depth in the book “The Big Switch” by Nicholas Carr. I encourage you to check it out if you are interested in the history of the cloud.\nThe concept of a cloud got its start all the way back in the 1950s with the introduction of time sharing on mainframe computers. This allowed many users to share the same computer, with each running her or his programs when the others were busy writing or debugging. Later on, many networking diagrams started to use a cloud symbol to denote unknown networks, leading to the name “the cloud” referring to any unknown resource on the internet.\nIn the 1990s, large businesses started connecting their branch offices with virtual private networks, or VPNs. In this way, a branch office in New York could access shared files and resources from the main office in California. Over time, the physical location of the resources became less and less important - if it could be in California, why couldn’t it just be anywhere? By this point, people were ready to adopt the cloud into their lives.\nThe explosion in popularity came with the rise of Web 2.0 after the dot-com bubble burst, but also in 2006 with the introduction of Amazon’s Elastic Compute Cloud, or EC2. Originally, companies who wanted to have a website or process large amounts of data would purchase large computing systems and place them directly in their own offices. They would also have to hire staff to manage and maintain those systems, as well as deal with issues such as power and cooling. Over time, they also started to realize that they may not be making efficient use of those systems, as many of them would remain idle during slower business periods. At the same time, larger companies with idle computing resources realized they could sell those resources to others in order to recoup some of their costs, and possibly even make a nice profit from it. Other companies realized that they could make money by providing some of those resources as a service to others, such as web hosting and email.\nAmazon was one such company. Since they were primarily in the retail business at that time, they had to build a system capable of handling the demands of the busy retail seasons. However, most of the time those resources were idle, and they simply cost the company money to maintain them. So, Amazon decided to sell access to those computing resources to other companies for a small fee, letting them use the servers that Amazon didn’t need for most of the year. They stumbled into a gold mine. This graph shows the total bandwidth used by Amazon’s global websites, and then the bandwidth for Amazon Web Services, part of their cloud computing offerings. Less than a year after it launched, Amazon was using more bandwidth selling cloud resources to others than their own website used, typically one of the top 10 websites in the world. Some people have referred to this graph as the “hockey stick” graph, showing the steep incline and growth of the cloud. Other companies quickly followed suit, and the cloud as we know it today was born.\nFor many cloud systems today, we use an architecture known as a “service-oriented” architecture. In essence, a cloud company could build their system out of a variety of services, either hosted by themselves or another cloud provider. For example, a social media site might have their own web application code, but the website itself is hosted by Amazon Web Services, and the data is stored in Google’s Firebase. In addition, their authentication would be handled by many different third-parties through OAuth, and even their code and testing is handled via GitHub, Travis and Jenkins. By combining all of those services together, a company can thrive without ever owning even a single piece of hardware. Many start-ups today are doing just this in order to quickly grow and keep up with new technologies.\nAccording to the National Institute of Standards and Technology, or NIST, there are a few primary characteristics that must be met in order for a computer system to be considered a “cloud” system. First, it must have on-demand, self-service for customers. That means that any customer can request resources as needed on their own, without any direct interaction with the hosting company. Secondly, that resource must be available broadly over the internet, not just within a smaller subnetwork. In addition, the system should take advantage of resource pooling, meaning that several users can be assigned to the same system, allowing for better scaling of those resources. On top of that, systems in the cloud should allow for rapid elasticity, allowing users to scale up and down as needed, sometimes at a moment’s notice. Finally, since users may be scaling up and down often, the service should be sold on a measured basis, meaning that users only pay for the resources they use.\nBeyond that, there are a few other reasons that cloud computing is a very important resource for system administrators today. First, cloud computing resources are location independent. They could be located anywhere in the world, and as long as they are connected to the internet, they can be accessed from anywhere. Along with resource pooling, most cloud providers practice multi-tenancy, meaning that multiple users can share the same server. Since most users won’t use all of a server’s computing resources all the time, it is much cheaper to assign several different users to the same physical system in order to reduce costs. Also, in many ways a cloud system can be more reliable than a self-hosted one. For cloud systems, it is much simpler for a larger organization to shift resources around to avoid hardware failures, whereas a smaller organization may not have the budget to maintain a full backup system. We’ve already discussed the scalability and elasticity, but for many organizations who are hoping to grow quickly, this is a very important factor in their decision to host resources in the cloud. Finally, no discussion of the cloud would be complete without talking about security. In some ways, the cloud could be more secure, as cloud providers typically have entire teams dedicated to security, and they are able to easily keep up with the latest threats and software updates. However, by putting a system on the cloud, it could become a much easier target for hackers as well.\nOne of the best examples demonstrating the power of the cloud is the story of Animoto. Animoto is a website that specializes in creating video montages from photos and other sources. In 2008, their service exploded in popularity on Facebook, going from 25,000 users to 250,000 users in three days. Thankfully, they had already configured their system to use the automatic scaling features of AWS, so they were able to handle the increased load. At the peak, they were configuring 40 new cloud instances for their render farm each minute, which was receiving more than 450 render requests per minute. Each render operation would take around 10 minutes to complete. Compare that to a traditional enterprise: there is no way a traditional company would be able to purchase and configure 40 machines a minute, and even if it was possible, the costs would be astronomical. For Animoto, the costs scaled pretty much linearly with their user base, so they were able to continue providing their service without worrying about the costs. Pretty cool, right?\nWhen dealing with the cloud, there are many different service models. Some of the most common are Software as a Service, Infrastructure as a Service, and Platform as a Service. Many times you’ll see these as initialisms such as SaaS, IaaS, and PaaS, respectively. This diagram does an excellent job of showing the difference between these models. First, if you decide to host everything yourself, you have full control and management of the system. It is the most work, but provides the most flexibility. Moving to Infrastructure as a Service, you use computing resources provided by a cloud provider, but you are still responsible for configuring and deploying your software on those services. AWS and DigitalOcean are great examples of Infrastructure as a Service. Platform as a Service is a bit further into the cloud, where you just provide the application and data, but the rest of the underlying system is managed by the cloud provider. Platforms such as Salesforce and Heroku are great examples here. Finally, Software as a Service is the situation where the software itself is managed by the cloud provider. Facebook, GMail, and Mint are all great examples of Software as a Service - you are simply using their software in the cloud.\nIn the past, there was a great analogy comparing these models to the different ways you could acquire pizza. You could make it at home, do take and bake, order delivery, or just go out and eat pizza at your favorite pizza restaurant. However, a few commentators recently pointed out a flaw in this model. Can you find it? Look at the Infrastructure as a Service model, for take and bake pizza. In this instance, you are getting pizza from a vendor, but then providing all of the hardware yourself. Isn’t that just the opposite of how the cloud is supposed to work?\nThey propose this different model for looking at it. Instead of getting the pizza and taking it home, what if you could make your own pizza, then use their oven to bake it? It in this way, you maintain the ultimate level of customizability that many organizations want when they move to the cloud, without having the hassle of dealing with maintaining the hardware and paying the utility bills associated with it. That’s really what the cloud is about - you get to make the decisions about the parts you care about most, such as the toppings, but you don’t have to deal with the technical details of setting up the ovens and baking the pizza itself.\nAlong with the different service models for the cloud, there are a few different deployment models as well. For example, you might have an instance of a cloud resource that is private to an organization. While this may not meet the NIST definition for a cloud resource, most users in that organization really won’t know the difference. A great example of this is Beocat here at K-State. It is available as a free resource to anyone who wants to use it, but in general they don’t have to deal with setting up or maintaining the hardware. There are, of course, public clouds as well, such as AWS, DigitalOcean, and many of the examples we’ve discussed so far. Finally, there are also hybrid clouds, where some resource are on-site and others are in the cloud, and they are seamlessly connected. K-State itself is a really great example of this. Some online resources are stored on campus, such as KSIS and HRIS, while others are part of the public cloud such as Canvas and Webmail. All together, they make K-State’s online resources into a hybrid cloud that students and faculty can use.\nOf course, moving an organization to the cloud isn’t simple, and there are many things to be concerned about when looking at the cloud as a possible part of your organization’s IT infrastructure. For example, cloud providers may have insecure interfaces and APIs, allowing malicious users to access or modify your cloud resources without your knowledge. You could also have data loss or leakage if your cloud systems are configured incorrectly. In addition, if the service provider experiences a major hardware failure, you might be unable to recover for several days while they resolve the problem. For example, several years ago K-State’s email system experienced just such a hardware failure, and it was nearly a week before full email access was restored across campus. Unfortunately, this happened about two weeks before finals, so it was a very stressful time for everyone involved.\nThere are also many security concerns. One of those is the possibility of a side-channel attack. In essence, this involves a malicious person who is using the same cloud provider as you. Once they set up their cloud system, they configure it in such a way as to scan nearby hardware and network connections, trying to get access to sensitive data from inside the cloud provider’s network itself. The recent Spectre and Meltdown CPU vulnerabilities are two great examples of side-channel attacks.\nAlso, if you are storing data on a cloud provider’s hardware, you may have to deal with legal data ownership issues as well. For example, if the data from a United States-based customer is stored on a server in the European Union, which data privacy laws apply? What if the data is encrypted before it is sent to the EU? What if the company is based on Japan? Beyond that, if there is a problem, you won’t have physical control of the servers or the data. If your cloud hosting provider is sold or goes out of business, you may not even be able to access your data directly. Finally, since cloud providers host many different organizations on the same hardware, you might become a bigger target for hackers. It is much more likely for a hacker to attack AWS then a small organization. So, there are many security concerns to consider when moving to the cloud.\nIn addition, there are a few roadblocks that may prevent your organization from being able to fully embrace the cloud. You should carefully consider each of these as you are evaluating the cloud and how useful it would be for you. For example, does the cloud provide the appropriate architecture for your application? Are you able to integrate the cloud with your existing resources? Will the change require lots of work to retrain your employees and reshape business practices to take full advantage of the cloud? I highly encourage you to read some of the linked resources below this video to find great discussions of the cloud and how it fits into an organization’s IT infrastructure.\nRegardless, many organizations are currently moving toward the cloud, and the rate of adoption is increasing each year. This infographic gives some of the predictions and trends for the cloud from 2015. For me, the big takeaways from this data is that companies are devoting more and more of their IT budgets toward the cloud, but they are hoping to turn that into cost-savings down the road. At the same time, their biggest concern moving to the cloud is finding people with enough experience to manage it properly while avoiding some of the security pitfalls along the way. So, learning how to work with the cloud will help you build a very valuable skill in the IT workplace of the future.\nSo, back to the question from the beginning of this video: what is the cloud? According to Esteban Kolsky, a cloud marketing consultant, the cloud is really a term for many things. In one way, the cloud is the internet itself, and all of the resources available on it. From Amazon to Yelp and everything in between, each of those websites and applications provides a service to us, their consumers. At the same time, the cloud is a delivery model, or a way to get information, applications, and data into the hands of the people who need it. This class itself is being delivered via the cloud, from that point of view at least. Finally, the cloud can also be seen as a computing architecture. For many organizations today, instead of worrying about the details of their physical hardware setup, they can just use “the cloud” as their computing architecture, and discuss the advantages and disadvantages of that model just like any other system.\nFor me, I like to think of the cloud as just a point of view. From the consumer’s perspective, any system that they don’t have to manage themselves is the cloud. For a system administrator, it’s not so easy. For example, for many of us, we can consider the K-State CS Linux servers as the cloud, as they are always available, online, and we don’t have to manage them. For our system administrator, however, they are his or her systems to manage, and are definitely not the cloud. Similarly, for Amazon, AWS is just another service to manage. So, I think that the cloud, as a term, really just represents your point of view of the system in question and how closely you have to manage it.\nWhat about you? If you have any thoughts or comments on how you’d define the cloud, I encourage you to post them in the course discussion forums.\nIn the next videos, we’ll dive into how to set up and configure your first cloud resources, using DigitalOcean as our Infrastructure as a Service provider.\n",
    "description": "",
    "tags": null,
    "title": "The Cloud Overview",
    "uri": "/5-the-cloud/02-the-cloud-overview/index.html"
  },
  {
    "content": "Automated provisioning of workstations and servers.\n",
    "description": "",
    "tags": null,
    "title": "Configuration Management",
    "uri": "/2-configuration-management/index.html"
  },
  {
    "content": "Lab 2 - Configuration Management Instructions Create two different Puppet Manifest Files that meet the specifications below. Each one will be applied to a newly installed virtual machine of the appropriate operating system configured as described in Task 0. The best way to accomplish this is to treat this assignment like a checklist and check things off as you complete them.\nIf you have any questions about these items or are unsure what they mean, please contact the instructor. Remember that part of being a system administrator (and a software developer in general) is working within vague specifications to provide what your client is requesting, so eliciting additional information is a very necessary skill.\nNote To be more blunt - this specification may be purposefully designed to be vague, and it is your responsibility to ask questions about any vagaries you find. Once you begin the grading process, you cannot go back and change things, so be sure that your machines meet the expected specification regardless of what is written here. –Russ\nAlso, to complete many of these items, you may need to refer to additional materials and references not included in this document. System administrators must learn how to make use of available resources, so this is a good first step toward that. Of course, there’s always Google !\nTime Expectation This lab may take anywhere from 1 - 6 hours to complete, depending on your previous experience working with these tools and the speed of the hardware you are using.\nTip Testing Manifest Files - When testing these manifest files, there is a three step process. First, apply the manifest, then reboot, then apply again. This is because any changes made to group memberships are not applied until after a user logs back in or the system reboots. So, you may get permission issues when creating files or assigning permissions due to incorrect group memberships. Ideally, those permission errors should be eliminated after a reboot. There is no good fix for this in Puppet itself, since it is an operating system issue. Therefore, this is the process that you should use, and it is the process that will be used when your manifest files are graded. Basically, if you get no errors after a reboot, you should be fine!\nTask 0: Create New Virtual Machines \u0026 Snapshots Create new Windows 10 and Ubuntu 22.04 virtual machines for this lab. When creating the virtual machines and installing the operating system, use the same information from Lab 1. You should create the cis527 account during installation. DO NOT PERFORM ANY ADDITIONAL CONFIGURATION AFTER THE INSTALLATION IS COMPLETE EXCEPT WHAT IS LISTED BELOW!\nAfter installing the operating system, install ONLY the following software:\nPuppet Agent 8 (Windows \u0026 Ubuntu) See Installing Agents and Enable the Puppet Platform on Apt from Puppet Documentation Recall that Ubuntu 22.04 is codenamed “Jammy Jellyfish”, so use the url https://apt.puppet.com/puppet8-release-jammy.deb to get the correct version on Ubuntu. VMware Tools (Windows) and either open-vm-tools-desktop or VMware Tools (Ubuntu) All System Updates (Windows \u0026 Ubuntu) On the Windows virtual machine only, create a folder at C:\\install and download the following installers. Do not change the name of the installers from the default name provided from the website. You may choose to do this step using the download_file Puppet module instead.\nFirefox (Firefox Setup 116.0.2.exe as of 8/15/2023) Thunderbird (Thunderbird Setup 115.1.0.exe as of 8/15/2023) Notepad++ (npp.8.5.6.Installer.x64.exe as of 8/15/2023) Note I have listed sample names of the installers as of this writing, and these will be the ones that I use for testing; however, you may receive newer versions with slightly different names. That is fine. Just be sure that you don’t get the default stub or web-only installers, which is what Firefox typically gives you unless you follow the links above. They will not work properly for this lab. –Russ\nOnce you have your virtual machines configured, make a snapshot of each called “Puppet Testing” for your use. As you test your Puppet manifest files, you’ll reset to this snapshot to undo any changes made by Puppet so you can test on a clean VM. The VMs used for grading will be configured as described here.\nWarning When you reset back to a snapshot, any new or modified files on the VM will be lost. So, make sure you keep a backup of the latest version of your manifest files on your host machine!\nTask 1: Puppet Manifest File for Ubuntu Create a Puppet Manifest File for Ubuntu 22.04 that defines the following configuration. This configuration is very similar to, but not exactly the same as, Lab 1, so read through it carefully. Assume that the machine you are applying the manifest file on is configured as described above in Task 0.\nUsers (Same as Lab 1) adminuser | AdminPassword123 (Administrator type or sudo group) normaluser | NormalPassword123 (Normal type) guestuser | GuestPassword123 (Normal type) eviluser | EvilPassword123 (Normal type) Create groups as needed below Files \u0026 Permissions (Same as Lab 1) Create a folder /docs (at the root of the system, not in a user’s home folder). Any user may read or write to this folder, and it should be owned by root:root (user: root; group: root). Within /docs, create a folder for each user created above except for cis527, with the folder name matching the user’s name. Make sure that each folder is owned by the user of the same name, and that that user has full permissions to its namesake folder. Create a group and set permissions on each folder using that group to allow both cis527 and adminuser to have full access to each folder created in /docs. No other user should be able to access any other user’s folder. For example, eviluser cannot access guestuser’s folder, but adminuser and cis527 can, as well as guestuser, who is also the owner of its own folder. In each subfolder of /docs, create a text file. It should have the same access permissions as the folder it is contained in. The name and contents of the text file are up to you. See this screenshot for what these permissions may look like in Terminal. Software (Same as Lab 1) Mozilla Firefox (firefox) Mozilla Thunderbird (thunderbird) Apache Web Server (apache2) Synaptic Package Manager (synaptic) GUFW Firewall Management Utility (gufw) ClamAV (clamav) Services - Ensure the following services are running: Apache Web Server Clam AntiVirus’ FreshClam Service Note You will have to find the appropriate name for each service. –Russ\nTask 2: Puppet Manifest File for Windows 10 Create a Puppet Manifest File for Windows 10 that defines the following configuration. This configuration is very similar to, but not exactly the same as, Lab 1, so read through it carefully. Assume that the machine you are applying the manifest file on is configured as described above in Task 0.\nUsers (Same as Lab 1) AdminUser | AdminPassword123 (Administrators \u0026 Users group) NormalUser | NormalPassword123 (Users group) GuestUser | GuestPassword123 (Guests group only) EvilUser | EvilPassword123 (Users group) Create groups as needed below Files \u0026 Permissions (Same as Lab 1) Create the folder C:\\docs. It should be owned by the cis527 account, but make sure all other users can read and write to that folder. Within C:\\docs, create a folder for each user created above except for cis527, with the folder name matching the user’s name. Make sure that each folder is owned by the user of the same name, and that that user has full permissions to its namesake folder. Create a group containing cis527 and AdminUser, and set permissions on C:\\docs for that group to have full access to each folder created in C:\\docs. No other user should be able to access any other user’s folder. For example, EvilUser cannot access GuestUser’s folder, but AdminUser and cis527 can, as well as GuestUser, who is also the owner of its own folder. In each subfolder of C:\\docs, create a text file. It should have the same access permissions as the folder it is contained in. The name and contents of the text file are up to you. Don’t remove the SYSTEM account or the built-in Administrator account’s access from any of these files. Usually this is as simple as not modifying their permissions from the defaults. See this screenshot and this screenshot for what these permissions may look like in PowerShell. Software - Install the latest version of the following software. The installation should be done SILENTLY without any user interaction required. In addition, Puppet should be able to detect if they are already installed, and not attempt to install them again. Mozilla Firefox Mozilla Thunderbird Notepad++ Note You will need to research the appropriate options to give to the installer through Puppet for them to install silently. For this lab, you should not use any Windows package managers such as Chocolatey or Ninite. The installation files will be already downloaded and stored in C:\\install. Also, you’ll need to make sure your resource names exactly match the names of the packages after they are installed, or Puppet will attempt to reinstall them each time the manifest file is applied. –Russ\nServices - Ensure the following services are running: DHCP Client DNS Client Windows Update Note You will have to find the appropriate name for each service. –Russ\nTask 3: Upload to Canvas \u0026 Contact Instructor Note Please add comments to your Puppet Manifest Files describing any Puppet Modules that must be installed prior to applying them.\nUpload your completed Puppet Manifest Files to Canvas and then contact the instructor for grading. You may continue with the next module once grading has been completed. In general, this lab does not require interactive grading, but you are welcome to request a time if you’d prefer.\n",
    "description": "",
    "tags": null,
    "title": "Assignment",
    "uri": "/2-configuration-management/03-assignment/index.html"
  },
  {
    "content": "Lab 4 - Directory Services Instructions Create four virtual machines meeting the specifications given below. The best way to accomplish this is to treat this assignment like a checklist and check things off as you complete them.\nIf you have any questions about these items or are unsure what they mean, please contact the instructor. Remember that part of being a system administrator (and a software developer in general) is working within vague specifications to provide what your client is requesting, so eliciting additional information is a very necessary skill.\nNote To be more blunt - this specification may be purposefully designed to be vague, and it is your responsibility to ask questions about any vagaries you find. Once you begin the grading process, you cannot go back and change things, so be sure that your machines meet the expected specification regardless of what is written here. –Russ\nAlso, to complete many of these items, you may need to refer to additional materials and references not included in this document. System administrators must learn how to make use of available resources, so this is a good first step toward that. Of course, there’s always Google !\nTime Expectation This lab may take anywhere from 1 - 6 hours to complete, depending on your previous experience working with these tools and the speed of the hardware you are using. Installing virtual machines and operating systems is very time-consuming the first time through the process, but it will be much more familiar by the end of this course.\nWarning Most students in previous semesters have reported that this lab is generally the most frustrating and time-consuming to complete. I recommend setting aside ample amounts of time to work on this lab. This is especially true if the system you are running your VMs on is not very powerful, since running multiple VMs at the same time may slow things down significantly. However, if you find that you are going in circles and not getting it to work, that would be a great time to ask for help. –Russ\nTask 0: Create 4 VMs For this lab, you’ll need the following VMs:\nA Windows 10 VM. You may reuse your existing Windows 10 VM from a previous lab. A Windows Server 2019 Standard (Updated Mar 2023) (or newer) VM. See Task 1 below for configuration details. An Ubuntu 22.04 VM labelled CLIENT. This should be the existing CLIENT VM from Lab 3. An Ubuntu 22.04 VM labelled SERVER. You have three options to create this VM: You can create a copy of your existing CLIENT VM from Lab 3, which does not have DHCP and DNS servers installed. Follow the instructions in the Lab 3 assignment to create a copy of that VM. In this case, you’ll need to reconfigure the VMware NAT network to handle DHCP duties. Make sure you label this copy SERVER in VMWare. This is generally the option that is simplest, and causes the least headaches. You may continue to use your exiting SERVER VM from Lab 3, with DHCP and DNS servers installed. You may choose to continue to use this server as your primary DNS and DHCP server for your VM network, which would truly mimic what an enterprise network would be like. Remember that you’ll need to have this VM running at all times to provide those services to other systems on your network. You may also choose instead to disable them and reconfigure the VMware NAT network to handle DHCP duties. Either approach is fine. This option is generally a bit closer to an actual enterprise scenario, but can also cause many headaches, especially if your system doesn’t have enough power to run several VMs simultaneously. You may create a new Ubuntu 22.04 VM from scratch, label it SERVER, and configure it as defined either in Lab 1 or using the Puppet manifest files from Lab 2. This is effectively the same as copying your CLIENT VM from Lab 3, but you get additional practice installing and configuring an Ubuntu VM, I guess. Warning Before starting this lab, make a snapshot in each VM labelled “Before Lab 4” that you can restore to later if you have any issues. In most cases, it is simpler to restore a snapshot and try again instead of debugging an error when setting up an LDAP or AD server. In addition, Task 6 below will ask you to restore to a snapshot in at least one VM before starting that step.\nTask 1: Install Windows Server 2019 Standard Create a new virtual machine for Windows Server 2019 Standard using the “Windows Server 2019 Standard (Updated Mar 2023)” installation media (you may choose a newer option if available, but this lab was tested on that specific version). You can download the installation files and obtain a product key from the Microsoft Azure Student Portal discussed in Module 1.\nTip For this system, I recommend giving the VM ample resources, usually at least 2 GB RAM and multiple processor cores if you can spare them. You may need to adjust the VM settings as needed to balance the performance of this VM against the available resources on your system.\nWhen installing the operating system, configure it as specified below:\nMake sure you choose the Desktop Experience option when installing, unless you want a real challenge! It is possible to perform these steps without a GUI, but it is much more difficult. Computer Name: cis527d-\u003cyour eID\u003e (example: cis527d-russfeld) Info This is very important, as it allows us to track your virtual machine on the K-State network in case something goes wrong in a later lab. By including both the class and your eID, support staff will know who to contact. A majority of students have missed this step in previous semesters, so don’t forget! The computer name must be changed after the Windows installation is complete –Russ\nPasswords: Use cis527_windows as the password for the built-in Administrator account Install Software VMware Tools Mozilla Firefox (you’ll thank me later) Install Windows Updates: Run Windows Update and reboot as necessary until all available updates are installed. Automatic Updates: Make sure the system is set to download and install security updates automatically. Make a snapshot of this VM once it is fully configured and updated. You can restore to this snapshot if you have issues installing Active Directory.\nTip You can use CTRL+ALT+Insert to send a CTRL+ALT+Delete to your VM in VMware without affecting your host OS.\nTask 2: Configure an Active Directory Domain Controller Configure your Windows Server as an Active Directory Domain Controller. Follow the steps and configuration details below:\nFirst, set a static IP address on your Windows Server VM. Use the IP address ending in 42 that was reserved for this use in Lab 3. For the static DNS entries, use that same IP address or the localhost IP address (127.0.0.1) as the first entry, and then use the IP address of your DNS server (either your Ubuntu Server from Lab 3 or VMware’s default gateway address, whichever option you are using) as the second DNS entry. In this way, the server will use itself as a DNS server first, and if that fails then it will use the other server. This is very important when dealing with Active Directory Domains, as the Domain Controller is also a DNS server. Follow the instructions in the resources section below to install and configure the Active Directory Domain Services role on the server. Domain Name: ad.\u003cyour eID\u003e.cis527.cs.ksu.edu (example: ad.russfeld.cis527.cs.ksu.edu) NETBIOS Domain Name: AD (this should be the default that is presented by the wizard) Passwords: Use cis527_windows for all passwords Add a User Account to your Active Directory Use your own eID for the username here, and cis527_windows as the password. Tip As of Summer 2021, there was a bug in Windows Server that prevented the built-in Administrator account from changing some settings, specifically network settings, once the server is promoted to a domain controller. This can make it difficult to fix networking issues in this or future labs. The easy fix for this is to copy the Administrator account in the Active Directory User and Computers tool and give the new copy a different name, such as “Admin”, and then use that account to log on to the server.\nResources Install Active Directory Domain Services (Level 100) from Microsoft Add User Accounts on Active Directory from Server-World AD DS Installation and Removal Wizard Page Descriptions from Microsoft Task 3: Join the Domain with Windows 10 Join your Windows 10 VM to the Active Directory Domain created in Task 2. Follow the steps and configuration details below:\nFirst, set static DNS entries on your Windows 10 VM. You SHOULD NOT set a static IP, just static DNS entries. Use the Windows Server IP address ending in 42 as the first entry, and the second entry should be the same one used on the server earlier (either your Ubuntu Server from Lab 3 or VMware’s default gateway, whichever option you are using). Join the system to the domain, following the instructions linked in the resources section below. Once the system reboots, you should be able to log in using the user account you created in Task 2. Resources Join Windows 10 PC to a Domain from Windows 10 Forums Task 4: Configure an OpenLDAP Server Install OpenLDAP on your Ubuntu VM labelled SERVER. Follow the steps and configuration details below:\nFirst, set a static IP address on your Ubuntu VM labelled SERVER, if it does not have one already. Use the IP address ending in 41 that was reserved for this use in Lab 3. For the static DNS entries, use that same IP address as the first entry to reference your DNS server from Lab 3 (see the provided model solutions if your server was not working in Lab 3), and then use the IP address for VMware’s default gateway address as the second DNS entry. In this way, the server will use itself as a DNS server first, and if that fails then it will use the other server. This is very important when dealing with LDAP domains, so that the server can properly resolve the ldap.\u003cyour eID\u003e.cis527.cs.ksu.edu address. If you haven’t already, make a snapshot of this VM that you can restore if you run into issues setting up an OpenLDAP server. Set up and configure an OpenLDAP server, following the first part of the instructions in the guide linked in the resources section below. Domain Name: ldap.\u003cyour eID\u003e.cis527.cs.ksu.edu (example: ldap.russfeld.cis527.cs.ksu.edu) Base DN: dc=ldap,dc=\u003cyour eID\u003e,dc=cis527,dc=cs,dc=ksu,dc=edu (example: dc=ldap,dc=russfeld,dc=cis527,dc=cs,dc=ksu,dc=edu) Passwords: Use cis527_linux for all passwords You DO NOT have to perform the other steps in the guide to configure TLS at this point Install LDAP Account Manager (LAM). See the video in this module for detailed instructions on how to install and configure LDAP Account Manager. Add a User Account to your OpenLDAP Directory Follow the instructions in the guide below to create ous for users, groups, and create an admin group as well. Use your own eID for the username, and cis527_linux as the password. Configure the server to use TLS. You should follow the Ubuntu Server Guide to create and sign your own certificates. Make sure you use the correct domain name! You should make a snapshot of your Ubuntu Server VM before attempting to add TLS. It is very difficult to undo any errors in this process, therefore it is much easier to just roll back to a previous snapshot and try again. At the end of the process, copy the certificate at /usr/local/share/ca-certificates/mycacert.crt to the home directory of the cis527 user for the next step. Of course, you may need to modify your firewall configuration to allow incoming connections to the LDAP server! If your firewall is disabled and/or not configured, there will be a deduction of up to 10% of the total points on this lab\nResources How to Install OpenLDAP on Ubuntu Server 22.04 from TechRepublic How To Install and Configure OpenLDAP and phpLDAPadmin on Ubuntu 16.04 from DigitalOcean (works for 22.04 as well, but PHPLDAPAdmin is not working on PHP 8.1 - use the new LDAP Account Manager instead) LDAP \u0026 TLS from the Ubuntu Server Guide Task 5: Configure Ubuntu to Authenticate via LDAP On your Ubuntu VM labelled CLIENT, configure the system to authenticate against the OpenLDAP server created in Task 4.\nFirst, confirm that you are able to resolve ldap.\u003cyour eID\u003e.cis527.cs.ksu.edu using dig on your client VM. If that doesn’t work, you may need to set a static DNS entry to point to your Ubuntu VM labelled SERVER as configured in Lab 3, or add a manual entry to your hosts file . Next, copy the mycacert.crt file from the home directory on your SERVER to the /usr/local/share/ca-certificates/ directory on the CLIENT, and then run sudo update-ca-certificates to install it. If you have configured SSH properly, you can copy the file from one server to another using scp. The command may be something like scp -P 23456 ldap.\u003cyour eID\u003e.cis527.cs.ksu.edu:~/mycacert.crt ./. Then, make sure that you can connect to the LDAP server using TLS. You can use ldapwhoami -x -ZZ -H ldap://ldap.\u003cyour eID\u003e.cis527.cs.ksu.edu and it should return anonymous if it works. Before you configure SSSD, make a snapshot of this VM. If your SSSD configuration does not work, you can restore this snapshot and try again. To test your SSSD configuration, use the command getent passwd \u003cusername\u003e (example: getent passwd russfeld) and confirm that it returns an entry for your LDAP user. To log in as the LDAP user, use the su \u003cusername\u003e command (example: su russfeld). Finally, reboot the system, and make sure you can log in graphically by choosing the “Not listed?” option on the login screen and entering your LDAP user’s credentials. Resources SSSD and LDAP on Ubuntu Server Guide (look for the “SSSD and LDAP” section) Task 6: Interoperability - Ubuntu Client on Windows Domain On your Ubuntu VM labelled CLIENT, make a snapshot labelled “OpenLDAP” to save your configuration you performed for Task 5. Open the Snapshot Manager (VM \u003e Snapshot \u003e Snapshot Manager) for that VM Restore the “Before Lab 4” Snapshot. This should take you back to the state of this VM prior to setting it up as an OpenLDAP client. Follow the instructions in the video in this module to join your Windows Active Directory Domain with your Ubuntu VM. You’ll want to confirm that you are able to resolve ad.\u003cyour eID\u003e.cis527.cs.ksu.edu using dig on your client VM. If that doesn’t work, you may need to set a static DNS entry to point to your Windows 10 server as configured in Lab 3, or add a manual entry to your hosts file . Warning If you get errors like “Insufficient permissions to join the domain”, you may need to install krb5-user and then add rdns=false to the [libdefaults] section of the /etc/krb5.conf file, as described in this thread . That seemed to fix the error for me.\nMake a snapshot labelled “ActiveDirectory” to save your configuration for this task. You can switch between snapshots to have this VM act as a client for either directory service. Resources SSSD and Active Directory from Ubuntu Join in Active Directory Domain from Server-World How to Join Ubuntu 18.04 / Debian 10 To Active Directory (AD) Domain from Computingforgeeks (works for 22.04) Task 7: Query Servers Using LDAPSearch From your Ubuntu VM labelled CLIENT, use the ldapsearch command (in the ldap-utils package) to query your Active Directory and OpenLDAP servers. Take a screenshot of the output from each command.\nBelow are example commands from a working solution. You’ll need to adapt them to match your environment. There are also sample screenshots of expected output.\nActive Directory Example Screenshot (instructive, but using old data) ldapsearch -LLL -H ldap://192.168.40.42:389 -b \"dc=ad,dc=russfeld,dc=cis527,dc=cs,dc=ksu,dc=edu\" -D \"ad\\Administrator\" -w \"cis527_windows\" OpenLDAP Example Screenshot (instructive, but using old data) ldapsearch -LLL -H ldap://192.168.40.41:389 -b \"dc=ldap,dc=russfeld,dc=cis527,dc=cs,dc=ksu,dc=edu\" -D \"cn=admin,dc=ldap,dc=russfeld,dc=cis527,dc=cs,dc=ksu,dc=edu\" -w \"cis527_linux\" Tip You’ll be asked to perform each of these commands as part of the grading process, but the screenshots provide good insurance in case you aren’t able to get them to work –Russ\nTask 8: Make Snapshots In each of the virtual machines created above, create a snapshot labelled “Lab 4 Submit” before you submit the assignment. The grading process may require making changes to the VMs, so this gives you a restore point before grading starts. Also, you may have multiple Lab 4 snapshots on some VMs, so feel free to label them accordingly.\nTask 9: Schedule A Grading Time Contact the instructor and schedule a time for interactive grading. You may continue with the next module once grading has been completed.\n",
    "description": "",
    "tags": null,
    "title": "Assignment",
    "uri": "/4-directory-services/03-assignment/index.html"
  },
  {
    "content": "Lab 5 - The Cloud Instructions Create two cloud systems meeting the specifications given below. The best way to accomplish this is to treat this assignment like a checklist and check things off as you complete them.\nIf you have any questions about these items or are unsure what they mean, please contact the instructor. Remember that part of being a system administrator (and a software developer in general) is working within vague specifications to provide what your client is requesting, so eliciting additional information is a very necessary skill.\nNote To be more blunt - this specification may be purposefully designed to be vague, and it is your responsibility to ask questions about any vagaries you find. Once you begin the grading process, you cannot go back and change things, so be sure that your machines meet the expected specification regardless of what is written here. –Russ\nAlso, to complete many of these items, you may need to refer to additional materials and references not included in this document. System administrators must learn how to make use of available resources, so this is a good first step toward that. Of course, there’s always Google !\nTime Expectation This lab may take anywhere from 1 - 6 hours to complete, depending on your previous experience working with these tools and the speed of the hardware you are using. Configuring cloud systems is very time-consuming the first time through the process, but it will be much more familiar by the end of this course.\nInfo This lab involves working with resources on the cloud, and will require you to sign up and pay for those services. In general, your total cost should be low, usually around $20 total. If you haven’t already, you can sign up for the GitHub Student Developer Pack to get discounts on most of these items.\nYou can get $200 credit at DigitalOcean using this link: https://try.digitalocean.com/freetrialoffer/ You can register a .me domain name for free using Namecheap at this link: https://nc.me/ If you have any concerns about using these services, please contact me to make alternative arrangements! –Russ\nTask 0: Create 2 Droplets Create TWO droplets on DigitalOcean. As you set up your droplets, use the following settings:\nChoose the Ubuntu 22.04 x64 distribution as the droplet image Select the smallest droplet size ($4-6/mo) Select any United States region Enable Virtual Private Cloud (VPC) Networking and Monitoring You may add any existing SSH keys you’ve already configured with DigitalOcean during droplet creation Droplet names: cis527\u003cyour eID\u003e-frontend cis527\u003cyour eID\u003e-backend The rest of this assignment will refer to those droplets as FRONTEND and BACKEND, respectively.\nResources How to Create a Droplet from the DigitalOcean Control Panel from DigitalOcean Virtual Private Cloud (VPC) from DigitalOcean (replaced private networking) Task 1: Configure Droplets Perform these configuration steps on both droplets, unless otherwise noted:\nCreate a cis527 user with administrative (root or sudo) privileges Warning DO NOT REUSE THE USUAL PASSWORD ON THIS ACCOUNT! Any system running in the cloud should have a very secure password on each account. Make sure it is a strong yet memorable password, as you’ll need it to run any commands using sudo.\nInstall all system updates Change the SSH port to 54321 Ensure the timezone is set to UTC Enable the firewall. Configure the firewall on both systems to allow connections to the following: incoming port 54321 (SSH) BACKEND ONLY: filter connections on port 54321 to only allow SSH connections from FRONTEND via its private networking IP address. You should still allow connections to port 80 and 443 from any address. incoming port 80 (HTTP) incoming port 443 (HTTP via TLS) Warning Many students misconfigure the firewall on the BACKEND server to allow SSH connections from the wrong IP addresses or ranges, or include a rule to allow connections from any address. I will be picky about this from now on. –Russ\nResources Initial Server Setup with Ubuntu 22.04 from DigitalOcean UFW Essentials: Common Firewall Rules and Commands from DigitalOcean How To Set Up Time Synchronization on Ubuntu 20.04 from DigitalOcean (works on Ubuntu 22.04) Task 2: SSH Configuration Configure your SSH servers and SSH keys as described here:\nOn your own computer, generate a set of SSH keys if you have not already. Add the public key from your computer to the cis527 account on FRONTEND. This should allow you to log in with that key. Add the grading SSH key to the cis527 account on FRONTEND as well. On the cis527 account on FRONTEND, generate a set of SSH keys with no passphrase. Add the public key from the cis527 account on FRONTEND to the cis527 account on BACKEND. This should allow you to log in with that key On the cis527 account on FRONTEND, create an SSH config file such that a user could simply type ssh backend to connect to the BACKEND droplet. Tip Make sure you use the private networking IP address for BACKEND in your config file. Otherwise, it will be blocked by the firewall.\nOnce all of the keys are in place, disable password authentication and root login via SSH on both systems. After doing these steps, you should only be able to access the cis527 account on FRONTEND via SSH using your SSH key or the grading SSH key, and you should only be able to access BACKEND using the SSH key present on the cis527 account on FRONTEND.\nNote You may contact me once you have installed the grading SSH key to confirm that it works correctly. I’d be happy to test it before grading. –Russ\nResources Extras - SSH How Does SSH Work from Hostinger SSH Essentials: Working with SSH Servers, Clients and Keys from DigitalOcean How to Set Up SSH Keys on Ubuntu 22.04 from DigitalOcean Simplify Your Life With an SSH Config File from Nerderati Task 3A: Install Apache on BACKEND Install the Apache web server on BACKEND. By default, the webserver should serve files from the /var/www/html directory. Place a simple HTML file named index.html in that directory on BACKEND. You may use the contents below as an example. Please modify the file appropriately to make it clear which server it is placed on.\nDo not configure virtual hosts at this time, as that will be covered in Task 5.\n\u003chtml\u003e \u003chead\u003e \u003ctitle\u003eCIS 527 Backend\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eThis is my CIS 527 Backend Server!\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003eTo test your system, you should be able to enter the public IP address of your BACKEND droplet in a web browser and be presented with the appropriate file.\nResources How To Install the Apache Web Server on Ubuntu 22.04 from DigitalOcean Task 3B: Install Docker on FRONTEND Install the Docker client, Docker engine, and Docker compose on the droplet named FRONTEND. Make sure you test your setup using the Hello World image to confirm it is working.\nResources Install Docker Engine on Ubuntu from Docker Install Docker Compose Plugin from Docker Task 4: Domain Names \u0026 DNS Register and configure a domain name, and add your new droplets to that domain.\nInfo If you already have your own domain name, you are welcome to use it for this portion of the lab. It should not conflict with any existing configuration, as long as you are managing your own DNS records. If not, you may need to perform some additional configuration. If you don’t have a domain name yet, this would be a great chance to get one registered. Namecheap will allow you to register a .me domain for free for one year as a student. If you register a domain name, I highly recommend enrolling in WhoisGuard to protect your personal information. It should be enabled for you automatically through Namecheap. If you have any concerns about registering a domain name, or would like to explore options for completing this portion without registering or using a public domain name, please contact me. –Russ\nConfigure the DNS settings for your domain name as follows:\nIf you are using a new domain, make sure it is configured to use your registrar’s DNS servers. You may also configure it to use DigitalOcean’s nameservers, and configure your DNS settings through DigitalOcean. Add an A record for host cis527alpha that points to the public IP address of FRONTEND. Add an A record for host cis527bravo that points to the public IP address of FRONTEND. Add an A record for host cis527charlie that points to the public IP address of BACKEND. Tip After updating your domain’s DNS settings, you may have to wait up to 24 hours for the changes to propagate across the internet due to DNS caching. You may be able to speed this up by restarting your computer and network devices, or by using 3rd party DNS services such as OpenDNS or Google DNS instead of your ISP’s DNS servers. However, in most cases it is better to just be patient and wait than to try and get around it. –Russ\nTo test your new DNS settings, you should be able to enter http://cis527charlie.\u003cyourdomain\u003e.\u003ctld\u003e in a web browser to access your backend server running Apache. For example, if your domain name is cis527.me, you would visit http://cis527charlie.cis527.me. Since we haven’t configured a server for frontend yet, we aren’t able to test it at this time.\nResources DNS Overview from DigitalOcean How to Point to DigitalOcean Nameservers from Common Domain Registrars from DigitalOcean Task 5A: Configure Apache Virtual Hosts Now that your domain name is working, configure an appropriate virtual host in Apache on BACKEND. In general, you can follow Step 5 of the guide linked below, but replace example.com with your server’s full domain name, such as cis527charlie.cis527.me in the example from Task 4. You’ll also need to copy the sample HTML file from Task 3 to the appropriate directory as configured in your virtual host. Make sure you disable the default site configuration when you enable the new site.\nFinally, you can test your virtual host configuration using the same URL given in Task 4 above.\nResources How To Install the Apache Web Server on Ubuntu 22.04 from DigitalOcean Task 5B: Configure Docker Reverse Proxy On FRONTEND, create a docker-compose.yml file in the home directory of the cis527 user that will create the following infrastructure in Docker:\nSet up two Docker containers running simple web servers. You may either use the whoami image from the example, or set up two Nginx containers. If you use Nginx, you’ll need to configure it to host different static content in each container (so it is easy to tell which one is which). See the documentation for how to set this up. Basically, there needs to be an obvious way to tell that you are reaching the correct container. These containers should only be connected to an internal Docker network. They should NOT have direct access to the internet, nor should they have any mapped ports. Set up a reverse proxy in Docker to handle connections from the outside world (on port 80) to the appropriate containers. You may use either Nginx , Nginx-proxy , or Traefik Proxy as shown in the lab module. This container should be connected to both the default Docker network as well as the internal network that is connected to the other two containers. You should configure one web server container to have hostname cis527alpha.\u003cyourdomain\u003e.\u003ctld\u003e and the other should have cis527bravo.\u003cyourdomain\u003e.\u003ctld\u003e. Once configured, you should be able to visit those URLs in a browser and clearly see information coming from the correct Docker container. Resources Reverse Proxy Task 6: Public Key Certificates Obtain and install a public key certificate for your Apache server on BACKEND. The simplest way to do so is to use Certbot from Let’s Encrypt.\nWhen you install the certificates, direct Certbot to redirect HTTP traffic to HTTPS for your server.\nOnce it is complete, you can test your certificates using the same URL given in Task 4 above. It should automatically redirect you from HTTP to HTTPS. You may have to clear the cache in your web browser if it does not work correctly. When you access the site, use your web browser to verify that the SSL certificate is present and valid.\nYou DO NOT have to configure public key certificates on FRONTEND using a reverse proxy. This can be done, but it is a bit more difficult than using Certbot since it requires manual steps or additional configuration. Feel free to attempt it on your own!\nResources Certbot from the Electronic Frontier Foundation (EFF) How To Secure Apache with Let’s Encrypt on Ubuntu 22.04 from DigitalOcean Task 7: Schedule A Grading Time Contact the instructor and schedule a time for interactive grading. You may continue with the next module once grading has been completed.\n",
    "description": "",
    "tags": null,
    "title": "Assignment",
    "uri": "/5-the-cloud/03-assignment/index.html"
  },
  {
    "content": "Lab 6 - Application Servers Instructions Create two cloud systems and four virtual machines meeting the specifications given below. The best way to accomplish this is to treat this assignment like a checklist and check things off as you complete them.\nIf you have any questions about these items or are unsure what they mean, please contact the instructor. Remember that part of being a system administrator (and a software developer in general) is working within vague specifications to provide what your client is requesting, so eliciting additional information is a very necessary skill.\nNote To be more blunt - this specification may be purposefully designed to be vague, and it is your responsibility to ask questions about any vagaries you find. Once you begin the grading process, you cannot go back and change things, so be sure that your machines meet the expected specification regardless of what is written here. –Russ\nAlso, to complete many of these items, you may need to refer to additional materials and references not included in this document. System administrators must learn how to make use of available resources, so this is a good first step toward that. Of course, there’s always Google !\nTime Expectation This lab may take anywhere from 1 - 6 hours to complete, depending on your previous experience working with these tools and the speed of the hardware you are using. Configuring application servers is very time-consuming the first time through the process, but it will be much more familiar by the end of this course.\nInfo This lab involves working with resources on the cloud, and will require you to sign up and pay for those services. In general, your total cost should be low, usually around $20 total. If you haven’t already, you can sign up for the GitHub Student Developer Pack to get discounts on most of these items. If you have any concerns about using these services, please contact me to make alternative arrangements! –Russ\nTask 0: Droplets \u0026 Virtual Machines For this lab, you will continue to use the two DigitalOcean droplets from Lab 5, labelled FRONTEND and BACKEND, respectively. This assignment assumes you have completed all steps in Lab 5 successfully; if not, you should consult with the instructor to resolve any existing issues before continuing.\nYou will also need a Windows Server 2019 VM configured as an Active Directory Domain Controller, along with a Windows 10 VM added as a client computer on that domain. In general, you may continue to use the resources created in Lab 4, but you may choose to recreate them as directed in Lab 4 if desired.\nIn addition, you will need two Ubuntu VMs, one labelled SERVER and the other labelled CLIENT. You may continue to use the Ubuntu VMs from Labs 3 and 4, or create new VMs for this lab. This lab does not assume any existing setup on these VMs beyond what is specified in Labs 1 and 2. You should also make sure your Ubuntu VM labelled SERVER has a static IP address.\nTask 1: Windows File Server Configure a file server on your Windows Server 2019 VM. It should have the following features:\nA shared folder on the server named public and stored at C:\\public that should be accessible by all users on your domain A shared folder on the server named admins and stored at C:\\admins that should only be accessible to users in the Domain Admins group in your domain Tip As of Summer 2021, there was a bug in Windows Server that prevented the built-in Administrator account from changing some settings, specifically network settings, once the server is promoted to a domain controller. This can make it difficult to fix networking issues in this or future labs. The easy fix for this is to copy the Administrator account in the Active Directory User and Computers tool and give the new copy a different name, such as “Admin”, and then use that account to log on to the server.\nResources How to Share Files and Folders in Windows Server 2016 from Tactig (should work for Server 2019) Task 2: Windows Group Policy Configure group policy objects (GPOs) on your Windows Active Directory domain to perform the following tasks:\nAll domain users should get the public folder automatically mapped to the Z:\\ drive on any system they log into. Users in the Domain Admins group should also get the admins folder automatically mapped to the Y:\\ drive on any system they log into. That drive should not be mapped for any user that is not a member of the Domain Admins group. Tip Pay close attention to how you attach and target these GPOs in the domain. You can use the domain Administrator account and the other domain account created in Lab 4 to test these on your Windows 10 client. –Russ\nResources Windows Group Policy How to Map Network Drives with Group Policy (Complete Guide) by Robert Allen on Active Directory Pro How to: Mapping Network Drives/Folders via Group Policy on Spiceworks Community Task 3: Ubuntu File Server Configure a file server using Samba on your Ubuntu VM labelled SERVER. It should have the following features:\nA shared folder on the server named public and stored at /public that should be accessible by all Samba users Enable shared home directories in Samba using the default [homes] share. Enable the cis527 user in the Samba password database. It should use the same cis527_linux password as the actual cis527 account. Of course, you may need to modify your firewall configuration to allow incoming connections to the file server! If your firewall is disabled and/or not configured, there will be a deduction of up to 10% of the total points on this lab\nResources File Server from Ubuntu Documentation Samba Server Guide from Ubuntu Community Help Wiki How to Set Up a Samba Share for a Small Organization on Ubuntu 16.04 from DigitalOcean (works for 20.04) Access User’s Home Folders via Samba on Ubuntu 17.04 from Website for Students (works for 20.04) How to Configure Samba Server Share on Ubuntu 20.04 Focal Fossa Linux from LinuxConfig.org Task 4: Ubuntu Drive Mapping Configure your Ubuntu VM labelled CLIENT to automatically access the Samba shares in the following manner:\nAdd an entry to \\etc\\fstab to automatically mount the public folder to /mnt/public at system boot. It should be readable and writable by all users. Use libpam-mount to automatically mount a user’s homes share from the server at login. This only needs to work for the cis527 user, as that user should be present on both systems and Samba. Note To be honest, this last part can be pretty tricky. I recommend following the instructions in this video in this module very carefully. If you have any issues, you can enable debugging and review /var/log/syslog for errors. –Russ\nResources Mount Windows Shares Permanently from Ubuntu Wiki How Do I Access Windows Shares from Bash on Ask Ubuntu Forums Use of pam-mount to Mount Home from Server on Ubuntu Forums pam_mount on Ubuntu Manpages pam_mount.conf on Ubuntu Manpages Task 5: Windows Web Application Server For this task, you will install and configure a .NET web application for IIS on your Windows Server 2016 VM. First, choose an application to install from the following list:\nBlogEngine.NET NOTE: If you choose BlogEngine.NET, make sure you read their site carefully. You don’t have to sign up for anything on their site to download the software itself, but the download link tends to be hidden in favor of their hosted options. As a sysadmin, you should definitely get into the habit of carefully reading and considering what you find online before you click! If you would like to work with an application not listed here, please contact the instructor. The application should have some sort of functionality beyond just displaying static pages. Any approved application can be added to this list for you to use. You are not allowed to use JitBit’s .NET Forum, as that was demonstrated in the video in this module.\nOnce you have selected your application, perform the following configuration steps:\nCreate two websites in IIS: blog.\u003cyour eID\u003e.cis527.cs.ksu.edu and site.\u003cyour eID\u003e.cis527.cs.ksu.edu. They should be stored in C:\\inetpub\\blog and C:\\intepub\\site, respectively. For the blog site, make sure you choose the .NET v4.5 Application Pool! Add a DNS forward lookup zone for \u003cyour eID\u003e.cis527.cs.ksu.edu to the Windows DNS server, and then add A records for the two sites described above. They should both point to the Windows Server’s IP address ending in .42. Place a static HTML file inside of the C:\\intepub\\site folder and confirm that you can access it using Firefox at http://site.\u003cyour eID\u003e.cis527.cs.ksu.edu Follow the instructions to install and configure your chosen application in C:\\inetpub\\blog. Pay special attention to any file permissions required. Use the IIS_IUSRS group when adding write permissions to any folders as described in the instructions. You should be able to access it at http://blog.\u003cyour eID\u003e.cis527.cs.ksu.edu using Firefox. Create a self-signed SSL certificate and attach it to both websites by adding an additional binding for HTTPS. Make sure you can access both websites using https://. Use the URL Rewrite module to configure URL redirection to automatically direct users from HTTP to HTTPS for both websites. Once these steps are complete, visiting http://blog.\u003cyour eID\u003e.cis527.cs.ksu.edu in your web browser should automatically redirect you to https://blog.\u003cyour eID\u003e.cis527.cs.ksu.edu and it should be secured using your self-signed certificate. You should also be able to demonstrate that the application is working properly by interacting with it in some meaningful way, such as logging in and making a new post on a blog. Finally, if you visit http://site.\u003cyour eID\u003e.cis527.cs.ksu.edu you should see the static content from that site instead of the blog, and it should also properly redirect to HTTPS.\nNote I recommend using Firefox for testing. Edge \u0026 Internet Explorer on Windows Server are locked-down by default and can be very frustrating to work with. See, I knew you’d appreciate having Firefox installed on your Windows server! –Russ\nResources URL Rewrite from Microsoft Add A Website to Windows Server 2016 using Host Headers from Ionos by 1\u00261 How to add DNS Forward Lookup Zone in Windows Server 2019 from Computingforgeeks How to add DNS A/PTR Record in Windows Server 2019 from Computingforgeeks How to Create a Self Signed Certificate in IIS from AboutSSL Microsoft Server 2016 - IIS 10 \u0026 10.5 - SSL Installation from SSLSupportDesk How to Install a SSL Certificate on IIS 10 from SSLs.com Setting up an HTTP/HTTPS Redirect in IIS from Namecheap Task 6: Ubuntu Web Application Server For this step, you will install and configure a web application running on Apache in Ubuntu on your DigitalOcean droplets. First, choose an application to install from the following list:\nWordpress If you would like to work with an application not listed here, please contact the instructor. The application should have some sort of functionality beyond just displaying static pages, and must support using a MySQL database on a separate host from the web server. In addition, the application must be installed manually - using pre-built images or Apt packages is not allowed here. Any approved application can be added to this list for you to use. You are not allowed to use phpBB, as that was demonstrated in the video in this module.\nOnce you have selected your application, choose ONE of the following configuration options:\nOption 1: Bare Hardware Install MySQL (and optionally phpMyAdmin) on your Ubuntu droplet labelled BACKEND and configure an appropriate username and database for your application. You should also enable SSL/TLS encryption on connections to the server if it is not already enabled in MySQL (this should be enabled by default in Ubuntu 20.04). When creating the user account in MySQL, make sure it is set to log in from the private network IP address of FRONTEND. Tip You may need to configure MySQL to listen on an external network interface. Make sure you use the private network IP address only - it should not be listening on all network interfaces. In addition, you will also have to open ports on the firewall, and you should restrict access to those ports to only allow connections from the private network IP address of FRONTEND, just like the SSH server in Lab 5. Points will be deducted for having a MySQL server open to the internet! –Russ\nInstall Apache and configure a new virtual host in Apache for your web application on FRONTEND. Also, add an appropriate A record to your domain name created in Lab 5 for this virtual host. You may shut down any Docker containers from Lab 5 that interfere with this configuration. Install your web application on your Ubuntu droplet labelled FRONTEND following the application’s installation instructions. When configuring the database for your application, you should have it use the MySQL database on BACKEND via the private network IP address. Of course, you may need to modify your firewall configuration to allow incoming connections to the database server! If your firewall is disabled and/or not configured, there will be a deduction of up to 10% of the total points on this lab Option 2: Docker Create two Docker containers on FRONTEND, one containing MySQL and another containing Wordpress. You may optionally add a container running phpMyAdmin if desired. The MySQL container must be isolated on its own internal network that cannot access the outside internet. Add an appropriate A record to your domain name created in Lab 5 for this docker container. You will also need to update your reverse proxy to properly route traffic to the Wordpress container. Make sure that Wordpress is properly configured via environment variables in Docker. Once these steps are complete, you should be able to visit your web application via HTTP and then interact with the application in some meaningful way to confirm that the database connection is working.\nResources Docker Wordpress Image Install MySQL on Ubuntu 20.04 LTS Linux from LinuxConfig.org How To Install MySQL on Ubuntu 22.04 from DigitalOcean (follow instructions in guide above to configure MySQL to listen on all network interfaces) How To Install and Secure phpMyAdmin on Ubuntu 22.04 from DigitalOcean How To Install the Apache Web Server on Ubuntu 22.04 from DigitalOcean How To Secure Apache with Let’s Encrypt on Ubuntu 22.04 from DigitalOcean How To Set Up A Remote Database to Optimize Site Performance with MySQL on Ubuntu 20.04 from DigitalOcean (works for 22.04) Task 7: Make Snapshots In each of the virtual machines created above, create a snapshot labelled “Lab 6 Submit” before you submit the assignment. The grading process may require making changes to the VMs, so this gives you a restore point before grading starts.\nTask 8: Schedule A Grading Time Contact the instructor and schedule a time for interactive grading. You may continue with the next module once grading has been completed.\n",
    "description": "",
    "tags": null,
    "title": "Assignment",
    "uri": "/6-application-servers/03-assignment/index.html"
  },
  {
    "content": " YouTube Video Resources Advanced Bash-Scripting Guide from The Linux Documentation Project (TLDP) Advanced Command Line How To from Ubuntu Beginners / Bash Scripting from Ubuntu Environment Variables from Ubuntu Cron How To from Ubuntu Magic Number on Wikipedia Video Transcript In this video, I’ll introduce the concepts for scripting using the Bash shell in Ubuntu Linux. In essence, scripting allows you to automate many common tasks that you would perform using the terminal in Linux. It is a very powerful skill for system administrators to learn.\nFirst, I’m going to create a bin folder inside of our home folder (this will become useful later).\nmkdir ~/bin In that command, the tilde ~ character represents your home folder. It is one of the special folder paths that you can use on the terminal in Linux. Now, we can open that folder:\ncd ~/bin To begin, let’s open a text file using Nano in the terminal. I’m going to use the .sh file extension to make it clear that this is a script, but that is not necessary on Linux.\nnano script.sh At the top of the file, we must define the script interpreter we would like to use. For Bash, place this line at the very top of the file:\n#!/bin/bash Some experienced system administrators will refer to the start of that line as a “sha-bang,” which may make it easy to remember. It is actually a two-byte magic number that tells Linux what type of file it is reading, and then the rest of the line gives the path to the program that can interpret that file. This allows Linux to determine the type of the file even without checking the file extension, which is what Windows does.\nBelow that line, you can simply place terminal commands, one per line, that you wish to have the script perform. For example, here is a simple Hello World script:\n#!/bin/bash echo \"Hello World\" exit 0 The echo command is pretty self-explanatory - it just prints text to the console. The final line gives the exit condition for the script. Returning zero 0 indicates that the script completed successfully. Any non-zero value is treated as an error, and can be used to diagnose failing scripts. See Chapter 6 of the Advanced Bash-Scripting Guide from TDLP for more information on exit conditions.\nTo run the script, save and exit the file using CTRL+X, then Y, then ENTER. Then, you can simply type:\n./script.sh and see if it works. Here, we are using the dot, or period ., as part of the path to the script. A single dot represents the current directory, so putting ./ in front of a file indicates that we want to run the script with that name from the current directory. At the end of the video, we’ll discuss how to modify your system so you don’t have to include the path to the script.\nUnfortunately, it doesn’t run. This is because Linux has a separate file permission to allow files and scripts to be executed. By default, most files aren’t given that permission, so we have to manually add it before executing the script. To do so, type:\nchmod u+x script.sh This will give the owner of the file u the execute permission +x. Then, try to run it again:\n./script.sh It should display “Hello World” on the screen. Congratulations, you’ve written your first Bash script!\nNow, let’s take a look at a more complex script. This is one I actually wrote years ago when I first interviewed for an instructor position here at K-State:\n#!/bin/bash #batman.sh if (( $# \u003c 1 || $# \u003e 1 )); then echo \"Usage: batman.sh \u003cname\u003e\" exit 1 fi if [ \"$1\" = \"Batman\" ]; then echo \"Hello Mr. Wayne\" elif [ \"$1\" = \"Robin\" ]; then echo \"Welcome back Mr. Grayson\" else echo \"Intruder alert!\" fi exit 0 There is quite a bit going on in this script, so let’s break it down piece by piece. First, this script includes parameters. The $1 variable references the first parameter, and obviously $2 would be the second, and so on. For parameter values above 9, include the number in curly braces, such as ${10} and ${11}.You can also access the number of parameters provided using $#, and the entire parameter string as $*.\nBelow that, the first if statement:\nif (( $# \u003c 1 || $# \u003e 1 )); then echo \"Usage: batman.sh \u003cname\u003e\" exit 1 fi uses double parentheses, representing arithmetic evaluation. In this case, it is checking to see if the number of parameters provided is either greater than or less than 1. If so, it will print an error message. It will also exit with a non-zero exit condition, indicating that the script encountered an error. Lastly, note that if statements are concluded with a backwards if, or “fi”, indicating the end of the internal code block. In many other programming languages, curly braces ({ and }) are used for this purpose.\nThe next if statement:\nif [ \"$1\" = \"Batman\" ]; then echo \"Hello Mr. Wayne\" elif [ \"$1\" = \"Robin\" ]; then echo \"Welcome back Mr. Grayson\" else echo \"Intruder alert!\" fi uses square brackets for logical test comparisons. This is equivalent to using the Linux “test” command on that statement. Here, the script is checking to see if the string value of the first parameter exactly matches “Batman”. If so, it will print the appropriate welcome message.\nSo, this simple script introduces two different methods of comparison, as well as conditional statements and parameters.\nHere’s another simple script to introduce a few other concepts:\n#!/bin/bash #listfiles.sh files=`ls $* 2\u003e /dev/null` for afile in \"$files\"; do echo \"$afile\" done exit 0 The first line of the script:\nfiles=`ls $* 2\u003e /dev/null` declares a new variable $files, and sets its value to the output of the command contained in backticks `. Inside of a script, any commands contained in backticks will be converted to the output of that script, which can then be stored in a variable for later use.\nThe next line:\nfor afile in \"$files\"; do represents a “foreach” loop. It will execute the code inside of that statement once for each item in the $files variable. Note that when variables are declared and assigned, they don’t require the dollar sign $ in front of them, but when they are used it must be included. The loop is concluded with a “done” line at the bottom.\nFinally, let’s review a couple more scripts:\n#!/bin/bash #simplemenu.sh OPTIONS=\"Build Run Clean Quit\" select opt in $OPTIONS; do if [ \"$opt\" = \"Build\" ]; then echo \"Building project...\" elif [ \"$opt\" = \"Run\" ]; then echo \"Running project...\" elif [ \"$opt\" = \"Clean\" ]; then echo \"Cleaning project...\" elif [ \"$opt\" = \"Quit\" ]; then echo \"Exiting...\" break else echo \"Invalid Input\" fi done exit 0 This script creates a simple menu with four options, stored in the $OPTIONS variable at the top. It then uses a “select” statement, telling the terminal to ask the user to choose one of the options provided, and then inside the select statement is a set of “if” statements determining which option the user chose. This is very similar to a “case” statement in many other programming languages. When you run this script, note that it repeats the options until it hits an option containing a “break” statement, so it is not necessary to encapsulate it in a loop.\nLastly, Bash scripts also provide the ability to read input from the user directly, instead of using command-line parameters:\n#!/bin/bash #simpleinput.sh echo \"Input your name and press [ENTER]\" read name echo \"Welcome $name!\" exit 0 This script uses the “read” command to read input from the user and store it in the $name variable. Pretty simple, right?\nIf you plan on writing scripts for your own use, there is one thing you can do to greatly simplify the process of using them. Notice that, currently, you must provide the full path to the script using either ./ or some other path. This is because, by default, Linux will look for commands and scripts in all of the folders contained in your PATH environment variable, but not the current folder. So, we must simply add the folder containing all of our scripts to the PATH variable.\nIn recent versions of Ubuntu, all you have to do is create a bin folder in your home folder, as we did above. Then, restart the computer and it should automatically add that folder to your path. This is because of a setting hidden in your Bash profile configuration file, usually stored in ~/.profile. The same process works for the Windows Subsystem for Linux (WSL) version of Ubuntu as well.\nYou can check the current PATH variable using the following command:\necho $PATH If you don’t see your ~/bin folder listed there after you’ve created it and rebooted your computer (it will usually have the full path, as in /home/cis527/bin or similar), you’ll need to add it manually. To do this, open your Bash settings file:\nnano ~/.bashrc And, at the very bottom of the file, add the following line:\nexport PATH=$PATH:$HOME/bin/ Then, save the file, close, and reopen your terminal window. You can check your PATH variable once again to confirm that it worked.\nNow, you can directly access any script you’ve created and stored in the ~/bin folder without providing a path. So, for example, the very first script we created, stored in ~/bin/script.sh, can now be accessed from any folder just by typing\nscript.sh on the terminal.\nThis is a very brief introduction to Bash scripting. I highly recommend reading the Advanced Bash-Scripting Guide from The Linux Documentation Project (TLDP) if you’d like to learn even more about scripting.\n",
    "description": "",
    "tags": null,
    "title": "Bash Scripting",
    "uri": "/x-extras/03-bash-scripting/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Docker Tutorial for Beginners by TechWorld with Nana on YouTube Docker Manuals from Docker Docker Official Images on DockerHub Docker Hello World on DockerHub Docker Nginx on DockerHub Video Transcript Now that we’ve covered the general architecture of containers, let’s dive a bit deeper into Docker itself. Docker is by far the most commonly used tool for creating and managing containers on a small scale, and learning how to use it is a super useful skill for just about any one in a system administration or software development field. So, let’s take a look at!\nFirst, let’s go over some general terminology related to containers. Recall that an image is a read-only template that is used to build a container. The image itself consists of several layers, and may be built on top of another existing image.\nWhen we launch an image, it is instantiated into a container. A container is a running instance of an image, and it includes the read/write layer as well as any connections to resources such as networks, volumes, and other tools.\nFinally, a volume in Docker is a persistent storage location for data outside of a container. We’ll explore how to create volumes and store data from a container later in this lab.\nDocker itself consists of several different components. The Docker engine, also known as the Docker daemon, is the tool that is installed on the system that will host the containers. It manages creating containers from images and connecting them to various resources on the system.\nThe Docker client is the command-line tool that we use to interface with the Docker engine. On systems with a GUI, we can also install Docker Desktop, which acts as a client for interfacing with Docker, but it also includes the Docker engine. In many cases, installing Docker desktop is the best way to get started with Docker on our own systems, but when working in the cloud we’ll generally install the traditional Docker engine and client.\nAlong with the Docker client, we have another tool known as Docker compose. Docker compose allows us to build configuration files that define one or more containers that should work together, and we can also use Docker compose to easily start and stop multiple containers quickly and easily. I generally prefer working with Docker compose instead of the Docker client, but it is really just personal preference. There are also 3rd party tools that can interface with Docker, such as Portainer, that can do much of the same work.\nFinally, we also need to know about registries for container images. Docker Hub is by far the most well known of these, but it is also important to know that both GitHub and GitLab can act as a registry for Docker images. Some companies, such as Microsoft, also choose to host their own registries, so many of the .NET Docker images must be downloaded directly from Microsoft’s own registry instead of Docker Hub. For this course, we’ll just use Docker Hub and the official images hosted there.\nSo, to get started with Docker, there are several commands that we’ll end up using. We’ll see these put into practice over the next few parts of this lab, so don’t worry about memorizing them right now, but I want to briefly review them so you’ll know what they are for when you see them used in the later examples.\nFirst, to get an image from an image registry, we use the docker pull command. This will download the image and store it locally on your system. To review the images available on your system, you can use the docker images commands.\nOnce we have an image, we can instantiate it into a container using the docker run command. Thankfully, docker run is also smart enough to download an image if we don’t have it locally, so in many cases it is sufficient to simply use docker run to both get an image and start a container.\nOnce a container is created, we can use docker stop and docker start to stop and start an existing container. This is helpful if we want to effectively “pause” a running container without losing any of its configuration.\nTo see the containers that are currently running, we can use docker ps. We can also see all containers, including those that are stopped, using docker ps -a.\nAnother very useful command is docker exec, which allows us to run a command within a running container. This is very useful if we need to perform some configuration within the container itself, but we can also use it to open an interactive shell directly within a container.\nFinally, most applications running within Docker containers are configured to print logs directly to the terminal, and we can view that output using the docker logs commands. This is very helpful when a container isn’t working like we expect, so we can explore the output produced by the application running in the container to help us debug the problem.\nBefore we can start a container, we must find an image that we’d like to run. To do that, one of the first things we can do is just head to Docker Hub itself and start searching for an image. However, as we’ve already heard, there are other registries available, so if we can’t find what we are looking for on Docker Hub, we may want to check sites such as GitHub or the application’s website to learn about what images are available.\nOnce we’ve found an image we’d like to run, we need to consider a couple of other factors. First, images on Docker Hub and other registries are marked with various tags. The tags are used to identify the particular version or architecture of the image. So, let’s take a look at the official image for Nginx to see what tags are available. (Navigate to Nginx).\nHere, we see many various versions of Nginx available, including images that are configured to include the perl module, and images based on the Alpine Linux project. If we scroll down the page a bit, we can see a discussion of the various image variants and how they differ.\nMost Docker images are based on one of the mainline Linux distributions, usually either Debian or Ubuntu. However, one of the image variants offered by Nginx, as well as many other applications, is based on the Alpine Linux project. Alpine is a very small Linux distribution that uses very compact libraries and doesn’t include a lot of tools, making it ideal as a basis for very small Docker images. However, this can make it more difficult to work with Alpine-based images, since there aren’t many tools included that can help with debugging. Likewise, building an image based on Alpine requires much more knowledge of the system and what libraries need to be included.\nBasically, if you aren’t sure, it is generally best to go with an image based on a mainline Linux distribution you are familiar with. However, if you’d like to make your images as small as possible, you can choose to use an Alpine-based image instead.\nFinally, if we scroll back to the top of the page, you’ll see that the Nginx image is tagged as a “Docker Official Image” meaning that it is one of the curated repositories that has been reviewed by Docker and will generally be well maintained and updated. You can click on that tag to learn more, and even learn how to search Docker Hub for other official images. When in doubt, it is best to look for one of these official images anytime you want to use a particular application.\nFor this example, we’re going to use another Docker Official Image called Hello World. This image is a great example of a minimal container, making it an easy place to get started with Docker. So, now that we’ve found the image we want to use, let’s go back to the terminal and learn how to create and run some containers.\nBefore you begin, you’ll need to install the Docker platform on your system. At the top of this page are links directly to the Docker instructions for installing Docker on various systems. If you are working on your local system or in a virtual machine, you will probably want to install Docker Desktop, since that handles everything for you. For installations in the cloud, you’ll need to install the Docker engine and client directly, and you may also want to add the Docker Compose plugin as well.\n[Demo Here]\nThere we go! That’s a quick overview of using Docker to create and run containers. However, this small example leaves many questions unanswered! For example, how can we access our Nginx server outside of the Docker container itself? What if we want to host our own files in Nginx instead of the default webpage? Also, how can we store data from our containers so that it isn’t lost each time the container restarts. And finally, is there a way to simplify these Docker commands so they are easy to replicate multiple times?\nOver the next few parts of this lab, we’ll work on addressing each of these questions.\n",
    "description": "",
    "tags": null,
    "title": "Docker",
    "uri": "/5a-containers/03-docker/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello and welcome to the week five announcements video for CIS 527 and CC 510 in fall 2023. This week we got a lot of stuff going on. We’ve got lab two that’s due on Wednesday, so tomorrow you should be getting done with lab two. There’s also the first discussion will be this Thursday, so the questions for our speaker for the first discussion are also due on Wednesday, so make sure you get those posted. Then we go into next Monday. You’ve got lab three quizzes due or due next Monday. Next Wednesday the discussion one responses due if you’re not able to attend a live discussion and then two weeks from now lab three is due and the second discussion questions will be due because our second speaker will be that week. So there’s stuff due every week, sometimes on Monday and Wednesday, so make sure you’re watching your calendar. Hopefully things go really smoothly, but it’s a really good time to keep keeping on your Canvas calendar to make sure you don’t miss anything that is due coming up.\nSo for lab two grading, all you have to do is submit your puppet manifest files on Canvas. There’s no meeting required, so you don’t have to schedule time with Matt or I. What will happen is Matt will download your files as he’s going through and grading them. He’ll run them on his VM. Basically what he’ll do is he’ll start at a snapshot that’s just what you should have for your assignment. He’ll run your manifest file, reboot, run your manifest file again, and then check to make sure it did everything correctly. We have to do the reboot so that all of your group permissions get applied properly, and it’s also a way to check to make sure that on Windows it doesn’t keep reinstalling some of the system, some of like Firefox and Thunderbird. It shouldn’t reinstall those. It should just see that they’re installed every time. So lab two grading, you don’t need to meet with anybody. Probably what I’ll do is on Thursday, anybody that’s already submitted lab two, I will go through and give you one point on that lab that will unlock the model solutions and also unlock access so you can get started on lab three because you want to go through that pretty quickly.\nSo our speaker for this coming week is Kyle Hudson. Kyle Hudson works at Kanren, which is the Internet Service Provider for K -State and other research. institutions in the state of Kansas. Kyle was formerly one of the folks that worked with Beocat. So he’s worked with us a lot. He’s spoken in the class before about his Beocat experience. But now I’m really excited to hear about his new job working at Kanren. Kyle will be here speaking Thursday at 1pm via Zoom in our usual Zoom meeting time. So remember to get points for the discussion, you have two options. You can either attend live and ask at least one question of the speaker or you can watch it either live or recorded and then you’ll be able to write a quick response. I’ll post the response prompt shortly after the speaker. So you’ll be able to work on that. And remember that’s Thursday at 1pm. Your questions are due Wednesday night.\nSo you’ll be going into Lab 3. Lab 3 in this class goes over core networking services. So we’re gonna set up our own little network in VMware. We’re gonna set our IP addresses. We’re gonna install a DNS and DHCP server and configure those. We’re going to an install an SNMP server and learn how to work with that. We’re gonna use tools like Wireshark to explore some network packets. Lots of stuff are going on in Lab 3. Lab 3 is probably the second hardest lab in this class. It’s a lot of new content if you’ve never worked with networking stuff before. In the textbook, there are some hints and some diagrams. So make sure you take a look at those. I show some pretty good examples of how to do debugging on DNS, for example. And of course, if nothing else, if you’re stuck, please ask questions. This is one of those labs you should start on very, very early and start working on a little bit at a time. Try and do one service at a time. So get DNS working, then get DHCP working, then get SNMP working. Make snapshots in VMware, snapshots are your friend. But if you get stuck, don’t be afraid to ask questions. Schedule a meeting, come to office hours. We’re happy to help. So that’s really it. Other than that, you can keep in touch either by posting on ed discussion or emailing us or joining the discussions Thursday at one o ‘clock.\nAfter Kyle’s done, I’ll stick around and answer any questions if you have any. You can always schedule a one -on -one office hours with either myself or Matt using our Calendly links. We’re always here to help. So hopefully things are going well. Hopefully you’re able to get up and networking this week. If you have any questions, let us know. Otherwise, best of luck and I look forward to seeing you at the discussion on Thursday.\n",
    "description": "",
    "tags": null,
    "title": "Fall '23 Week 5",
    "uri": "/y-announcements/week05/index.html"
  },
  {
    "content": " YouTube Video Resources Slides ITIL on Wikipedia ITIL from Axelos ISO/IEC 20000 on Wikipedia ISO/IEC 20000-1:2018 from ISO What is IT Service Management (ITSM) from Atlassian What is ITSM? Managing IT to serve business needs from CIO Video Transcript In this video, I’ll be taking a detour from all of the technical topics in this class to discuss a more philosophical topic: the art of system administration. Specifically, this video will introduce you to ITIL, one of the core resources for many IT organizations worldwide.\nFirst, you might be wondering what ITIL is. Originally, it was an acronym for the “Information Technology Infrastructure Library,” but it has since been officially shortened to just ITIL. ITIL is a comprehensive set of standards and best practices for all aspects of an IT infrastructure. Many organizations refer to this as “IT Service Management” or “ITSM” for short. By making use of the information found in the ITIL resources, you should be able to keep your IT systems running and make your customers happy.\nThis slide gives a brief history of ITIL. It was originally a set of recommendations developed by the UK Central Computer \u0026 Telecommunications Agency in the 1980s. That information was shared with many government and private entities, and they found it to be a very useful resource. By the 1990s, it was expanded to include over 30 volumes of information on a number of IT processes and services. As the technology matured, those volumes were collected into 9 sets of guidelines in the early 2000s, and by 2007, a complete rework was released, consisting of just 5 volumes encompassing 26 processes in all. This version is known as ITIL Version 3.\nIn 2011, a small update to version 3 was released by the UK Office of Government Commerce, the maintainers of ITIL at that time. However, starting in 2013, the licensing and maintenance of ITIL was handed over to Axelos. They also handle ITIL certifications. Recently, Axelos announced that they are working on ITIL 4, which should be released sometime in the next year.\nAs mentioned before, the current version of ITIL consists of 5 volumes: Service Strategy, Service Design, Service Transition, Service Operation, and Continual Service Improvement.\nThese volumes are interrelated, forming the backbone of any IT organization. This diagram shows how they all fit together. At the core is strategy, which defines the overall goals and expectations of the IT organization. Next, design, transition, and operation deal with how to put those strategies into practice and how to keep them running smoothly. Finally, Continual Service Improvement is all about performing an introspective look at the organization to help it stay ahead of trends in technology, while predicting and responding to upcoming threats or issues.\nTo break it down even further, this diagram lists most of the 26 processes included in ITIL. “Supplier Management” under “Design” is omitted for some reason. As you can see, there are quite a variety of processes and related items contained in each volume of ITIL.\nHowever, I’ve found one of the easiest ways to discuss ITIL is to relate it to a restaurant. This analogy is very commonly found on the internet, but the version I am using can be most closely attributed to Paul Solis. So, let’s say you want to open a restaurant, but you’d like to follow the ITIL principles to make sure it is the best, most successful restaurant ever. So, first you’ll work on your strategy. That includes the genre of food you’d like to serve, the location, and an analysis of future trends in the restaurant business. You don’t want to enter a market that is already saturated, or serve a food genre that is becoming less popular.\nNext, you’ll discuss details in the design. This includes determining specific menu items, the hours of operation, staffing needs, and how you’ll go about hiring and training the people you need.\nOnce you have everything figured out, its time to enter the transition phase. During this phase, you’ll test the menu items, maybe have a soft open to make sure the equipment and staff are all functioning properly, and validating any last-minute details of your design before the grand opening.\nOnce the restaurant is open for business, it’s time to enter the operational phase. In this phase, you’ll be looking at the details in the day-to-day operations of the restaurant, such as how customers are greeted and seated when they enter, how your wait staff enters the food orders, how the kitchen prepares the food, and finally how your clients are able to pay and be on their way. You’ll always be looking for issues in any of these areas, and work diligently to correct them as quickly as possible.\nIn addition, great restaurants will undergo continual service improvement, always looking for ways to be better and bring in more business. This includes surveys, market research, testing new recipes, and even undergoing full menu changes to keep things fresh and exciting.\nOverall, if you follow these steps, you should be able to build a great restaurant. If you’d like a great exercise applying this in practice, I encourage you to watch an episode or two of a TV show that focuses on fixing a failing restaurant, such as “Restaurant Impossible” or “Kitchen Nightmares,” and see if you can spot exactly where in this process the restaurant started to fail. Was the menu too large and confusing? Were the staff not trained properly? Did they fail to react to customer suggestions? Did the market change, leaving them behind? I’ve seen all of those issues, and more, appear during episodes of these TV shows, but each one always links directly back to one of the volumes and processes in ITIL. My hope is that you can see how these same issues could affect any IT organization as well.\nITIL also proposes a maturity model, allowing organizations to rate themselves based on how well they are conforming to ITIL best practices. Level 0 represents the total absence of a plan, or just outright chaos. At Level 1, the organization is starting to formulate a plan, but at best it is just a reactive one. I like to call this the “putting out fires” phase, but the fires are starting as fast as they can be put out. At Level 2, the organization is starting to be a bit more active in planning, so at this point they aren’t just putting out fires, but they are getting to the fires as fast as they develop.\nBy Level 3, the organization is becoming much more defined in its approach, and is working proactively to prevent fires before they occur. There are still fires once in a while, but many of them are either predicted or easily mitigated. At Level 4, most risks are managed, and fires are minimal and rare. Instead, they are preemptively fixing problems before they occur, and always working to provide the best service possible. Lastly, at Level 5, everything is fully optimized and automated, and any error that is unexpected is easily dealt with, usually without the customers even noticing. I’d say that Netflix is one of the few organizations that I can say is very comfortably at Level 5, based on the case-study from Module 5. Most organizations are typically in the range of Level 3 or 4.\nThere are a variety of factors that can limit and organization’s ITIL maturity level, but in my opinion, they usually come down to cost. For an organization to be at level 5, it requires a large investment in IT infrastructure, staff, and resources. However, for many large organizations, IT is always seen as a “red line” on the budget, meaning that it costs the organization more money than it brings in. So, when the budget gets tight, often IT is one of the groups to be downsized, only to lead to major issues down the line. Many companies simply don’t figure the cost of an IT failure into their budget until after it happens, and by then it is too late.\nConsider the recent events at K-State with the Hale Library fire. If K-State was truly operating at Level 5, there would most likely have to be a backup site or distributed recovery center, such that all systems could be back online in a matter of hours or minutes after a failure. However, that would be an astronomical cost to the university, and often it is still cheaper to deal with a few days of downtime instead of paying the additional cost to maintain such a system.\nSo, in summary, why should an IT organization use ITIL? First, IT is a very complex area, and it is constantly changing. At the same time, user satisfaction is very important, and IT organizations must do their best to keep customers happy. Even though it is a very technical field, IT can easily be seen as a service organization, just like any restaurant. Because of that, many IT organizations can adopt some of the same techniques and processes that come from those more mature fields, and ITIL is a great reference that brings all of that industry knowledge and experience to one place. Lastly, poor management can be very expensive, so even though it may be expensive to maintain your IT systems at a high ITIL maturity level, that expense may end up saving money in the long run by preventing catastrophic errors. If you’d like to know more, feel free to check out some of the resources linked below the video in the resources section. Unfortunately, the ITIL volumes are not freely available, but many additional resources are.\nI wanted to come back to this lecture and add a bit more information at the end to clarify a few things regarding the relationship between ITIL and a different term, IT Service Management or ITSM. I added a couple of links below this video that provide very good discussions of what ITSM is and how it relates to ITIL.\nIn short, IT service management is the catch-all term for how an IT organization handles its work. It could deal with everything from the design of systems to the operational details of keeping it up and running and even the management of changes that need to be applied.\nWithin the world of ITSM, one of the most common frameworks for IT Service Management is ITIL. So, while I focus on ITIL in this video as the most common framework, it is far from the only one out there, and it has definite pros and cons.\nSo, as you move into the world of IT, I recommend looking a bit broader at IT Service Management as a practice, and then evaluate the many frameworks that exist for implementing that in your organization.\n",
    "description": "",
    "tags": null,
    "title": "ITIL",
    "uri": "/7-backups-monitoring-devops/03-itil/index.html"
  },
  {
    "content": " Windows Manifest (5) Compiles Runs Without Errors Comments for any modules to install Windows Configuration (10) Users (cis527, AdminUser, NormalUser, GuestUser, EvilUser) Groups (Administrators, Users, Files) Software (Firefox, Thunderbird, Notepad++) Windows Files (10) Run Get-ChildItem -Recurse | Get-Acl | Format-List Check for groups and user on each folder Ubuntu Manifest (5) Compiles Runs Without Errors Comments for any modules to install Ubuntu Configuration (10) Users (cis527, AdminUser, NormalUser, GuestUser, EvilUser) Groups (files) Software (Firefox, Thunderbird, Synaptic) Ubuntu Files (10) Run ls -lR Check for groups and user on each folder ",
    "description": "",
    "tags": null,
    "title": "Lab 2 Grading Checklist",
    "uri": "/z-instructor-resources/03-lab-2-grading-checklist/index.html"
  },
  {
    "content": " YouTube Video Resources Slides The TCP/IP Guide Classful Networks on Wikipedia Dot-Decimal Notation on Wikipedia Classless Inter-Domain Routing on Wikipedia IPv6 on Wikipedia Reserved IP Addresses on Wikipedia Video Transcript In this video, I’ll discuss how the network layer of the 7-layer OSI model works.\nThe network layer is used to route packets from one host to another via a network. Typically, most modern networks today use the Internet Protocol or IP to perform this task, though there are other protocols available. In addition to computers, most routers also perform tasks at layer 3, since they need to be able to read and work with the address information contained in most network packets.\nFor most of the internet’s history, the network layer used the Internet Protocol version 4, commonly known as IPv4. This slide gives the packet structure of IPv4, showing the information it adds to each packet that goes through that layer. The most important information is the source and destination IP address, giving the unique address on the network for the sender and intended recipient of this packet of information. Let’s take a closer look at IP addresses, as they are a very important part of understanding how a computer network works.\nFor IPv4, an IP address consists of a 32-bit binary number, giving that host’s unique identifier on the network. No two computers can share the same IP address on the same network. In most cases, the IP address is represented in Dot-Decimal notation.\nHere’s an example of that notation. Here, the binary number 10101100 is represented as the decimal number 172 in the first part of the IP address. Each block of 8 bits, or one byte, is represented by the corresponding decimal number, separated by a dot or period. This makes the address much easier to remember, almost like a phone number.\nIn fact, on the early days of the internet, that is exactly how it was set up. The early internet used a form of routing called “classful networking” to determine how IP addresses were divided. The type of network was determined by the first 4 bits of the IP address, much like an area code in modern phone numbers. There were several classes of networks, each of various sizes.\nWhen an organization wanted to connect to the internet, they would register to receive a block of network addresses for their use. For a large network, such as IBM, they may receive an entire Class A network, whereas smaller organizations would receive a smaller Class B or Class C network. So, for a Class A network, the IP address would consist of the prefix 0, followed by 7 bits identifying the network. Then, the remaining 24 bits would identify the host on that network, usually assigned by the network owner. This helped standardize which parts of the IP address denoted the owner of the network, and which part denoted the unique computer on that network. For example, in this system, K-State would have the Class B network with the prefix 129.130.\nYou can even see some of this structure in this map of the IPv4 internet address space created by XKCD from several years ago. Many of the low numbered IP address blocks are assigned to a specific organization, representing the Class A networks those organizations had in the early days of the internet.\nHowever, as the internet grew larger, this proved to be very inefficient. For example, many organizations did not use up all of their IP address space, leading to a large number of addresses that were unused yet unavailable for others to use. So, in the early 1990s, the internet switched to a new method of addressing, called Classless Inter-Domain Routing, or CIDR, sometimes pronounced as “cider.” Instead of dividing the IPv4 address space into equal sized chunks, they introduced the concept of a subnet, allowing the address space to be divided in many more flexible ways.\nLet’s take a look at an example. Here, we are given the IP address 192.168.2.130, with the accompanying subnet mask of 255.255.255.0. To determine which part of the IP address is the network part, simply look at the bits of the IP address that correspond to the leading 1s in the subnet mask. If you are familiar with binary operations, you are simply performing the binary and operation on the IP and subnet to find the network part. Similarly, for the host part, look at the part of the IP address that corresponds to the 0s in the subnet mask. This would be equivalent to performing the binary and operation on the IP address and the inverse of the subnet mask.\nHere’s yet another example. Here, you can see that the subnet mask has changed, therefore the network and host portion of the IP address is different. In this way, organizations can choose how to divide up their allocated address space to better fit their needs.\nTo help make this a bit simpler, you can use a special form of notation called CIDR Notation to describe networks. In CIDR notation, the network is denoted by its base IP address, followed by a slash and the number of 1s in the subnet mask. So, for the first example on the previous slide, the CIDR notation of that network would be 192.168.2.0/24, since the network starts at IP address 192.168.2.0 and the subnet mask of 255.255.255.0 contains 24 leading 1s. Pretty simple, right?\nWith CIDR, an IP address can be subdivided many times at different levels. For example, several years ago the website freesoft.org had this IP address. Looking at the routing structure of the internet, you would find that the first part of that IP address was assigned to MCI, a large internet service provider. They then subdivided their network among a variety of organizations, one of those being Automation Research Systems, who received a smaller block of IP addresses from MCI. They further subdivided their own addresses, and eventually one of those addresses was used by freesoft.org.\nThe groups who control the internet, the Internet Engineering Task Force (IETF) and the Internet Assigned Numbers Authority (IANA), have also marked several IP address ranges as reserved for specific uses. This slide lists some of the more important ranges. First, they have reserved three major segments for local area networks, listed at the top of this slide. Many of you are probably familiar with the third network segment, starting with 192.168, as that is typically used by home routers. If you are using the K-State wireless network, you may notice that your IP address begins with a 10, which is part of the first segment listed here. There are also three other reserved segments for various uses. We’ll discuss a couple of them in more detail later in this module.\nHowever, since many local networks may be using the same IP addresses, we must make sure that those addresses are not used on the global internet itself. To accomplish this, most home routers today perform a service called Network Address Translation, or NAT. Anytime a computer on the internal network tries to make a connection with a computer outside, the NAT router will replace the source IP address in the packet with its own IP address, while keeping track of the original internal IP address. Then, when it receives a response, it will update the incoming packet’s destination IP address with the original internal IP of the sender. This allows multiple computers to share the same external IP address on the internet. In addition, by default NAT routers will block any incoming packets that don’t have a corresponding outgoing request, acting as a very powerful firewall for your internal network. If you have ever hosted a server on your home network, you are probably familiar with the practice of port-forwarding on your router, which adds and entry to your router’s NAT table telling it where to send packets it receives.\nFor many years, IPv4 worked well on the internet. However, as internet access became more common, they ran into a problem. IPv4 only specified internet addresses which were 32-bits in length. That means that there are only about 4.2 billion unique IPv4 addresses available. With a world population over 7 billion, this very quickly became a limiting factor. So, as early as the 1990s, they began work on a new version of the protocol, called IPv6. IPv6 uses 128-bit IP addresses, allowing for 340 undecillion unique addresses. According to a calculation posted online, assuming that there was a planet with 7 billion people on it for each and every star in each and every galaxy in the known universe, you could assign each of those people 10 unique IPv6 addresses and still have enough to do it again for 10,000 more universes. Source\nIPv6 uses a very similar packet structure to IPv4, with things simplified just a bit. Also, you’ll note here that the source and destination addresses are 4 times as large, making room for the larger address space.\nIPv6 addresses, therefore, are quite a bit different. They are represented digitally as 128-bit binary numbers, but typically we write them as 8 groups of 4 hexadecimal digits, sometimes referred to as a “hextet,” separated by colons. Since IPv6 addresses are very long, we’ve adapted a couple of ways to simplify writing them.\nFor example, consider this IPv6 address. To simplify it, first you may omit any leading zeros in a block. Then, you can combine consecutive blocks of 0 together, replacing them with a double colon. However, you may only do that step once per address, as two instances of it could make the IP address ambiguous. Finally, we can remove all spaces to get the final address, shown in the last line.\nIPv6 addresses use a variety of different methods to denote which part of the address is the network and host part. In essence, this is somewhat similar to the old classful routing of IPv4 addresses. Each prefix on an IPv6 address indicates a different type of address, and therefore a different parsing method. This slide gives a few of the most common prefixes you might see.\nFor this course, I won’t go too deep into IPv6 routing, as most organizations still primarily use IPv4 internally. In addition, in many cases the network hardware automatically handles converting between IPv6 and IPv4 addresses, so you’ll spend very little time configuring IPv6 unless you are working for an ISP or very large organization.\nIn the next video, we’ll continue this discussion on the transport layer.\n",
    "description": "",
    "tags": null,
    "title": "Network Layer",
    "uri": "/3-core-networking-services/03-network-layer/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello, and welcome to the week three announcements video for CIS 527 in summer 2022. So this week you should be wrapping up lab two, which is due tonight at 7pm. I’ll talk about that briefly in just a second. You also should be working on the week two discussion questions and responses which are due tonight at 11:59pm. So make sure you leave enough time to watch that video, write your responses and respond to the question prompts there as well. This Friday, you’ll have the week three quizzes that are due and then next Monday, you’ll be turning in lab three, and also doing the week three discussion.\nSo for lab two grading, you should be working on that today, all you have to do is submit your puppet manifest files to Canvas. So you should have a windows.pp and the linux.pp. Sometimes people have built them as single files, that’s fine. If there’s any libraries or modules I need to install in puppet to make your files work, just put those in the comments at the top of your file. But really, all you should have to do is submit your to manifest files for lab two. And then what I will do for grading is I will download them, I will run them on my system, I will reboot my system, I will run them again, just like I discussed in the lab. That way you don’t have to worry if some of your group memberships don’t take the first time I run it, I reboot it, I run it again. And then I’ll go through and check to see if your file did all of the configuration that it’s supposed to do. Hopefully it does. If you have any questions or concerns on lab two, I’m available, you do not need to schedule a meeting with me to discuss lab two. I’ll just create it on canvas. But if you have any questions or want to discuss lab two, I’m totally open for meetings today. Just let me know.\nSo your speaker for this week is Dr. Gary Pratt. He was the K-State CIO and has been in that role since 2017. Probably his biggest claim to fame at K-State among other things is he was hired about a month before the Hale Library fire that took down all of our systems. For those of you that are unaware, in 2017. In about May, we had a fire in Hale Library that didn’t do a ton of damage to the library itself. But as part of putting out the fire, there was a lot of water in smoke damage. And the water actually filtered down into the data center that is in the basement of Hale Library where a significant amount of K-State’ss IT systems were kept. And so that created a lot of issues where many central IT systems down were down for several weeks, I think cases was brought back online the day before enrollment was supposed to start in June. So it was a really big deal. He talks a little bit about that in his video as well. But the bigger picture as a CIO, he oversees all of K-State’s IT and he’s really in charge of the big picture and long range planning that goes on at K State. He does have an education background. So he loves teaching. He loves talking to students. Just like Seth last week, Dr. Pratt is also more than willing to answer questions from this class. So if you submit any questions as part of your responses, I will collect those and for those on the Dr. Pratt and hopefully get a response from him sometime later this summer.\nSo this week, you should be working on lab three, lab three is where we’re going to pivot and start working on a lot of networking. So we’ll study core network services, we’ll talk about setting IP addresses in our systems, we’re going to install a DNS and a DHCP server in Linux, we’ll also install an SNMP, server in Linux. And we’ll do some playing around with Wireshark to capture some network packets and see what that looks like. lab three becomes much more complex than lab two, depending on your background and networking. I understand for many of you, this might be the first time you’ve worked with a lot of these technologies. And so it can be very complicated. You’ll be reading a lot of documentation and trying to figure out how those things work. Just like with lab two, my best advice is to make snapshots often whenever you get something to work. And if something does not work like you expect it, feel free to roll back to that snapshot and try again, specifically SNMP, I do have some notes in there, the DigitalOcean documentation works up to a certain point, but it does not give the correct instructions for how to set a password for a new user account. So there’s a second set of instructions that you want to follow for that particular step of setting up SNMP. It does totally work. But you may have to play around a little bit to see which process actually works for you. Thankfully, at the end of lab three, there are some hints and network diagrams that you can look at. There’s a great video on how to debug a DNS server using dig so you can check that out. So make sure you watch all of the content at the end of lab three before you ask questions. It really will help you understand what we’re trying to do and how to debug it. But if you get stuck, don’t be afraid to ask questions. I’m more than happy to help with this.\nOther than that, feel free to keep in touch. We’ve got discussions on Discord. If you have any questions, you can join us for tea time office hours on Tuesdays at 330 and Fridays at 1030. If you want to chat about anything going on, I also have one on one office hours available via the Calendly link that you use for scheduling grading. So feel free to use that. And you can always send me an email and I’ll be happy to answer your questions there. So other than that, that’s where we’re going this week you’re going to start working with networking. It’s a big step in towards system administration. Next week after this we’re going to be working on LDAP and active directory so you’ll have a our core authentication services setup and then week five we pivot over into the cloud but this is the week that start setting up the real system administration part where we get networking working as always if you have any questions let me know otherwise good luck this week and I will talk to you next week\n",
    "description": "",
    "tags": null,
    "title": "Summer '22 Week 3",
    "uri": "/y-announcements/old/summer2022/week03/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Virtualization on Wikipedia Virtualization Whitepaper from VMware Accessing VMware Software on CS Support Video Script In this class, we will be making heavy use of virtualization software to allow us to run multiple operating systems simultaneously on the same computer. This video provides a quick overview of what virtualization is and how it works.\nFirst, a simple view of how a computer works. In essence, whenever you tell the computer to run a program, you are actually telling the operating system what to do. It will then load the requested application software into memory, and begin executing it. The software will send instructions through the operating system to the hardware, describing what actions to take. The hardware will then use electronic circuits to perform those operations.\nThis diagram shows that a computer would look like without virtualization. This describes how most computers work today, and it has been this way for over 30 years.\nRecently, however, virtualization has become much more commonplace. In fact, you may be using some forms of virtualization right now, and not even realize it. In essence, virtualization software emulates some part of a computer system, typically the hardware. By doing so, it allows the hardware to run multiple different operating systems at the same time. Also, it prevents the different operating systems from conflicting with each other.\nAs an added bonus, by using virtualization, it becomes much easier to migrate operating systems across different hardware setups. Instead of having to reinstall and reconfigure the operating system on the new hardware, simply install the virtualization platform, and copy the virtual machine as if it were any other file.\nIn essence, virtualization adds one more layer of abstraction between the kernel and the hardware, allowing it to be much more portable and configurable than in a traditional setup. This diagram gives an overview of what that would look like.\nOf course, there are many different types of virtualization out there. In this class, we’ll be primarily working with hosted virtualization, which involves installing virtualization software within a “host” operating system. You can also install virtualization software directly on the hardware, which is called bare-metal or hypervisor virtualization.\nBeyond that, you can choose to virtualize more than just the hardware. Parts of the operating system itself can be virtualized, leading to the concept of containers, such as those employed by Docker. You can also virtualize parts of individual applications to make them more secure. This is commonly called sandboxing, and many mobile operating systems and browsers today already employ sandboxing between the apps and pages they work with.\nThis diagram shows what hosted virtualization looks like. You’ll notice that there is a host operating system installed on the hardware itself, and the virtualization layer is installed on top of that.\nCompare that to bare-metal virtualization, where the virtualization layer is installed directly on top of the hardware. This is especially powerful, since the system doesn’t have to devote any resources to running the host operating system if it won’t be directly used.\nThis diagram gives a good overview of containers. In this case, the docker engine acts as the virtualization layer. Inside the containers, instead of having a full operating system, you simply have libraries and applications. They all share the same kernel, provided by the host operating system, though they can have different configurations within each container. It is a very powerful way to deploy applications on cloud servers.\nFinally, this diagram gives a quick look at sandboxing. By virtualizing the parts of the system that an application interacts with, you can prevent it from performing malicious actions and interfering with other parts of the system.\nTo quote Men in Black, the “old \u0026 busted” way of doing things involved installing a single operating system per server, and then a single application on that server. This resulted in organizations managing large numbers of physical servers. In addition, most servers were only running at 5% capacity, so the resources were very inefficiently used. Finally, it was a management nightmare, and the only way to add more redundancy to the system was to buy more servers, compounding the problem.\nCompare that to the “new hotness” of today, where we can use virtualization to install many operating systems on a single physical server, with each one dedicated to a single application. That results in fewer physical servers, more efficient use of resources, and much simpler redundancy. However, it is up for debate if that truly made management easier, or if it just shifted the management chore from installing and configuring individual systems to installing and configuring deployment and automation tools.\nFor this course, we’ll be primarily working with hosted virtualization using VMware Workstation. If you are using an Apple computer, you’ll be using VMware Fusion, which is very similar. I highly recommend using this software, as all of the materials in this class have been tested on it, and I am very familiar with it in case you need help. It is available to all K-State CS students for free. A link to the instructions for finding that software is in the resources section below.\nHowever, you may choose to use a different virtualization software package to meet your needs. The only thing to keep in mind is that I can make no guarantees that it will work, and if you run into major issues that we cannot fix, you may be asked to continue working on the labs in this class using VMware products instead. Here is a list of a few other software packages that could be used instead of VMware Workstation or fusion.\nAgain, if you do not have access to a computer with sufficient resources to install and use VMware, please contact me so we can make other arrangements.\nFinally, beyond just virtualization software, there are many cloud providers that will host virtual machines for you. We’ll deal with these more starting in module 5. This list gives a few of the more popular ones out there.\nIn addition, many cloud providers offer more than just virtual machines, such as containers and application hosting. Again, we’ll discuss these more starting in module 5, but here are a few you may have heard of.\nAt this point, you should be ready to complete the first task of Lab 1, which is to install Virtualization Software. Make sure you install the proper version, as listed on the Lab 1 Assignment page. If you have any questions, please use the discussion boards to ask your fellow classmates or contact me.\n",
    "description": "",
    "tags": null,
    "title": "Virtualization \u0026 VMware",
    "uri": "/1-secure-workstations/03-virtualization-vmware/index.html"
  },
  {
    "content": " YouTube Video Resources Slides K-State IT Help Desk - Email helpdesk@ksu.edu Syllabus K-State Canvas Help Instructure Canvas Guides K-State Libraries K-State CS Support K-State CS Discord K-State CS Advising K-State Engineering Student Services K-State Office of Student Life K-State Report It Video Script As you work on the materials in this course, you may run into questions or problems and need assistance. This video reviews the various types of help available to you in this course.\nFirst and foremost, anytime you have a questions or need assistance in the course, please post in the course Discord room. It is the best place to go to get help with anything related to this course, from the tutorials and projects to issues with Codio and Canvas. Before you post on Discord, take a minute to look around and make sure the question has not already been posted before. It will save everyone quite a bit of time.\nThere are a few major reasons we’ve chosen to use Discord in this program. Our goal is to respond as quickly as possible, and having a single place for all questions allows the instructors and the TAs to work together to answer questions and solve problems quickly. As an added bonus, it reduces the amount of email generated by the class. Discord includes lots of features to make your messages easily readable using both markdown and code blocks. Finally, by participating in discussions on Discord and helping to answer questions from your fellow students, you can earn extra credit points!\nOf course, if you aren’t able to find an answer in either of those places, the next step would be to search online for an answer. For better or worse, in the real world as a system administrator, many times you simply won’t know the answer, and will be tasked with finding the best one available. Learning how to leverage online search engines is a powerful skill to develop, and one major desired outcome of this course is to help you get better at finding and evaluating online resources for your work.\nIf all else fails, please email me and let me know. Make sure you clearly explain your question and the steps you’ve taken to solve it thus far. If I feel it can be easily answered by one of the earlier steps, I may redirect you back to those before answering it directly. But, at times there are indeed questions that come up that don’t have an easy answer, and I’m more than happy to help answer them as needed.\nBeyond Discord, there are a few resources you should be aware of. First, if you have any issues working with K-State Canvas, the YouTube Videos, K-State IT resources, or any other technology related to the delivery of the course, your first source of help is the K-State IT Helpdesk. They can easily be reached via email at helpdesk@ksu.edu. Beyond them, there are many online resources for using Canvas and YouTube, many of which are linked in the resources section below the video. As a last resort, you may contact me via email, but in most cases I will not be able to help and will simply redirect you to the K-State helpdesk for assistance.\nNext, we have grading and administrative issues. This could include problems or mistakes in the grade you received on a lab, missing course resources, or any concerns you have regarding the course and the conduct of myself and your peers. Since this is an online course, you’ll be interacting with us on a variety of online platforms, and sometimes things happen that are inappropriate or offensive. There are lots of resources at K-State to help you with those situations. First and foremost, please email me as soon as possible and let me know about your concern, if it is appropriate for me to be involved. If not, or if you’d rather talk with someone other than me about your issue, I encourage you to contact either your academic advisor, the CS department staff, College of Engineering Student Services, or the K-State Office of Student Life. Finally, if you have any concerns that you feel should be reported to K-State, you can do so at https://www.k-state.edu/report/. That site also has links to a large number of resources at K-State that you can use when you need help.\nFinally, if you find any errors or omissions in the course content, or have suggestions for additional resources to include in the course, please email me. There are some extra credit points available for helping to improve the course, so be on the lookout for anything that you feel could be changed or improved.\nSo, in summary, if you have any issues using Canvas or accessing the course content, contact the K-State Helpdesk as a first step. If you run into issues with the labs, your first step should be to consult the course modules and discussion forums. For grading questions and errors in the course content, contact me directly. For other issues, please contact either myself or your academic advising resources as appropriate.\nOur goal in this program is to make sure that you have the resources available to you to be successful. Please don’t be afraid to take advantage of them and ask questions whenever you want.\n",
    "description": "",
    "tags": null,
    "title": "Where to Find Help",
    "uri": "/0-introduction/03-where-to-find-help/index.html"
  },
  {
    "content": "Getting your enterprise online.\n",
    "description": "",
    "tags": null,
    "title": "Core Networking Services",
    "uri": "/3-core-networking-services/index.html"
  },
  {
    "content": "Lab 7 - Backups, Monitoring \u0026 DevOps Instructions Create two cloud systems and four virtual machines meeting the specifications given below. The best way to accomplish this is to treat this assignment like a checklist and check things off as you complete them.\nIf you have any questions about these items or are unsure what they mean, please contact the instructor. Remember that part of being a system administrator (and a software developer in general) is working within vague specifications to provide what your client is requesting, so eliciting additional information is a very necessary skill.\nNote To be more blunt - this specification may be purposefully designed to be vague, and it is your responsibility to ask questions about any vagaries you find. Once you begin the grading process, you cannot go back and change things, so be sure that your machines meet the expected specification regardless of what is written here. –Russ\nAlso, to complete many of these items, you may need to refer to additional materials and references not included in this document. System administrators must learn how to make use of available resources, so this is a good first step toward that. Of course, there’s always Google !\nTime Expectation This lab may take anywhere from 1 - 6 hours to complete, depending on your previous experience working with these tools and the speed of the hardware you are using. Working with each of these items can be very time-consuming the first time through the process, but it will be much more familiar by the end of this course.\nInfo This lab involves working with resources on the cloud, and will require you to sign up and pay for those services. In general, your total cost should be low, usually around $20 total. If you haven’t already, you can sign up for the GitHub Student Developer Pack to get discounts on most of these items. If you have any concerns about using these services, please contact me to make alternative arrangements! –Russ\nTask 0: Droplets \u0026 Virtual Machines For this lab, you will continue to use the two DigitalOcean droplets from Labs 5 and 6, labelled FRONTEND and BACKEND, respectively. This assignment assumes you have completed all steps in the previous labs successfully; if not, you should consult with the instructor to resolve any existing issues before continuing.\nYou will also need a Windows Server 2019 VM configured as an Active Directory Domain Controller, along with a Windows 10 VM added as a client computer on that domain. You should continue to use the systems from Lab 6.\nFinally, you will need two Ubuntu 20.04 VMs, but they don’t need any particular configuration beyond what is specified in Lab 1 or Lab 2. You may continue to use your Ubuntu VMs from Lab 6.\nTask 1: Backup \u0026 Restore Windows Active Directory This task requires you to successfully demonstrate a backup and restore procedure for your Windows Server 2016 Active Directory domain. To complete this item, follow these steps:\nCreate a user named backupuser on your Active Directory domain Create a group named BackupGroup on your Active Directory domain, and add the new backupuser user to that group Log on to your Windows 10 client VM as backupuser and take a screenshot showing the successful login and the system time of your host system. Create a backup of your Active Directory domain using the Windows System State backup tool. You should store this backup on an external hard disk, such as a flash drive, that is mounted in your Windows Server 2019 VM. Alternatively, you may add a secondary hard disk to your Windows Server 2019 VM and use that location to store the backup. See the video in the resources section for instructions. Once the backup is complete, delete the backupuser user and BackupGroup group from the Active Directory domain. Reboot your Windows 10 client VM and attempt to log on as backupuser. It should fail. Take a screenshot showing a failed login and the system time of your host system Perform an authoritative restore of the Active Directory domain from the backup. This should restore the deleted user and group. Take a screenshot showing the successful completion of the authoritative restore process and the system time of your host system. Info The documentation for this portion is unclear. In my testing, you may be able to just checkmark the “Perform an authoritative restore of Active Directory files” option when restoring the backup and avoid any command-line work. That seems to be working correctly as of Summer 2021. However, if that doesn’t work or you choose to do the authoritative restore via command-line, you need to get the path correct. To help with that, here’s a hint: for my sample domain ad.russfeld.cis527.cs.ksu.edu and account backupuser, I’ll need to use the command restore object \"cn=backupuser,cn=Users,dc=ad,dc=russfeld,dc=cis527,dc=cs,dc=ksu,dc=edu\" to restore the correct account on the domain. –Russ\nReboot your Windows 10 client VM and log on to that system as backupuser. Take a screenshot showing the successful login and the system time of your host system. Tip You’ll present those 4 screenshots as part of the grading process for this lab, so I recommend storing them somewhere memorable so they are easy to find. –Russ\nResources These resources mostly refer to Windows Server 2012 or 2016, but should work for 2019 as well.\nAD Forest Recovery - Backing up a full server from Microsoft Windows IT Pro Center How to Backup Active Directory Fully in Windows Server 2016 from Tactig How to perform Authoritative Restore of Active Directory Objects - 2012 R2 from ITIngredients (should work for 2019) Windows Server 2012 - Active Directory - Backup and Restore, Part 1: System State from David M Tech Blog (should work for 2019) How to Add Additional Virtual Hard Disk Drive in VMWare Workstation Tutorial by The Teacher on YouTube Task 2: Backup Ubuntu Web Application For this task, you will perform the steps to create a backup of the web application installed on your Ubuntu droplet in Lab 6. To complete this item, prepare an archive file (.zip, .tar, .tgz or equivalent) containing the following items:\nWebsite data and configuration files for the web application. This should NOT include the entire application, just the relevant configuration and data files that were modified after installation. For Docker installations, any information required to recreate the Docker environment, such as a Docker Compose file, should also be included. Relevant Apache or Nginx configuration files (virtual hosts, reverse proxy, etc.) A complete MySQL server dump of the appropriate MySQL database. It should contain enough information to recreate the database schema and all data. Clear, concise instructions in a README file for restoring this backup on a new environment. Assume the systems in that new environment are configured as directed in Lab 5. These instructions would be used by yourself or a system administrator of similar skill and experience to restore this application - that is, you don’t have to pedantically spell out how to perform every step, but you should provide enough information to easily reinstall the application and restore the backup with a minimum of effort and research. Resources How to Compress and Extract Files using the tar Command on Linux from How-To Geek How to Import and Export Databases and Reset a Root Password in MySQL from DigitalOcean How To Backup MySQL Databases on an Ubuntu VPS from DigitalOcean (steps should still be valid for 20.04) Task 3: Ubuntu Monitoring Part 1 For this task, you will set up either Munin or Ganglia on your Ubuntu droplets from Lab 6. To complete this item, follow these steps:\nConfigure the Ubuntu droplet named FRONTEND as the primary server for either Munin or Ganglia. Add the Ubuntu droplet named BACKEND as a client on either Munin or Ganglia Send the URL of the Munin or Ganglia server in your grading packet. Make sure that both FRONTEND and BACKEND are appearing in the data. Of course, you may need to modify your firewall configuration to allow incoming connections for Munin! If your firewall is disabled and/or not configured, there will be a deduction of up to 10% of the total points on this lab\nTip As always, you may have to deal with Apache virtual hosts and firewalls for this setup. In addition, you may want to add a new A record to your domain name for this site, and request an SSL certificate via CertBot. –Russ\nResources Server Monitoring with Munin and Monit on Ubuntu 16.04 LTS (Xenial Xerus) from HowtoForge (should work on 20.04) How to Install the Munin Monitoring Tool on Ubuntu 14.04 from DigitalOcean (should work on 20.04) How to Install and Configure Ganglia Monitor on Ubuntu 16.04 from HostPresto (should work on 20.04) Introduction to Ganglia on Ubuntu 14.04 from DigitalOcean (should work on 20.04) Task 4: DevOps Setup an automatically deployed Git repository on your Ubuntu droplet. For this task, perform the following:\nCreate a GitLab repository on the K-State CS GitLab instance. Clone that repository on your own system, and verify that you can make changes, commit them, and push them back to the server. Clone that repository into a web directory on your Ubuntu droplet named BACKEND. You can use the default directory first created in Lab 5 (it should be cis527charlie in your DNS). Create a Bash script that will simply use the git pull command to get the latest content from the Git repository in the current directory. Install and configure webhook on your Ubuntu droplet named BACKEND. It should listen for all incoming webhooks from GitLab that match a secret key you choose. When a hook is received, it should run the Bash script created earlier. Configure a webhook in your GitLab repository for all Push events using that same secret key and the URL of webhook on your server. You may need to make sure your domain name has an A record for the default hostname @ pointing to your FRONTEND server. To test this setup, you should be able to push a change to the GitLab repository, and see that change reflected on the website automatically. For offline grading, add the instructor and GTA to the repository as maintainers, and submit the repository and URL where the files can be found in your grading packet. Provided the webhook works correctly, they should be able to see a pushed change to the repository update the website. Of course, you may need to modify your firewall configuration to allow incoming connections for Webhook! If your firewall is disabled and/or not configured, there will be a deduction of up to 10% of the total points on this lab\nTip Since the Webhook process runs as the root user on BACKEND, you’ll need to make sure a set of SSH keys exist in the root user’s home folder /root/.ssh/ and add the public key from that directory to your GitLab account . You should then use the root account (use sudo su - to log in as root) to run git pull from the appropriate directory on BACKEND at least once so you can accept the SSH fingerprint for the GitLab server. This helps ensure that root can properly run the script. –Russ\nResources Extras - Git webhook on GitHub webhook Hook Examples on GitHub Webhooks on GitLab Documentation EXTRA CREDIT TASK: Ubuntu Monitoring Part 2 Note This task is worth 10 points extra credit toward your overall lab grade. It can be tricky to perform, since you need to provide significant amounts of RAM to your Ubuntu system for it to work. Don’t worry too much if you can’t get it to work - I struggle with this one sometimes! -Russ\nFor this task, you will install Elasticsearch, Logstash, and Kibana (a.k.a. the Elastic Stack, sometimes known as the ELK stack) on your Ubuntu 20.04 VMs, and configure both Filebeat and Metricbeat to collect information about those hosts.\nOn the Ubuntu VM you’ll be using as the server, make sure you have at least 4GB of RAM assigned to the VM. Unfortunately, Elasticsearch and Kibana won’t run properly with less than 4GB of RAM available. Follow the instructions in the DigitalOcean guide to install the Elastic stack on one of your Ubuntu VMs. If you already have Apache installed on this system, I recommend changing the port for nginx to something other than 80, such as 8080. This can be done in the nginx configuration file for the site that is created in the guide. Install Filebeat on that VM, and configure it to send data through Logstash following the instructions on the DigitalOcean guide. Confirm that you can access data from Filebeat/Logstash in Kibana before continuing. Using the other DigitalOcean guide, install Metricbeat on both Ubuntu VMs. Metricbeat should be configured to directly send data to Elasticsearch, without going through Logstash Don’t forget to allow the appropriate ports through the firewall You’ll need to configure Elastic Search for Single-node discovery or else it won’t start Verify that you can see metric data such as CPU usage in Kibana (look for the [Metricbeat System] Overview ECS dashboard). Take a screenshot of the [Metricbeat System] Overview ECS and add it to your grading packet. Alternatively, schedule a time for grading to review this part of the lab. Resources How to Install Elasticsearch, Logstash, and Kibana (Elastic Stack) on Ubuntu 20.04 from DigitalOcean How to Gather Infrastructure Metrics with Metricbeat on Ubuntu 18.04 from DigitalOcean (should work on 20.04 as well) Install ELK on Ubuntu 20.04 Focal Fossa Linux from Linuxconfig.org Elasticsearch Boostrap Checks from Elastic.co (for configuring single-node discovery) Task 6: Submit Files This lab may be graded completely offline. To do this, submit the following items via Canvas:\nTask 1: 4 screenshots clearly showing the system time, showing a successful login before the test user is deleted, an unsuccessful login after the user was deleted, a successful restoration of the AD, and a successful login showing that the user was restored. Task 2: An archive file containing a README document as well as any files or information needed as part of the backup of the Ubuntu web application installed in Lab 6. Task 3: The URL of your Munin or Ganglia instance, clearly showing data from both FRONTEND and BACKEND. Task 4: A GitLab repository URL and a URL of the website containing those files. Make sure the instructor and GTA are added to the repository as maintainers. They should be able to push to the repository and automatically see the website get updated. Extra Credit Task: A screenshot of the [Metricbeat System] Overview ECS dashboard showing data from both Ubuntu VMs If you are able to submit all 5 of the items above, you do not need to schedule a grading time. The instructor or GTA will contact you for clarification if there are any questions on your submission.\nFor Tasks 3 - 5, you may also choose to do interactive grading, especially if you were unable to complete it and would like to receive partial credit.\nTask 7: Schedule A Grading Time If you are not able to submit information for all 5 tasks for offline grading, you may contact the instructor and schedule a time for interactive grading. You may continue with the next module once grading has been completed.\n",
    "description": "",
    "tags": null,
    "title": "Assignment",
    "uri": "/7-backups-monitoring-devops/04-assignment/index.html"
  },
  {
    "content": " Note TODO This video was recorded for Ubuntu 18.04, but works for Ubuntu 22.04 as well. When creating a droplet, simply select the newest version of Ubuntu LTS. This video shows an older version of the DigitalOcean UI, but should be similar to what you see today. –Russ\nYouTube Video Resources Slides Student Developer Pack from GitHub Education Referral Program from DigitalOcean Extras - SSH Extras - Windows Subsystem for Linux How to Create a Droplet from the DigitalOcean Control Panel from DigitalOcean An Introduction to Cloud-Config Scripting from DigitalOcean Initial Server Setup with Ubuntu 22.04 from DigitalOcean Virtual Private Cloud (VPC) from DigitalOcean (replaced private networking) UFW Essentials: Common Firewall Rules and Commands from DigitalOcean How Does SSH Work from Hostinger SSH Essentials: Working with SSH Servers, Clients and Keys from DigitalOcean How to Set Up SSH Keys on Ubuntu 22.04 from DigitalOcean Simplify Your Life With an SSH Config File from Nerderati How To Add Swap Space on Ubuntu 22.04 from DigitalOcean (not recommended) Video Transcript For the rest of this module, you’ll be working with DigitalOcean to create and configure online cloud resources. I chose DigitalOcean primarily because of its ease of use compared to other providers such as AWS, as well as its continuing popularity and great user documentation.\nDigitalOcean was first founded in 2011 by Ben and Moisey Uretsky. They felt that most hosting companies of the time were targeting large enterprises, leaving many smaller software developers and startups behind. DigitalOcean was one of the first hosting providers to offer virtual machines exclusively on solid state storage, giving them a performance edge over many of their peers, while often being cheaper overall. Currently, they are the 3rd largest hosting provider in the world, with 12 worldwide datacenters serving over 1 million customers.\nIn this video, we’ll discuss the first steps for getting your first cloud server, referred to as a “droplet” on DigitalOcean, configured and secured. Let’s get to it!\nFirst, if you haven’t signed up for the GitHub Education Student Developer Pack, I highly encourage you to do so. Among many other perks included in the pack is a $50 credit for new DigitalOcean users. So, if you haven’t used DigitalOcean before, or would like to create a new account for this class, you should take advantage of that resource. In total, the entire class should use less than half of that credit as long as you don’t get behind, so you’ll have plenty left over for other projects. Finally, if you have a friend or colleague already using DigitalOcean, you can contact them for their referral URL before you sign up to do them a solid favor, netting you a $10 sign-up credit, and they’ll get a $25 credit once you spend $25 at DigitalOcean. It’s a win-win for everyone involved!\nOnce you are logged in, you’ll be ready to create your first droplet. I’m going to walk through the steps here and talk through some of the options, just so you know what is available here.\nFirst, you’ll be prompted to select an image. DigitalOcean offers many different types of images, including Linux distributions, containers, and one-click applications. The last two options are really handy if you need just a particular service or type of machine, but in our case, we’ll select the Ubuntu distribution. Currently, DigitalOcean offers all LTS versions of Ubuntu that are still supported. We’ll choose the “Ubuntu 20.04 x64” option.\nNext, you’ll need to select a droplet size. Droplets on DigitalOcean are sized by the amount of memory they offer, as well as the number of virtual CPUs available and the size of the storage disk. There are many different options to fit a variety of needs. For this class, we’ll select the cheapest option, which has 1 GB of memory, 1 virtual CPU, and 25 GB of storage space. It is more than enough for our needs, and only costs $5/month.\nDigitalOcean also provides the option to have automatically created backups of your droplets for just an additional fee. I won’t enable that option, but you are welcome to do so if you’d like to have that feature available.\nSimilarly, they also offer the ability to have your storage volumes separate from your droplets. This is handy if you’ll be building or rebuilding your droplets and want to make sure the stored data is unaffected. We won’t be using this option for this course.\nBelow that, you’ll be able to choose your datacenter region. In general, it is best to select a region close to you and where you’ll be accessing these droplets. So, I’d recommend selecting one of the New York or San Francisco options. If you are creating multiple droplets, as you will for this class, make sure they are in the same datacenter region so they are able to communicate with one another internally.\nThere are a few other options you can enable. The first is private networking, which allows droplets in the same datacenter region to communicate on a private network that is internal to DigitalOcean’s datacenter. This is great if you’ll be storing data on one droplet and accessing it via another, as you use the private network to protect that connection from eavesdropping.\nNote Private networking has been replaced by Virtual Private Cloud (VPC) networking, but it works effectively the same.\nNext, you can enable IPv6 access to the droplet. Depending on your network infrastructure, you may or may not find this useful. We won’t be enabling it in this course.\nThe user data option allows you to provide some initial configuration information to your droplet using the cloud-init program. If you are creating many droplets from scratch that all need the same configuration, this can be a very powerful tool. However, for this course we’ll be performing our configuration manually, so we won’t be using this right now.\nFinally, DigitalOcean offers advanced droplet monitoring at no extra charge. Let’s enable that option here, and later we’ll look at the information it collects for us.\nYou can also add SSH keys directly to your droplet. If you do so, the system will be configured to prevent SSH login via password, and you won’t receive the root password via email from DigitalOcean. If you have already configured an SSH key with DigitalOcean, you can add it to your droplet here, but if not, I’ll walk you through the steps to do that later in this video.\nLastly, you can create multiple droplets with the same settings. For the lab, you’ll need to create two droplets, named FRONTEND and BACKEND. For this example, however, I’m just going to create one, and name it EXAMPLE. As part of the lab, you’ll have to extrapolate what I demonstrate here on a single droplet to your setup with multiple droplets.\nFinally, I can click the “Create” button to create my droplet. After a few seconds, you should see the IP address of your droplet in your dashboard. Let’s click on it to see additional information about the droplet.\nAt the top of the page, you’ll see your droplet’s public IP address, as well as the private IP address for the internal network. Notice that the private IP address begins with a 10, which is one of the reserved network segments we discussed in Module 3. On the left of the page, you’ll see several options you can explore. For example, clicking the “Access” option will allow you to launch a virtual console to connect to your server. This is very handy if you accidentally lock yourself out of the droplet via SSH. Of course, you’ll need to actually know a password for an account on the system to log in via the console. Thankfully, if you forget your root password, there is a button below to reset that password and have it emailed to you.\nFor this example, however, let’s use SSH to connect to our droplet. I’m going to use one of my Ubuntu VMs from the earlier labs for this process, but you are welcome to use tools from your own host machine instead of running it in a VM. Mac and Linux users have easy access to SSH via the terminal already. For Windows users, I recommend using the Windows Subsystem for Linux to get access to SSH through Ubuntu installed directly on Windows. See the video on WSL in the Extras Module for more information on installing and configuring that application.\nTo connect, you’ll use a command similar to this one:\nssh root@\u003cip_address\u003e where \u003cip_address\u003e is the public IP address of your droplet. When prompted, use the password for the root account you should have received via email from DigitalOcean. If everything works correctly, you should now be logged in as the root user of your droplet.\nOf course, if you’ve been paying attention to this class, you should know that it is a very bad idea to use the root account directly on a system. That goes double for working in the cloud! So, the first thing we should do is create a new user named cis527. As you’ll recall from Module 1, you can do so using the adduser command:\nadduser cis527 It will ask you a few simple questions about that user, and then create that user for you. You’ll need to also give that user administrative privileges:\nusermod -a -G sudo cis527 Once that is done, we should log out of the root account and log in as the new account. So, let’s exit for now:\nexit Back on our local machine, we’ll need to set up some SSH keys so that we can log in securely. If you’d like to learn more about this process, please review the SSH video in the Extras Module.\nFirst, if you haven’t already, you’ll need to create a set of SSH keys on this system:\nssh-keygen -t rsa -b 4096 Then, once the key is created, you can copy it to your DigitalOcean droplet using a command similar to this:\nssh-copy-id cis527@\u003cip_address\u003e where \u003cip_address\u003e is the public IP address of your droplet. You’ll be asked to provide the password for the cis527 account you just created, and then it will install your SSH key for that user. Finally, you can log in to the system using SSH:\nssh cis527@\u003cip_address\u003e If everything works correctly, you should not be asked for your password at all, unless you put a passphrase on your SSH key. Of course, at this point you could also create an SSH config file to make this process even easier - the SSH video in the Extras module has details for how to go about doing that.\nOnce you’ve created your new user and switched to that account, there are a few important security steps you should take to make sure your droplet is properly secured. First, you should configure the firewall to only allow the ports you intend to use on this system. For example, you might allow port 80 for HTTP traffic, port 443 for HTTPS traffic, and port 22 for SSH traffic. In addition, you could restrict the IP addresses that are able to access certain ports. For this example droplet, I’m going to enable those three ports:\nsudo ufw allow 22 sudo ufw allow 80 sudo ufw allow 443 Then, I can enable the firewall:\nsudo ufw enable and check the status using:\nsudo ufw status For the lab assignment, your firewall configuration will be a bit more complex, so make sure you read the assignment carefully before enabling the firewall. If you end up locking yourself out of the system, remember that you can use the console through the DigitalOcean website to get in and fix problems with your firewall configuration.\nIn addition, I recommend securing your SSH server by disabling password authentication and root login. You can do so by editing the SSH configuration file:\nsudo nano /etc/ssh/sshd_config SSH servers on the internet receive sometimes hundreds or event thousands of login attempts per day from hackers trying to exploit systems with weak passwords. If your system doesn’t even accept passwords, it is just one less way they can potentially get in.\nAlso, you can change the port that the SSH server is listening on. This is a little bit of “security by obscurity,” which isn’t really security by itself, but it can definitely help cut down on those malicious login attempts. The lab assignment directs you to do just this. Of course, make sure you update your firewall configuration accordingly when you make this change. Once you’ve made your changes, you can restart the SSH server:\nsudo systemctl restart ssh Lastly, you may want to perform steps such as updating the timezone of your server and configuring the Network Time Protocol (NTP) client to make sure the system’s time is synchronized properly.\nOne other step that you may come across in many guides is enabling swap. Swap in Linux allows you to effectively use more RAM than what is available on your system. This is especially handy if you are dealing with large datasets that won’t fit in RAM. The desktop version of Ubuntu typically does this automatically, but many cloud providers such as DigitalOcean disable this feature. Since they are in the business of providing cloud resources, in general they will just want you to scale your resources accordingly to have enough RAM available instead of using swap. In addition, using swap on a solid-state drive can degrade performance and shorten the life of the drive. So, I don’t recommend enabling swap on your DigitalOcean droplets at all, unless you find a particular use case that would make it worthwhile.\nWith that information, you should be ready to configure and secure your first cloud server. Next, we’ll look at how to access a cloud resource using a domain name and configuring that resource accordingly.\n",
    "description": "",
    "tags": null,
    "title": "Cloud Resource Setup",
    "uri": "/5-the-cloud/04-cloud-resource-setup/index.html"
  },
  {
    "content": " YouTube Video Resources Cron How To from Ubuntu Linux Cron Guide from LinuxConfig.org Video Transcript This video introduces the Cron tool on Linux, which can be used to run tasks automatically at specific times on your system. It is a very handy piece of software, but a little confusing at first.\nTo view the current list of scheduled tasks for your user, use the following command:\ncrontab -l Generally, you probably won’t see anything there at first. You can also view the list for the root user using the sudo command\nsudo crontab -l Depending on the software installed on your system, you may already see a few entries there.\nTo edit the schedule, use the same command with a -e option:\ncrontab -e It should open the file with your default text editor, usually Nano. At the top of the file, it gives some information about how the file is constructed. At the bottom, you can add lines for each command you’d like to run. There are 6 columns present in the file, in the following order:\nm - minute (0-59) h - hour (0-23) dom - day of month (1-31) mon - month (1-12) dow - day of week (0=Sunday, 1=Monday, … 7=Sunday) command - full path to the command to be run, followed by any parameters The Linux Cron Guide from LinuxConfig.org has a great graphic describing these columns as well. All you have to do is add your entries and save the file, and the new settings will be applied automatically.\nHonestly, the best way to learn cron is by example. For example, assume we have a script at /home/cis527/bin/script.sh that we’d like to run. If we add the following entry:\n15 5 * * * /home/cis527/bin/script.sh it will run the script at 5:15 AM every day. Note that the amount of whitespace between each column doesn’t matter; it will treat any continuous sequence of spaces and tabs as a single whitespace, much like HTML. So, feel free to align your columns however you choose, as long as they are in the correct order and separated by at least one whitespace character.\nLet’s look at some other examples:\n0 */6 * * * /home/cis527/bin/script.sh This will run the script every six hours (note the */6, meaning that every time the hour is divisible by 6, it will run the script). So, it will run at 12 AM, 6 AM, 12 PM, and 6 PM each day.\n10,30,50 * 15 * 5 /home/cis527/bin/script.sh This will run every hour at 10, 30 and 50 minutes past the hour, but only on the 15th day of each month and every Friday. Note that the day of week option is generally separate from the day of month option. If both are included, it will run on each indicated day of the week, as well as the indicated day of the month.\n0 5 * * 1-5 /home/cis527/bin/script.sh This will run at 5:00 AM each weekday (Monday=1 through Friday=5)\n@reboot /home/cis527/bin/script.sh This will run once each time the system starts up.\nThere are many, many more ways to use cron, but hopefully this video gives you enough information to get started. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Cron (Linux Scheduled Tasks)",
    "uri": "/x-extras/04-cron-linux-scheduled-tasks/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello and welcome to the week eight announcements for CIS 527 and CC 510 in fall 2023. So this week you should be wrapping up the second discussion. We had Adam come in and talk about his work on Beocat. So hopefully you’re getting a response put together for that. Also due tomorrow are the quizzes for Lab 4, so hopefully you’re getting those done. And then next week by Wednesday Lab 4 is due, so hopefully you’re able to get started working on that. So some quick hints real quick about success in this course because I get asked this question every once in a while. Things that you can do to be really successful in this course is to read the lab assignments very, very carefully. There’s a lot of information in there, but there’s also a lot of hints in there about ways to do things or pitfalls we’ve run into. Those red and green and blue boxes are things that I’ve added in over the years that I think are very helpful. The linked documentation is also very helpful. A lot of times the work you’re trying to do in the lab can be accomplished by following that first or second documentation link and following the steps there. I try and post them in the order of relevance, so most likely start with the first one. It will probably answer most of your questions. But if you do get stuck in this class, if you’re not making forward progress after about an hour or if something breaks and you can’t figure it out, that is a great time to take a stop and ask questions. There’s a lot of things both Matt and I know how to help you with that it’s just impossible for us to explain in the class. For example, things like how to check your system log, how to restart services and look for bugs and how to look at these config files and see what’s going on. Especially with Lab 4, you have to be diagnosing network issues and also looking at domain issues and trying to get your certificates to work. There’s a lot of intricacy in Lab 4. If you get stuck, don’t spin your wheels. Please come talk to us. We’ve got a lot of time. Matt will be having office hours on Thursday during the one o ‘clock time. I should be there as well for a little while. Hopefully, you can use some of these tips to be successful in this course.\nFor Lab 4, we’re going to start working on authentication. For Lab 4, you’re going to be working on authentication. to create a new virtual machine using Windows Server. Windows Server is kind of a fat VM, so beware. It does require a lot of RAM and quite a bit of storage space, so hopefully that works out for you. What you’re going to do in Windows Server is you’re going to create an Active Directory domain, which is the default standard for what most systems use for this. K -State CS, even though we’re a Linux system, actually uses Active Directory as K -State does as well. On the Linux side, you’re going to create an open LDAP server, which is kind of the Linux open source equivalent of this. And then for each one, you’re going to configure a client that connects to those systems. And you’re also going to configure a little of interoperability, where you have an Ubuntu client that logs in using an Active Directory domain on Windows. As before, the biggest thing I can tell you in this lab is to make snapshots. Some of the times that are really useful to make snapshots are on your Windows Server before you promote it to an Active Directory domain controller. That process fails maybe one out of 20 times. And so every semester, there’s at least one student where that process fails. Just go back to your snapshot and try again. Same thing with OpenLDAP, the process that usually fails is adding the TLS certificate to the LDAP server. So make a snapshot before you start that process. Thankfully, the process is mostly just following the instructions and mimicking the commands from the guide. But if you screw something up, it’s much easier to roll back and try again, then to try and figure out what happened and debug it and undo it. So use the snapshots very wisely as much as you can in Lab 4.\nOnce you get done with Lab 4, you’re welcome to push right ahead and go to Lab 5. Lab 5, we’re going to shift gears and we’re going to work in the Cloud for this lab. So you’re going to create a couple of Cloud resources. In most cases, these will be droplets on DigitalOcean. You can use other Cloud providers if you would like. I have had a couple of students do these labs on AWS. I haven’t had anybody try to do them on Azure yet. I really recommend DigitalOcean. It’s the system I’m familiar with and so for debugging and things, it’s the one that I’d be most able to help you with. So you create a couple of Cloud resources, you’re configure SSH in your firewall, set up some virtual hosts in Apache, and then we’re also going to start working with Docker. And so we’re going to set up a couple of Docker containers with reverse proxy so that you can see what that looks like as well. It’s really, really fun. So as always, feel free to keep in touch. We’ve got discussions on Discord. We’ve got the ed discussion board that you can chat with. You can schedule one -on -one office hours. We’ve got our usual office hour on Thursday. I’m working on trying to schedule a couple more guest speakers. It’s just got to try and work in their schedule, but I’m going to try and get Seth Galitzer if I can. I’ll try and get somebody from K -State Central IT if I can, and I’m going to try and get somebody from industry. So I’ll be reaching out and making some of those connections and trying to schedule those over the next few weeks.\nQuick punch in here because I forgot to record this slide earlier. I have a couple of days of upcoming travel coming up this week. I will be out of the office most of Thursday through Sunday this week and also Thursday through Sunday of next week. Because of that, my response is maybe a little bit delayed and if you need any grading done by me, it may be a little delayed as well. Match should be available, so you’re more than welcome to schedule with him during those times. times. If you need any help, please don’t email me directly. Make sure you email the CIS 527 Help email address or post on ed discussion. That message goes to both myself and to Matt. So Matt will be able to jump in and help you out with things. But just making you aware that I’ve got some upcoming travel, which means I’ll be gone Thursday through Sunday the next couple weeks. So if you email me or try and contact me on either Thursday or Friday of those weeks, it might be Monday or Tuesday of the following week before I came back to you. So just bear that in mind. I’ve got a couple of days where I’m going to be gone. So hopefully that doesn’t cause any concerns. And if you need anything during those times, Matt should be able to help you out as well.\nSo it’s week eight. We’re finally at the halfway mark of the semester. Hopefully things are going well and you’re rolling into lab four with a lot of confidence. If you have any questions or concerns, please let me know or talk to Matt. He’s also really more than capable of helping you out. So best of luck on lab four. Lab four is probably one of the more difficult labs in this class. So I hope it goes well. But as always if you have any questions let us know and I will see you again in a couple weeks\n",
    "description": "",
    "tags": null,
    "title": "Fall '23 Week 8",
    "uri": "/y-announcements/week08/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Standard RAID Levels on Wikipedia Nested RAID Levelson Wikipedia Parity from Crystal Clear Mathematics What are the Different Types of Storage: Block, Object and File? on Ubuntu Blog NAS vs. SAN vs. DAS: Which is Right for You? from Seagate Blog Getting Started with Storage: Understanding SAN vs NAS vs DAS from Vanilla Video Storage Education and Learning Resources for VMware Admins from VirtualizationSoftware.com Video Transcript The first type of application server we will cover in this module is the file server. In essence, a file server is simply a system on the network that is responsible for sharing files and storage resources to users in our organization. While that may seem simple on the surface, there is quite a bit going on behind the scenes.\nOf course, the major component of any storage server is the actual storage medium itself. Typically most servers today use either the traditional, rotational hard disk drive, or HDD, or the newer solid state drives, or SSD. Each one has significant tradeoffs in terms of storage size, price, and performance, so you should look at each option closely to determine which one is best for your organization.\nOnce you have your storage, you’ll also need to understand how it is viewed by your computer or operating system. Typically storage devices can be accessed in one of three ways. Most computers use a file storage system, where data is stored and represented as files in a hierarchical file system. This is what we have been dealing with so far in this course. However, you can also use storage devices as block storage, which stores binary data in identically sized blocks. In fact, the file systems you are familiar with are actually just abstractions on top of a block-based storage device. However, block storage might be preferred for some uses, such as databases or large files. Finally, you can also treat a storage device as an object store, where data is stored as independent objects on the disk, regardless of any underlying block structure. This is very uncommon right now, but it may become more common going forward. Amazon’s Simple Cloud Storage Service, or S3, is a great example of object storage.\nAnother major concept in file servers is the use of RAID. RAID originally stood for “Redundant Array of Inexpensive Disks,”, but more recently it has been referred to as “Redundant Array of Independent Disks” as well. In a RAID, multiple disks are combined in unique ways to either increase the overall performance of the system, or to provide for better data protection in case of a failure. In some cases, RAID can even be used to achieve both goals. While you may not deal with RAID in a cloud environment, it is still very common on certain storage devices, so it is helpful to understand what it is.\nRAID uses a variety of “levels” to determine how the disks are combined. There are several commonly used RAID levels, so I’ll cover just a few of them. First is RAID 0, commonly known as “striping.” In this setup, each data file is split, or “striped” across two drives. In this way, any attempts to read or write the file are much faster than on a single drive, since they can work in tandem. However, if either drive experiences a failure, the data on both drives will be unusable. So, you gain performance, but it increases the risk of data loss.\nRAID 1 is known as “mirroring,” and is effectively the opposite of RAID 0. In RAID 1, each data file is written in its entirety to each disk. So, the disks are perfect copies of each other. If one disk fails, the system can continue to run using the other disk, often without the user even noticing the difference. With RAID 1, you gain increased resistance to data loss, but it doesn’t add any performance. Thankfully, RAID 1 can generally perform just about as well as a single drive, so there isn’t much of a performance loss, either.\nThe next most commonly used RAID is RAID 5. Yes, RAID 2 through RAID 4 exist, but they aren’t used very much in practice today. They are somewhat like B batteries in that regard. In RAID 5, typically four disks are used. When data is stored to the drive, it is written across three of them, with the fourth drive containing a “parity” section that can be used to verify the data. That parity sections are spread across each of the four drives, making read and write performance higher than if it was stored on a single drive. With this setup, if any one drive experiences a failure, the data on it can be reconstructed using the information present on the other three drives. With this setup, you can gain some performance over using a single drive, while still getting better data protection as well. Unfortunately, you have to give up one quarter of your storage space for this to work, but I’d say it is probably worth it.\nSo, what is parity? At its core, parity is just a checksum value that is based on the other values in the data. For binary, typically we use either Even Parity or Odd Parity. In Even Parity, we would make sure that the data plus the parity bit has an even number of 1s, while in Odd Parity we would make sure there are an odd number of 1s. So, in this example, looking at the second line, we see that the data includes five 1s. To make it an even number of 1s, we would set the Even Parity bit to 1. If we are using Odd Parity instead, we would set it to 0. Then, if we lost any single bit in the original data, we could determine what it was just by looking at the remaining data and the parity bit. With RAID levels such as RAID 5, we can use that same trick to reconstruct data on a drive that failed, just by looking at the remaining drives. Pretty neat, right?\nBack to RAID levels, RAID 6 is very similar to RAID 5, but with two different parity sections spread across typically 5 or more drives. RAID 6 is designed in such a way that any two drives can fail and the data will still be secure. As with RAID 5, RAID 6 also includes some performance boosts as well.\nLastly, RAID levels can sometimes be combined in unique ways. For example, RAID 1+0 is a RAID 0 made up of two RAID 1 setups. In this way, you can gain the performance boost of striping with the enhanced data protection of mirroring, all in a single RAID. There are many other ways that this can be done, depending on the number of disks you have available and the characteristics you’d like your RAID to have.\nOk, so once you’ve determined how to configure your disks, the next step is to create partitions. A partition is a division of a physical disk into multiple parts. Each partition can be used for different things, such as block storage, different file systems or operating systems, or even as swap space. Years ago, it was very common to partition disks several ways in order to store multiple operating systems on the same disk, or to separate the operating system from the data. However, as hard drives have grown in size and dropped in price, coupled with the rise of virtualization, it is very uncommon to have multiple data partitions on the same disk today. Your operating system may automatically manage a few small partitions for system recovery and boot information, but in general, you probably won’t be dealing with partitions much in the future.\nAs you set up your file server, you may also have to deal with the various protocols that can be used to access the files. Most servers today typically use the Windows-based protocol Server Message Block or SMB, which is sometimes also referred to as the Common Internet File System or CIFS. Windows servers and clients natively use these protocols to share files, and Linux-based systems can use Samba to do the same. If files are being shared between multiple Linux systems, they could also use the Network File System or NFS protocol. In the lab assignment, you’ll learn how to set up servers using the SMB protocol using Windows Server and Samba.\nIn addition, there are protocols for sharing files over the internet. The most commonly used are the File Transfer Protocol, or FTP, and the SSH File Transfer Protocol, or SFTP, which transfers files via an SSH-secured tunnel. Finally, some storage devices maybe accessed using lesser-known protocols such as the Internet Small Computer Systems Interface, or iSCSI, and the Fibre Channel Protocol, or FCP. These are typically used in storage area networks to access data stored on block devices, as well see a bit later in this video.\nAnother concern that comes with storage is the location of the storage itself. There are typically three ways to think about where to locate your storage. First, you can use DAS, or direct-attached storage. This would be large storage devices connected directly to your system, such as a large external hard drive. In some cases, this might be the best option since it is generally the simplest and cheapest. However, it isn’t very flexible if you need to share that information with multiple systems.\nThe other two options are NAS, or network attached storage, and SAN, or a storage area network. As you can probably tell, these two terms are often confusing, so I’ll try to explain them in detail.\nThis graphic shows the major difference between a NAS and a SAN. Network attached storage simply refers to storage that is available via a network. Typically, NAS is accessed at the file level, using protocols such as SMB or NFS. In essence, you can think of any dedicated file server as a NAS device. A SAN, on the other hand, is a collection of storage devices that are typically accessed at the block level by a file server, using protocols such as iSCSI or FCP. If a file server needs access to a large amount of storage, you might set up a SAN inside your datacenter, with several large-scale block storage devices that actually handle the storage of the data. Hopefully this helps you make sense of these two similar terms.\nLastly, I’d like to introduce one major trend in storage, which is the concept of storage virtualization. Just like with hardware virtualization, it is possible to add a virtual layer between the physical storage devices and the systems using those devices. In that way, you could seamlessly migrate storage across multiple physical systems without interrupting access to the data. This diagram does a great job of showing how you could use different disks across a variety of hardware setups to create virtual datastores for your data. However, storage virtualization does come with a downside, being that the virtualization layer could represent a single point of failure in your infrastructure. So, you may have to carefully analyze the risks that such as setup would bring compared to the added flexibility it provides.\nThat should give you a pretty good overview of some common terms and concepts you’ll come across when dealing with file servers. The next two videos will discuss some of the implementation steps for setting up your own file server in both Windows and Samba on Ubuntu.\n",
    "description": "",
    "tags": null,
    "title": "File Servers Overview",
    "uri": "/6-application-servers/04-file-servers-overview/index.html"
  },
  {
    "content": " Note The video refers to Puppet version 6, but Puppet 8 is now the latest version. For the lab assignment, you’ll want to install Puppet 8 and not Puppet 6. This changes the URL used to get the release package - see the assignment page for more information. The basic process is otherwise the same.\nYouTube Video Resources Puppet Agent Windows Downloads from Puppet (look for puppet-agent-x64-latest.msi) About Puppet Platform and Its Packages from Puppet Puppet Learning VM from Puppet Puppet System Requirements from Puppet Puppet Documentation from Puppet Installing Puppet Agent from Puppet Add Executables to your PATH from Puppet Video Script To begin work on Lab 2, you’ll need to install Puppet Agent on your new VMs. This video will walk you through that process.\nFirst, let’s look at installing Puppet Agent on Ubuntu. Here I have an Ubuntu VM configured as described in the Lab 2 assignment, except I have not installed the Puppet Agent software yet. To install Puppet Agent, we must first enable the Puppet Platform repositories. A link to these instructions are in the resources section below the video. On that page, scroll down to the section titled “Enable the Puppet Platform on Apt” and enter the two commands given.\nHowever, we’ll need to determine the URL required for our version of Puppet and Ubuntu. In this case, we’d like to install Puppet version 6, and we are using Ubuntu 20.04, which is codenamed “Focal Fossa”. So, our URL would consist of ‘puppet6’ as the platform name, and ‘focal’ as the OS abbreviation. So, the full URL will be https://apt.puppetlabs.com/puppet6-release-focal.deb. When we place that first URL after wget in the first command, it will simply download a .DEB installation file to your computer. The second command uses the dpkg tool to install that file.\nOnce we’ve done that, we can use sudo apt update and sudo apt install puppet-agent to install the Puppet Agent program on Ubuntu.\nHowever, you won’t be able to use those commands until we add them to the PATH environment variable. The PATH variable is a list of folders that contain the commands you can access from the Terminal. If you have reviewed the information in the Extras module for Bash Scripting, you are already familiar with the PATH variable. The instructions for installing Puppet Agent on Linux linked in the resources section gives one way to add these commands to your PATH variable, but it is incomplete and will not work in all cases. So, there are two options: one would be to use the full path each time you need to use the Puppet commands, and the second is to modify the PATH variable. In this video, I’ll walk you through the steps to modify your PATH to enable direct access to these commands.\nFirst, you must add it to your own PATH variable. To do so, use the following command to edit your Bash configuration file:\nnano ~/.bashrc Use the arrow keys to navigate to the bottom of the file. Then, on a new line, add the following:\nexport PATH=/opt/puppetlabs/bin:$PATH Then press CTRL+X, then Y, then ENTER to save and close the file. Finally, close and reopen Terminal to load the new PATH variable. If you did it correctly, you should be able to run the puppet command, as you can see here.\nHowever, if you try to use sudo to run the puppet command as root, you’ll notice that it still cannot find the command. This is due to the fact that the system protects the PATH variable from changes when using root privileges in order to enhance system security. So, you’ll also need to edit the PATH variable used by the sudo command. To do so, use the following command to open the sudo configuration file:\nsudo visudo This command will open the \\etc\\sudoers file on your system for editing. Near the top, you’ll see a line for Defaults secure_path containing the PATH variable used by the sudo command. Carefully edit that line by adding the following text to the end, before the closing quotation mark:\n:/opt/puppetlabs/bin Note that I added a colon to the end of the existing line, then the new path. On my system, the full line now looks like this:\nDefaults secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/opt/puppetlabs/bin\" Once you are done editing, you can use CTRL+X, then Y, then ENTER to save and close the file. You should now be able to use the sudo puppet command as well.\nNow, let’s switch over to Windows and install the Puppet Agent there. First, you’ll need to download the Puppet Agent using the link on the resources page below this video. Remember to find the latest version of the Puppet Agent installer, as there are many listed on this page. Once you have downloaded the file, simply double-click on it to run the installer. It will install Puppet Agent on the computer. It’s that simple!\n",
    "description": "",
    "tags": null,
    "title": "Installing Puppet",
    "uri": "/2-configuration-management/04-installing-puppet/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Docker Tutorial for Beginners by TechWorld with Nana on YouTube 16 Best Container Orchestration Tools and Services from DevopsCube Docker Compose from Docker Installing Docker Compose from Docker Compose File Specification from Docker Video Transcript Up to this point, we’ve only looked at how we can start and run containers one at a time on a single platform. While that is great for development and testing, we should know by now that manually doing anything in an enterprise situation is not a great idea. So, let’s explore some of the methods and tools we can use to better orchestrate our containers and automate the creation and deployment of them.\nContainer orchestration is the overarching term I’ll use for this concept, though it maybe isn’t the most descriptive term. When are orchestrating containers, we are really talking about ways to manage and deploy containers at scale, sometimes on a single node, and sometimes across multiple nodes. For now, we’ll only focus on a single node, and we’ll address working with multiple nodes later in this module.\nOrchestration also includes things such as managing the routing of network data and other resource between the various nodes, and possibly automating the ability to restart a node if it fails.\nIn short, we want to take the same approach we took in Lab 2 related to building individual systems and apply that same idea to working with containers.\nHere’s a quick diagram showing what a full-fledge container orchestration setup might look like. At the top, we have a configuration file defining the architecture we want to deploy - in this case, a Docker Compose file, which is what we’ll cover in this lesson. That file is then used to control multiple Docker engines running across multiple individual computing nodes, or servers on the cloud. This allows us to run containers across multiple nodes and handle situations such as load balancing and automated recovery in case of an error.\nUnderneath the Docker engine, we may have multiple logical “clusters” of nodes or containers, which represent the different environments that our nodes are operating within. For example, we may have a test cluster and a production cluster, or our application may be separated across various physical or logical locations.\nKubernetes follows a similar architecture, but we’ll look at that more in depth later in this module.\nSo, for this example, let’s look at another Docker tool, Docker Compose. Docker Compose is used to create various isolated Docker environments on a single node, usually representing various application stacks that we need to configure. Using Docker Compose, we can easily record and preserve the data required to build our container infrastructure, allowing us to adopt the “infrastructure as code” principle. Once we’ve created a Docker Compose file, we can easily make changes to the infrastructure described in the file and apply those changes to the existing setup.\nFinally, using Docker Compose makes it easy to move an infrastructure between various individual systems. A common use-case is to include a Docker Compose file along with a repository for a piece of software being developed. The Docker Compose file defines the infrastructure of other services that are required by the application, such as a database or message queue.\nThe Docker Compose tool uses files named docker-compose.yml that are written using the YAML format. Inside of that file, we should see a services heading that defines the various services, or containers, that must be created. This particular file lists two services, nginx and mysql. Those second-level service names can be anything we want them to be, but I’ve found it is easiest to match them closely to the name of the Docker image used in the service. As a minimal example, each service here contains the name of a Docker image to be used, as well as a friendly name to assign to the container. So, this Docker Compose file will create two containers, one running Nginx and another running Mysql. It’s pretty straightforward, but as we’ll soon see, these files can quickly become much more complex.\nOnce we’ve created a Docker Compose file, we can use the docker compose commands to apply that configuration. Previously, docker-compose was a separate Docker client that was installed individually, but with the release of version 2.0 it was integrated within the main Docker client as a plugin. If you installed Docker Desktop, you should already have docker compose available as well, but if not you may have to install the Docker Compose plugin. The instructions for this are available in the links at the top of this page.\n[demo here]\nOnce major feature of the Docker Compose file format that many users may not be aware of is the ability to extend a service definition from another file. This allows us to create individual files for each service, and them compose them together in a single docker-compose.yml file. We can also use this same technique to share some settings and configuration between many different services within the same file.\nIn this example, we see our main docker-compose.yml file that contains a service definition for a webserver. That definition extends the basic nginx service that is contained in another file named nginx.yml in the same directory. Using this method, we can create multiple different web servers that all inherit the same basic nginx configuration, but each one can be customized a bit further as needed. This greatly reduces the amount of duplicated code between services, and can also help to greatly reduce the size of the main docker-compose.yml file for very complex setups.\nThere we go! That’s a quick crash course in using Docker Compose to build a defined configuration for a number of Docker containers. For the rest of this lab, we’ll mainly work in Docker Compose, but we’ll show some basic Docker commands and how they compare to the same setup in Docker Compose.\n",
    "description": "",
    "tags": null,
    "title": "Orchestration",
    "uri": "/5a-containers/04-orchestration/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello, and welcome to the week for announcements video for CIS 527 in summer 2022. This week, you should be working on lab three, which is due today by 7pm. So make sure you getting that wrapped up and also that you have a time scheduled with me to get your grading done. I’ll talk about that in just a minute. You’re also should be working on the week three discussion, which is due tonight by 11:59pm. So make sure you watch the video, write some responses and also provide some new questions for our guests. This week, you’re working on week four. So we four quizzes are due Friday. And then the next lab which is lab four is due next Tuesday. We’re postponing it today to to the holiday on Monday, the July 4. So there is a lab that’s due next Tuesday, you’ll need to schedule a time with me to get that graded. But there is no discussion this week, we’re pushing off the discussion until week five. So you don’t have to worry about a discussion this week, just focusing on lab four.\nSo for lab three grading, you should have done a lot of different things revolving around core networking in this lab. So the things I’m going to be looking for in a grade is the remote connections. So making sure you can use remote desktop into your Windows system and using SSH into your Linux system. You should also have set a static IP on your Ubuntu Server VM. And then on that server, we’re going to check your DNS settings your DNS lookups, we’re going to check DHCP. And I’m also going to look at SNMP and Wireshark. Hopefully, you can do the live demo of SNMP. But a lot of times, it doesn’t always work the way you want it to. So I’m hoping that you follow the labs and took screenshots so that I can look through everything. Same thing with Wireshark, you should have screenshots for a bunch of things in Wireshark, to show that you’re able to find those individual packets and find the data that we’re looking for. As always, if you have any questions about this, let me know you can talk to me and ask questions before grading starts. But as soon as we start grading, you can’t change anything. So if you realize you missed something or something is incorrect. Once we start grading, it’s too late, I have to take it as it’s submitted. So if you have any questions on lab three, please let me know.\nSome tips for success on lab three, this is one of the first more difficult labs in this class. So I really encourage you have to take some time, read the lab very carefully and make sure that you understand what it is asking you to do. This lab in the next lab I give quite a lot of information on so it should be pretty easy to implement it as it’s written. But you do have to read carefully and make sure you understand what I’m asking for. You may also have to spend some time reading various bits of documentation to understand how to do what I’m asking you to do. But thankfully for a lot of this, I’ve provided some tutorials that you can pretty much walk through and adapt a little bit to your environment to make sure it works. Make sure you check out the posted hints, especially at the end of lab three, there are a couple of extra pages on how to do debugging with DNS, what your network diagram should look like things like that. Make sure you use the resources linked in this lab, especially in the lab itself. There’s a lot of resources that are what I use when I have to do these things. So make sure you take advantage of those. Like I said, don’t be afraid to ask me questions. And finally, the big thing with this lab is don’t spin your wheels. If you feel like you’re getting stuck, if it’s taking you a long time to solve something. Take a minute slow down, ask me questions. Generally, if you’ve spent more than about an hour trying to solve something, getting it to work, and it doesn’t seem to work, that’s a good chance to take a step back and ask for help. It is entirely possible for you to run into issues that you cannot solve in this lab. Sometimes students will bring to me issues that I cannot solve. And so if I can’t fix it, it’s not worth spending any of your time trying to fix it as well. So don’t spend your wheels ask for help if you get stuck.\nSo our speaker coming up is Kyle Hutson, this is actually the week five speaker that you’ll be looking at, but I’m going to briefly talk about him now. Kyle Hutson is one of the Beocat admins at K-State and manages K-State’s supercomputer. If you’ve been an engineering building, you’ve probably seen it right outside the computer science classrooms. He has a lot of experience working on powerful hardware and large multi cluster systems. He does a lot of talking about working in the cloud versus working on premises. So he brings a really unique look at how we do this. He also does a lot with scientific computing. And so scientific computing is a different realm to work in than most industrial computing setups because you really want to maximize your CPU capabilities there. So Kyle is going to talk all about Beocat, you’ll get to watch that. And that is due in week five.\nSo coming up after this week, the next lab is lab four, where you’re going to be working on Active Directory and LDAP. Basically doing authentication against the server. So you’ll set up a Windows Server VM, you’ll configure it for Active Directory, you’ll add your Windows client so that so it uses the Active Directory to log in. You’ll also add one of your Ubuntu clients to that so that logs into Active Directory. And then likewise on your Ubuntu server, you’ll set up an open LDAP server and configure your one of your Ubuntu clients to log in via open LDAP. So it’s kind of complex, but it sets some of the groundwork for a lot of other things that you can do in system administration. It really mimics what we actually have set up in the computer science department where we have an Active Directory server that everything logs in against. Biggest hints for lab four - take the time and make snapshots, especially, I encourage you to make a snapshot of your Windows Server VM before you try and install Active Directory. The Active Directory install process is notorious for failing every once in a while. And so if you can roll back to that snapshot and try it again, it will make your life a lot easier. So take snapshots anytime you get something working, or before you start something so that you can roll back. The same goes for open LDAP. When you configure TLS, for open LDAP, it is a very fraught process, it even takes me a couple of tries to get it right. So make snapshots with open LDAP before you start trying to do the TLS process. So you can roll back to that snapshot if it doesn’t work.\nSo after lab four, we have lab five, lab five has a lot of new content in it this year. So we’ll have to see how it goes. In lab five, we’re going to configure a couple of cloud resources, we’ll set up SSH, configs, and firewalls. And then the two big new things this year is we’re going to add Docker to our cloud resources. And we’re going to set up some Docker containers and a Docker reverse proxy. So those are some new things we’re going to look at this year. I’m open to feedback. So if you have any questions or concerns about the new content around Docker, please let me know so we can improve that and make that better for the future.\nOther than that, don’t be afraid to keep in touch. We’ve got discussion times on Discord. We’ve got Tea Time office hours, Tuesdays at 330 Fridays at 1030. You’re always welcome to join and hang out anytime that we’re on Tea Time. You can also schedule a one on one office hours with me, you can shoot me an email if you have questions. There’s lots of ways that you can get help. So don’t be afraid to take advantage of that. Other than that, hopefully this week and next week are not too frustrating, but I will be open with you. lab three and lab four are probably the most difficult and frustrating labs in this class. Especially if you’ve never worked with this technology before. It really can be painful to get it working the first time so don’t let it get to you take some time. Take some breaks, ask questions if you get stuck. And as always, I wish you the best of luck and happy Fourth of July and I will talk to you next Tuesday.\n",
    "description": "",
    "tags": null,
    "title": "Summer '22 Week 4",
    "uri": "/y-announcements/old/summer2022/week04/index.html"
  },
  {
    "content": " YouTube Video Resources Slides The TCP/IP Guide Transmission Control Protocol on Wikipedia User Datagram Protocol on Wikipedia List of TCP and UDP Port Numbers on Wikipedia Video Transcript Now, let’s take a look at layer 4 of the OSI model - the transport layer.\nThe transport layer is responsible for facilitating host-to-host communication between applications on two different hosts. Depending on the protocol used, it may also offer services such as reliability, flow control, sustained connections, and more. When writing a networked application, your application typically interfaces with the transport layer, creating a particular type of network socket and providing the details for creating the connection. On most computer systems today, we use either the TCP or UDP protocol at this layer, though many others exist for various uses.\nFirst, let’s look at the Transmission Control Protocol, or TCP. It was developed in the 1980s, and is really the protocol responsible for unifying the various worldwide networks into the internet we know today. TCP is a stateful protocol, meaning that it is able to maintain information about the connection between packets sent and received. It also provides many services to make it a reliable connection, from acknowledging received packets, resending missed packets, and rearranging packets received out of order, so the application gets the data in the order it was intended. Because of this, we refer to TCP as a connection-oriented protocol, since it creates a sustained connection between two applications running on two hosts on the network.\nHere is a simplified version of the state diagram for TCP, showing the process for establishing and closing a connection. While we won’t focus on this process in this course, you’ll see packets for some of these states a bit later when we use Wireshark to collect network packets.\nSince TCP is a stateful protocol, it includes several pieces of information in its packet structure. The two most notable parts are the sequence and acknowledgement fields, which allow TCP to reorganize packets into the correct order, resend missing packets, and acknowledge packets that have been successfully received. In addition, you’ll notice that it lists a source and destination port, which we’ll cover shortly.\nThe other most commonly used transport layer protocol is the User Datagram Protocol, or UDP. Unlike TCP, UDP is a stateless, unreliable protocol. Basically, when you send a packet using UDP, you are given no guarantees that it will arrive, or no acknowledgement when it does. In addition, each packet sent via UDP is independent, so there is no sustained connection between two hosts. While that may seem to make UDP completely useless, it actually has several unique uses. For example, the domain name system or DNS uses UDP, since each DNS lookup is essentially a single packet request and response. If a request is sent and no response is received quickly enough, it can simply resend another request, without the extra overhead of maintaining any state for the previous connection. Similarly, UDP is also helpful for streaming media. A single lost packet in a video stream is not going to cause much of an issue, and by the time it could be resent, it is already too old to be of use. So, by using UDP, the stream can have a much lower overhead, and as long as enough packets are received, the stream can be displayed.\nSince UDP is stateless, the packet structure is also much simpler. It really just includes a source and destination port, as well as a length and checksum field to help make sure the packet itself is correct.\nSo, to quickly compare TCP and UDP, TCP is great for long, reliable connections between two hosts, whereas UDP is great for short bursts of data which could be lost in transit without affecting the application itself.\nOne great way to remember these is through the two worst jokes in the history of Computer Science.\nWant to hear a TCP joke? Well, I could tell it to you, but I’d have to keep repeating it until I was sure you got it.\nWant to hear a UDP joke? Well, I could tell it to you, but I’d never be sure if you got it or not.\nSee the difference?\nBoth TCP and UDP, as well as many other transport layer protocols, use ports to determine which packets are destined for each application. A port is simply a virtual connection point on a host, usually managed by the networking stack inside the operating system. Each port is denoted by a 16-bit number, and typically each port can only be used by one application at a time. You can think of the ports like the name on the envelope from our previous example. Since multiple people could share the same address, you have to look at the name on the envelope to determine which person should open it. Similarly, since many programs can be running on the same computer and share the same IP, you must look at the port to figure out which program should receive the packet.\nThere are several ports that are considered “well known” ports, meaning that they have a specific use defined. While there is no rule that says you have to adhere to these ports, and in some cases it is advantageous to ignore them, these “well known” ports are what allows us to communicate easily over the internet. When an application establishes an outgoing connection, it will be assigned a high-numbered “ephemeral” port to use as the source port, whereas the destination port is typically a “well known” port for the application or service it wishes to communicate with. In that way, there won’t be any conflicts between a web browser and a web server operating on the same host. If they both tried to use port 80, it would be a problem!\nWhen ports are written with IP addresses, they are typically added at the end following a colon. So, in this example, we are referencing port 1234 on the computer with IP address 192.168.0.1.\nThere are over 1000 well known ports that have common usage. Here are just a few of them, along with the associated application or protocol. We’ll look more closely at several of these protocols later in this module.\nSo, to summarize the OSI 7-layer network model, here’s the overall view of the postal service analogy we’ve been using. At layers 5-7, you would write a letter to send to someone. At layer 4, the transport layer, you’d add the name of the person you’d like to send it to, as well as your own name. Then, at layer 3, the network layer would add the to and from mailing address. Layer 2 is the post office, which would take your envelope and add it to a box. Then, at the physical layer, a truck would transport the box containing your letter along its path. At several stops, the letter may be inspected and placed in different boxes, similar to how a router would move packets between networks. Finally, at the receiving end, each layer is peeled back, until the final letter is available to the intended recipient.\nIn networking terms, the application creates a packet at layers 5 - 7. Then, the transport layer adds the port, and the network layer adds the IP addresses to the packet. Then, the data link layer puts the packet into one or more frames, and the physical layer transmits those frames between nodes on the network. At some points, the router will look at the addresses from the third layer to help with routing the packet along its path. Finally, once it is received at the intended recipient, the layers can be removed until the packet is presented to the application.\nI hope these videos help you better understand how the OSI 7-layer network model works. Next, we’ll discuss how to use these concepts to connect your systems to a network, as well as how to troubleshoot things when those connections don’t work.\n",
    "description": "",
    "tags": null,
    "title": "Transport Layer",
    "uri": "/3-core-networking-services/04-transport-layer/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Working with user accounts and groups is one of the key tasks within system administration. Before we get to working directly within a system, here is some background information about users and groups to help you understand why they are so important.\nEarly computers did not have any concept of user accounts. If you had physical access to a machine and knew how to use it, you could. As computers became more powerful, the concept of time-sharing became increasingly important. While one user was reviewing outputs or rewriting code, another user could use the computer to run a program. To keep track of each person’s usage, user accounts were introduced.\nWith user accounts, each person could be assigned different permissions, allowing some users to have full access while protecting the system from other users who may not have as much knowledge or experience. In addition, user accounts aid in the process of auditing, useful when you need to determine which user performed a malicious action. Finally, user accounts can help protect your systems from unauthorized use. A strong user account and permissions setup is one of the first lines of defense in any cybersecurity scenario.\nOne of the major concepts in user accounts is authentication and authorization. Authentication is the process of confirming a user’s identity by a computer, usually through the use of a password or some other authentication factor. We’ll discuss those more deeply on the next slide.\nOnce a user is authenticated, the user account can then be given authorization to access resources on the computer system. One important caveat to keep in mind: authentication does not imply authorization. Put simply, just because a computer system is able to recognize your user account does not mean you’ll automatically be given access to that system.\nLet’s focus a bit more on authentication. Typically authentication is performed by a computer system confirming the presence of one or more authentication factors from the user. There are three traditional types of authentication factors:\nOwnership - something the user has, such as keys, keycards, tokens, phones, etc. These are typically something physical, but could also be the ownership of a particular email account or phone number. Knowledge - something the user knows, such as a password, pass phrase, PIN, or other item. Most computer systems rely solely on knowledge factors for authentication. Inherence - something the user is. This includes things such as fingerprints, retina scans, DNA, or other factors about the user which would be very difficult to change or duplicate. Many modern computer systems, particularly ones used online or in highly secure areas, may use multiple factors of authentication, typically referred to as “two-factor” or “multi-factor” authentication. Some examples are using a debit card, where the card is the ownership factor and the PIN is the knowledge factor, or logging on to an online game, where the password is the knowledge factor and the phone number or email address used for verification is the ownership factor.\nOnce the user is authenticated, the system has several methods for determining if the user is authorized to access system resources. Typically, the system has some sort of security policy, access control list, or file security in place to determine what resources a user can access. We’ll discuss these in more detail as we work with each type of system.\nOne important concept in user accounts is the use of a user identifier for each account. Internally, the operating system refers to the user account by that identifier instead of the username. This allows the user to change usernames in the future, and for users to reuse usernames from old accounts without inheriting that user’s authorization. User identifiers should never be reused. On Linux, this is referred to as the user identifier or UID, while Windows uses the term security identifier or SID.\nBeyond the identifier, most operating systems store a few other bits of information about the user account. Namely, the username, password, location of the user’s home directory, and any groups the user should be a member of.\nSpeaking of groups, they are simply a list of user accounts. Their usefulness comes in the fact that you can assign system permissions to a group of users instead of each user individually. That way, as users come and go on a system, they can simply be assigned to the correct groups in order to access resources. Of course, users can be a member of multiple groups, and each group has a unique identifier as well.\nFinally, let’s review some best practices for working with user accounts on a system. First and foremost, each user of the system should have a unique account. Do not allow users to share accounts, as that will make auditing and working with users who leave that much more difficult.\nIn addition, enforce strong password policies, and require your users to change their passwords regularly. The recommendation varies, but I’ve found that changing the password every few months is a good practice.\nWhen you are assigning user permissions, follow the principle of least privilege. Only give users access to the smallest number of resources they need to complete their task. Most users do not need to be full administrators, and don’t need access to all of the files on the system. It is much better to be a little overprotective at first than to allow information to be lost due to a user having unnecessary access.\nAlso, it is very important to create and maintain logs for when users access a system or use administrator privileges. Most computer systems can be configured to create these logs with minimal effort, so it is well worth your time to do so.\nAs users leave your organization, it is also very important to quickly disable their access. There are many stories online of users leaving a company, only to access the systems after they left to either steal resources or destroy data. By having a policy to disable old user accounts as soon as possible, you’ll be able to minimize that risk.\nFinally, as a system administrator, it is very important to get into the practice of not using an administrator account as your normal user account on the system. In most cases, you yourself won’t need administrator access very often, so by using a normal account, you’ll protect your entire organization in the off chance that your account is compromised.\nThat is all the background information regarding users and groups for this module. Hopefully it will be useful as you work on the first lab in this class.\n",
    "description": "",
    "tags": null,
    "title": "Users \u0026 Groups",
    "uri": "/1-secure-workstations/04-users-groups/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Why Circuit City Busted, While Best Buy Boomed Video Script Before you begin this course, here is a quick overview of what you’ll be learning and why it is important.\nFirst, the labs. Lab 1 - Secure Workstations, is all about installing and configuring computers for your end users to use in a large organization. This is a great place to start in system administration, because it is very similar to working on your own, personal computer. You’ll install an operating system, learn about its various parts, and configure a secure and stable workstation. If you’ve taken my CIS 225 course, this is basically a review of a large portion of that material.\nLab 2 - Configuration Management, builds on the first lab by showing how you can use tools to automate much of the workstation configuration process. For this class, we’ll be learning how to use Puppet, but many other tools exist for this task. At the end of this lab, you should have a script available that will automatically perform most of the configuration you did in lab 1.\nLab 3 is all about networking. Most modern computers are completely useless without access to network resources, so this is a vital part of system administration. In this lab, you’ll configure a network for your virtual machines, as well as several networking services for them to use. We’ll also be exploring several common networking protocols, such as HTTP, DNS, and DHCP, using tools such as Wireshark to capture and inspect individual network packets. Some of this material comes directly from CIS 525, but with a focus more on system administration than building networked programs.\nLab 4 expands upon the previous labs by creating centralized directory services for your workstations, Using such a service, users can use a single username and password to access resources throughout an organization, much like your eID is used here at K-State. This lab is notoriously one of the most difficult and frustrating labs to complete in this course, but I believe it is also one of the most useful ones as well.\nLab 5 shifts the focus in this class toward the cloud. Many companies today operate some, if not all, of their centralized assets in the cloud. In this lab, you’ll configure several cloud resources, and we’ll discuss the tradeoffs and differences between cloud resources and other traditional computing resources.\nLab 6 focuses on building servers for a variety of enterprise use cases, including file servers, application servers, web servers, and more. We’ll discuss what it takes to provide a large number of resources for an organization, and you’ll get hands-on experience working with several of them.\nThe final lab introduces many smaller concerns for system administrators, but each of them is vital to creating truly stable and effective resources. You’ll learn about state-of-the-art techniques in backups, system monitoring, developer operations (or DevOps), and some of the theory behind being an effective system administrator through ITIL, formerly the Information Technology Infrastructure Library.\nAt the end of the semester, you’ll complete a final project that integrates and demonstrates all of your knowledge about system administration. You’ll be asked to communicate information effectively, show that you can perform your own research and analysis, and more. The final project module is already available on K-State Online for you to view, so I recommend you review it soon.\nAt this point, you may be asking yourself why this information is so important to learn? Here are a few thoughts from my own experience to help answer that question.\nFirst and foremost, computers are ubiquitous in today’s world. There is hardly a career left that doesn’t involve technology in some way, from engineering and science to agriculture and even hotel and restaurant management. By learning how to work closely with these tools, you’ll have an indispensable skill set for your entire career, no matter what path you choose.\nLikewise, this class gives you a chance to hone your own technical skills. Students who have previously taken this class have reported that the information covered was very helpful for them during their internships and job searches, and they were quickly able to understand how to use the corporate systems available to them.\nSimilarly, you’ll also learn how to maximize your effectiveness on your personal computer, through the use of scripting, virtual machines, and operating system features. It will truly take you from a normal computer user to a power user.\nIn addition, throughout the class you’ll be honing your communication skills, which is constantly cited as one of the most important soft skills for finding a job in the tech industry. If you are able to communicate better than your peers, you’ll be that much closer to landing your dream job.\nSpeaking of which, there are lots of great careers out there if you do decide to pursue system administration. Every company both large and small has need of someone with this particular skill set, and each industry has a unique set of challenges and requirements. A system administrator will always be in demand, and I can guarantee that the job is never boring.\nYou’ll also be able to put your skills and knowledge to good use by helping others discover how to use their computing resources most effectively. It is a great opportunity to share your knowledge with others and build those working relationships.\nFinally, yes, you may even find yourself in a management position, or you may be asked to work with management. Having a strong understanding of every aspect of an organization’s technical infrastructure and what’s out there can help you make good decisions and lead effectively. There are countless stories of companies who miss out on the latest technological advances, only to find themselves left behind or worse. In the resources section below the video, you can read more about what happened to Circuit City, a large electronics retailer that chose to become complacent, and ended up bankrupt.\nWith that in mind, I hope you are excited to continue this course and start working on the first module.\n",
    "description": "",
    "tags": null,
    "title": "What You'll Learn",
    "uri": "/0-introduction/04-what-youll-learn/index.html"
  },
  {
    "content": " Note This video uses Windows Server 2016, but the process should be functionally identical in Windows Server 2019. Also make sure you refer to the Lab 4 assignment for the correct root domain name. –Russ\nYouTube Video Resources Step-By-Step: Setting Up Active Directory in Windows Server 2016 from Microsoft Add User Accounts on Active Directory from Server-World AD DS Installation and Removal Wizard Page Descriptions from Microsoft NetBIOS on Wikipedia Video Transcript In this video, I’ll go through some of the process of configuring a Windows Server 2016 VM to act as an Active Directory Domain Controller. The goal of this video isn’t to show you all the steps of the process, but provide commentary on some of the more confusing steps you’ll perform.\nIn general, I’ll be following the two guides linked in the resources section below this video. The third guide gives quite a bit of additional information about each option in the Active Directory Domain Services installation wizard, and is a very handy reference as you work through this process.\nFirst, I’ll need to set a static IP address on this system. I’ll be using the IP address ending in 42 for this server, which was reserved in in the DNS server configured in Lab 3. I’ll also need to configure some static DNS entries. For a Domain Controller, you’ll always want to make the first DNS entry the IP address of the server itself. This is because a Domain Controller also acts as a DNS server for the domain, so you’ll need to tell this system to refer to itself for some DNS lookups. For the other entry, I’ll use the VMware router, which is the same as the default gateway address in my setup.\nNext, I’m going to follow the steps to add the “Active Directory Domain Services” role to this server. This process is pretty straightforward, so I’ll leave it to you to follow the guide.\nOnce the installation is complete, you’ll be prompted to promote the server to a domain controller. This is the step where you’ll actually configure the Active Directory installation. For this setup, we’re going to create a new forest, using the cis527.local root domain name. You’ll need to adjust these settings to match the required configuration for the lab assignment. By using a .local top-level domain, we can guarantee that this will never be routable on the global internet. The use of a .local suffix here doesn’t actually comply with current DNS standards, but for our internal VM network it will work just fine. Of course, if you are doing this on an actual enterprise system, you may actually use your company’s internet domain name here, with a custom prefix for your directory. For example, the K-State Computer Science department might use a domain name of ldap.cs.ksu.edu for this server.\nNext, you’ll need to configure the functional level of the domain. If you have older systems in your network, you will want to choose the option here for the oldest domain controller in the domain. Since we’re just creating a new system, we can use the latest option here.\nYou’ll also need to input a password here for Directory Services Restore Mode. In an enterprise setting, this is the password that you’ll use if you ever have to restore your domain from a backup, or work with a domain controller that will not boot correctly. So, you’d generally make this a unique password, and store it in a safe location. For our example, I’m just going to use the password we’ve been using for all of our Windows systems, but in practice you would make it much more secure.\nNext are the DNS options. If you are running a separate DNS server in your organization, you’ll need to checkmark this box to create a DNS delegation. However, for this lab assignment you won’t need to do that, since we aren’t trying to combine our Windows and Linux environments.\nFollowing that, you’ll need to configure the NetBIOS name for the domain. In general, this is just the first part of your fully domain name. NetBIOS is a very old protocol that is not commonly used, but some Windows systems will still use it to find resources on a network. You can just accept the default option here, unless a different name would be more reasonable.\nIn addition, you’ll be able to configure the storage paths on your system. For this example, we’ll just keep the default paths. However, on a production server, it might make more sense to store these on a different drive, just in case you have to reinstall the operating system in the future. These are the folders that store the actual database and information for the Active Directory domain.\nFinally, you’ll be able to review the options before applying them. If you’d like, you can click the “View script” button to view a PowerShell script that you could use to perform these steps on a Windows Server without a GUI installed.\nWith all the settings in place, you can continue with the configuration process. The first step is a prerequisites check to make sure that the server is able to complete the installation. You may get a couple of warnings about weaker cryptography algorithms and DNS server delegation, but you can ignore those for this example. If the prerequisites check is passed, you can click “Install” to install the service.\nOnce it has installed and rebooted, you’ll notice that you are now prompted to log in as a domain user. On Windows systems, the domain name is shown before the \\, followed by your username. So, in this example, it now wants me to log in as CIS527\\Administrator. This is a different account than the Administrator account on the computer itself, which we were using previously. To log in as a local account, you can use the computer name, followed by a \\ and then the username. If you don’t remember the computer name, you can click the “How do I sign in to another domain?” link below the prompt to find it. For now, we’ll log in as the domain administrator account.\nOnce you’ve logged in, you’ll need to create at least one user on the Active Directory domain. You can find the Active Directory Users and Computers application under the Tools menu on the Server Manager. Thankfully, adding a user here is very similar to adding a user in the Computer Management interface on a Windows computer. As before, make sure you uncheck the box labeled “User must change password at next logon” and checkmark the “Password never expires” box for now.\nThat should get the server all set up! Next, we’ll look at adding a Windows 10 computer to your new Active Directory Domain.\n",
    "description": "",
    "tags": null,
    "title": "Windows Active Directory Installation",
    "uri": "/4-directory-services/04-windows-active-directory-installation/index.html"
  },
  {
    "content": "Consistent usernames across devices.\n",
    "description": "",
    "tags": null,
    "title": "Directory Services",
    "uri": "/4-directory-services/index.html"
  },
  {
    "content": "Lab 3 - Core Networking Services Instructions Create three virtual machines meeting the specifications given below. The best way to accomplish this is to treat this assignment like a checklist and check things off as you complete them.\nIf you have any questions about these items or are unsure what they mean, please contact the instructor. Remember that part of being a system administrator (and a software developer in general) is working within vague specifications to provide what your client is requesting, so eliciting additional information is a very necessary skill.\nNote To be more blunt - this specification may be purposefully designed to be vague, and it is your responsibility to ask questions about any vagaries you find. Once you begin the grading process, you cannot go back and change things, so be sure that your machines meet the expected specification regardless of what is written here. –Russ\nAlso, to complete many of these items, you may need to refer to additional materials and references not included in this document. System administrators must learn how to make use of available resources, so this is a good first step toward that. Of course, there’s always Google !\nTime Expectation This lab may take anywhere from 1 - 6 hours to complete, depending on your previous experience working with these tools and the speed of the hardware you are using. Installing virtual machines and operating systems is very time-consuming the first time through the process, but it will be much more familiar by the end of this course.\nTask 0: Create 3 VMs For this lab, you’ll need to have ONE Windows 10 VM, and TWO Ubuntu 22.04 VMs available. You may reuse existing VMs from Lab 1 or Lab 2. In either case, they should have the full expected configuration applied, either manually as in Lab 1 or via the Puppet Manifest files created for Lab 2.\nFor the second Ubuntu VM, you may either quickly install and configure a new VM from scratch following the Lab 1 guide or using the Puppet Manifest from Lab 2, or you may choose to create a copy of one of your existing Ubuntu VMs. If you choose to copy one, follow these steps:\nCompletely shut down the VM - do not suspend it. Close VMware Workstation. Make a copy of the entire folder containing the VM. Open the new VM in VMware Workstation (look for the .VMX file in the copied folder). When prompted, select “I copied it” to reinitialize the network interface. THIS IS IMPORTANT! Boot the new VM, and change the hostname to cis527s-\u003cyour eID\u003e. How to Change Hostname on Ubuntu 20.04 from LinuxConfig.org Warning If you do not follow these instructions carefully, the two VMs may have conflicts on the network since they’ll have identical networking hardware and names, making this lab much more difficult or impossible to complete. You have been warned! –Russ\nClearly label your original Ubuntu VM as CLIENT and the new Ubuntu VM as SERVER in VMware Workstation so you know which is which. For this lab, we’ll mostly be using the SERVER VM, but will use the CLIENT VM for some testing and as part of the SNMP example in Task 5.\nNote VMware Fusion (Mac) Users - Before progressing any further, I recommend creating a new NAT virtual network configuration and moving all of your VMs to that network, instead of the default “Share with my Mac” (vmnet8) network. In this lab, you’ll need to disable DHCP on the network you are using, which is very difficult to do on the default networks. You can find relevant instructions in Add a NAT Configuration and Connect and Set Up the Network Adapter in the VMware Fusion 8 Documentation.\nTask 1: Remote Connections PART A: On your Windows 10 VM, activate the Remote Desktop feature to allow remote access.\nBoth the cis527 and AdminUser accounts should be able to access the system remotely, as well as the default system Administrator account. In addition, change the port used by Remote Desktop to be 34567. Tip You’ll need to edit the registry and reboot the computer to accomplish this task. –Russ\nYou’ll also need to make sure appropriate firewall rules are in place to accept these incoming connections, and ensure the firewall is properly enabled. You can test your connection from your Linux VM using the Remmina program. PART B: On your Ubuntu 22.04 VM labelled SERVER, install and activate the OpenSSH Server for remote access.\nBoth the cis527 and AdminUser accounts should be able to access the system remotely. In addition, change the port used by the SSH server to 23456. You’ll also need to make sure the appropriate firewall rules are in place to accept these incoming connections, and ensure the firewall is properly enabled. You can test your connection from your Windows VM using the ssh command in PowerShell, or from the Ubuntu 22.04 VM labelled CLIENT using the ssh command. Tip See the appropriate pages in the Extras module for more information about WSL and SSH. –Russ\nResources How to Set Up and Use Remote Desktop for Windows 10 from groovyPost How to Change the Listening Port for Remote Desktop from Microsoft Support Change Remote Desktop RDP Port from Tweaks.com OpenSSH Server from the Ubuntu Server Guide Configuring OpenSSH from Ubuntu Community Help Wiki Ubuntu 20.04 SSH Server from LinuxConfig.org Task 2: Ubuntu Static IP Address On your Ubuntu 22.04 VM labelled SERVER, set up a static IP address. The host part of the IP address should end in .41, and the network part should remain the same as the one automatically assigned by VMware.\nNote So, if your VMware is configured to give IP addresses in the 192.168.138.0/24 network, you’ll set the computer to use the 192.168.138.41 address.\nYou’ll need to set the following settings correctly:\nIP Address Subnet Mask Default Gateway Note VMware typically uses host 2 as its internal router to avoid conflicts with home routers, which are typically on host 1. So, on the 192.168.138.0/24 network, the default gateway would usually be 192.168.138.2. When in doubt, you may want to record these settings on one of your working VMs before changing them.\nDNS Servers. Use one of the following options: Your Default Gateway Address (easiest). VMware’s internal router also acts as a DNS resolver for you, just like a home router would Off Campus: OpenDNS (208.67.222.222 and 208.67.220.220) or Google DNS (8.8.8.8 and 8.8.4.4) On Campus: K-State’s DNS Servers (10.130.30.52 and 10.130.30.53) Tip I personally recommend using the graphical tools in Ubuntu to configure a static IP address. There are many resources online that direct you to use netplan or edit configuration files manually, but I’ve found that those methods aren’t as simple and many times lead to an unusable system if done incorrectly. In any case, making a snapshot before this step is recommended, in case you have issues. –Russ\nResources How to Configure Static IP Address on Ubuntu 20.04 Focal Fossa Desktop/Server from LinuxConfig.org How to Configure Static IP Address on Ubuntu 18.04 from LinOxide (should work for 20.04 as well) The Command Line Way: Network Configuration from Ubuntu Server Guide. Task 3: DNS Server For this step, install the bind9 package on the Ubuntu 22.04 VM labelled SERVER, and configure it to act as a primary master and caching nameserver for your network. You’ll need to include the configuration for both types of uses in your config file. In addition, you’ll need to configure both the zone file and reverse zone file, as well as forwarders.\nTip These instructions were built based on the How To Configure BIND as a Private Network DNS Server on Ubuntu 22.04 guide from DigitalOcean. In general, you can follow the first part of that guide to configure a Primary DNS Server, making the necessary substitutions listed below. –Russ\nIn your configuration, include the following items:\nAll files: Since you are not creating a Secondary DNS Server, you can leave out any allow-transfer entries from all configuration files. named.conf.options file: Create an ACL called cis527 that includes your entire VM network in CIDR notation. Do not list individual IP addresses. Enable recursion, and allow all computers in the cis527 ACL to perform recursive queries. Configure DNS forwarding, using one of the options given above in Task 2. I recommend using the same option as above, since you have (hopefully) already confirmed that it works for your situation. named.conf.local file: Create a zone file and reverse zone file, stored in /etc/bind/zones. Note The DigitalOcean guide uses a /16 subnet of 10.128.0.0/16, and includes the 10.128 portion in the reverse zone file name and configuration. For your VM network, you are most likely using a /24 subnet, such as 192.168.40.0/24, so you can include the 192.168.40 portion in your zone file name and configuration. In that case, the zone name would be 40.168.192.in-addr.arpa, and the file could be named accordingly. Similarly, in the reverse zone file itself, you would only need to include the last segment of the IP address for each PTR record, instead of the last two. Either way is correct.\nList those files by path in this file in the correct zone definitions. Zone files: Use \u003cyour eID\u003e.cis527.cs.ksu.edu as your fully qualified domain name (FQDN) in your configuration file. (Example: russfeld.cis527.cs.ksu.edu) Use ns.\u003cyour eID\u003e.cis527.cs.ksu.edu as the name of your authoritative nameserver. You can use admin.\u003cyour eID\u003e.cis527.cs.ksu.edu for the contact email address. Note Since the at symbol @ has other uses in the DNS Zone file, the email address uses a period . instead. So, the email address admin@\u003cyour eID\u003e.cis527.cs.ksu.edu would be written as admin.\u003cyour eID\u003e.cis527.cs.ksu.edu.\nDon’t forget to increment the serial field in the SOA record each time you edit the file. Otherwise your changes may not take effect. Create an NS record for ns.\u003cyour eID\u003e.cis527.cs.ksu.edu. Tip HINT: The DigitalOcean guide does not include an at symbol @ at the beginning of that record, but I’ve found that sometimes it is necessary to include it in order to make the named-checkzone command happy. See a related post on ServerFault for additional ways to solve that common error.–Russ\nForward Zone File: Create an A record for ns.\u003cyour eID\u003e.cis527.cs.ksu.edu that points to your Ubuntu 22.04 VM labelled SERVER using the IP address in your network ending in 41 as described above. Create an A record for ad.\u003cyour eID\u003e.cis527.cs.ksu.edu that points to the IP address in your network ending in 42. (You’ll use that IP address in the next assignment for your Windows server.) This record will be for the Active Directory server in Lab 4 Create a CNAME record for ubuntu.\u003cyour eID\u003e.cis527.cs.ksu.edu that redirects to ns.\u003cyour eID\u003e.cis527.cs.ksu.edu. Create a CNAME record for ldap.\u003cyour eID\u003e.cis527.cs.ksu.edu that redirects to ns.\u003cyour eID\u003e.cis527.cs.ksu.edu. Create a CNAME record for windows.\u003cyour eID\u003e.cis527.cs.ksu.edu that redirects to ad.\u003cyour eID\u003e.cis527.cs.ksu.edu. Reverse Zone File: Create a PTR record for the IP address ending in 41 that points to ns.\u003cyour eID\u003e.cis527.cs.ksu.edu. Create a PTR record for the IP address ending in 42 that points to ad.\u003cyour eID\u003e.cis527.cs.ksu.edu. Tip HINT: The periods, semicolons, and whitespace in the DNS configuration files are very important! Be very careful about formatting, including the trailing periods after full DNS names such as ad.\u003cyour eID\u003e.cis527.ksu.edu.. –Russ\nOnce you are done, I recommend checking your configuration using the named-checkconf and named-checkzone commands. Note that the second argument to the named-checkzone command is the full path to your zone file, so you may need to include the file path and not just the name of the file. Example: named-checkzone russfeld.cis527.cs.ksu.edu /etc/bind/zones/db.russfeld.cis527.cs.ksu.edu\nOf course, you may need to update your firewall configuration to allow incoming DNS requests to this system! If your firewall is disabled and/or not configured, there will be a deduction of up to 10% of the total points on this lab\nTo test your DNS server, you can set a static DNS address on either your Windows or Ubuntu VM labelled CLIENT, and use the dig or nslookup commands to verify that each DNS name and IP address is resolved properly.\nNote See the Bind Troubleshooting page for some helpful screenshots of using dig to debug DNS server configuration.\nWarning As of 2023, the DNS servers on campus do not seem to support DNSSEC, which may cause issues with forwarders. If you are connected to the campus network, I recommend changing the setting in named.conf.options to dnssec-validation no; to disable DNSSEC validation - that seems to resolve the issue.\nResources How To Configure BIND as a Private Network DNS Server on Ubuntu 22.04 from DigitalOcean Bind9 Server How-To from Ubuntu Community Help Wiki DNS Configuration from Ubuntu Server Guide BIND Manuals from bind9.net Reverse DNS/bind named-checkzone “zone NS has no address records (A or AAAA) error” on ServerFault (common error in previous semesters) Task 4: DHCP Server Warning IMPORTANT! Make ABSOLUTELY sure that the VMware virtual network you are using is not a “Bridged” or “Shared” network before continuing. It MUST be using “NAT”. You can check by going to Edit \u003e Virtual Network Editor in VMware Workstation or VMware Fusion \u003e Preferences \u003e Network in VMware Fusion and looking for the settings of the network each of your VMs is configured to use. Having your network configured incorrectly while performing this step is a great way to break the network your host computer is currently connected to, and in a worst case scenario will earn you a visit from K-State’s IT staff (and they won’t be happy)! –Russ\nNext, install the isc-dhcp-server package on the Ubuntu 22.04 VM labelled SERVER, and configure it to act as a DHCP server for your internal VM network.\nIn your configuration, include the following items:\nIn general, the network settings used by this DHCP server should match those used by VMware’s internal router. You can also look at the network settings received by your Windows 10 VM, which at this point are from VMware’s internal router. Use \u003cyour eID\u003e.cis527.cs.ksu.edu as the domain name. (Example: russfeld.cis527.cs.ksu.edu) For the dynamic IP range, use IPs ending in .100-.250 in your network. For DNS servers, enter the IP address of your Ubuntu 22.04 VM labelled SERVER ending in .41. This will direct all DHCP clients to use the DNS server configured in Task 3. Do not use the domain name of your DNS server in your DHCP config file. While it can work, it depends on your DNS server being properly configured in Task 3. Alternatively, for testing if your DNS server is not working properly, you can use one of the other DNS options given above in Task 2. However, you must be using the DNS server from Task 3 when graded for full credit. Tip A working solution can be fewer than 20 lines of actual settings (not including comments) in the settings file. If you find that your configuration is becoming much longer than that, you are probably making it too difficult and complex. –Russ\nOf course, you may need to update your firewall configuration to allow incoming DHCP requests to this system! If your firewall is disabled and/or not configured, there will be a deduction of up to 10% of the total points on this lab\nOnce your DHCP server is installed, configured, and running properly, turn off the DHCP server in VMware. Go to Edit \u003e Virtual Network Editor in VMware Workstation or VMware Fusion \u003e Preferences \u003e Network in VMware Fusion and look for the NAT network you are using. There should be an option to disable the DHCP server for that network there.\nOnce that is complete, you can test the DHCP server using the Windows VM. To do so, restart your Windows VM so it will completely forget any current DHCP settings. When it reboots, if everything works correctly, it should get an IP address and network information from your DHCP server configured in this step. It should also be able to access the internet with those settings. An easy way to check is to run the command ipconfig in PowerShell and look for the DNS suffix of \u003cyour eID\u003e.cis527.cs.ksu.edu in the output.\nResources Install and Configure ISC DHCP Server from Ubuntu Documentation Dynamic Host Configuration Protocol (DHCP) from Ubuntu Server Guide How to Install DHCP Server in Ubuntu \u0026 Debian from TecAdmin.net Task 5: SNMP Daemon Install an SNMP Daemon on the Ubuntu 22.04 VM labelled SERVER, and connect to it from your Ubuntu 22.04 VM labelled CLIENT. The DigitalOcean and Kifarunix tutorials linked below are a very good resource to follow for this part of the assignment. In that tutorial, the agent server will be your SERVER VM, and the manager server will be your CLIENT VM.\nIn the tutorial, configure a user cis527 using the password cis527_snmp for both the authentication and encryption passphrases. This user should not be created in the snmpd.conf file, and any “bootstrap” users should be removed. The DigitalOcean tutorial includes information for modifying the configuration file to make SNMP listen on all interfaces. The DigitalOcean method for creating users does not work. Use the method in the Kifarunix tutorial for configuring SNMP version 3 users. The DigitalOcean tutorial includes information for downloading the MIBS and configuring a ~/.snmp/snmp.conf file that can store your user information. Of course, you may need to update your firewall configuration to allow incoming SNMP requests to this system! If your firewall is disabled and/or not configured, there will be a deduction of up to 10% of the total points on this lab\nThen, perform the following quick activity:\nWhile logged into the CLIENT VM, use the SNMP tools to query the number of ICMP Echos (pings) that have been received by the SERVER VM. Take a screenshot with the command used and the result clearly highlighted in the terminal output. You may use either snmpget and the OID number or name, or use snmpwalk and grep to find the requested information. Sent at least 10 ICMP Echos (pings) from the CLIENT VM to the SERVER VM and make sure they were properly received. Take a screenshot of the output, clearly showing how many pings were sent. If they weren’t received, check your firewall settings. Once again, use the SNMP tools from the CLIENT VM to query the number of ICMP Echos (pings) that have been received by the SERVER VM. It should clearly show that it has increased by the number sent during the previous command. Take a screenshot with the command used and the result clearly highlighted in the terminal output. It should match the expected output based on the previous two screenshots. Note Be prepared to duplicate this activity during the interactive grading process! If you are unable to duplicate it, you can present the screenshots as proof that it worked before for partial credit. You may preform all three commands in a single screenshot if desired. See this example for an idea of what the output should look like. –Russ\nResources How to Install and Configure an SNMP Daemon and Client on Ubuntu 18.04 from DigitalOcean (works for 22.04 as well) Install and Configure SNMP on Ubuntu 22.04/Debian 11 from Kifarunix (includes correct command to create user accounts) How to Use The Net-SNMP Tool Suite to Manage and Monitor Servers from DigitalOcean (works for 22.04) SNMP Agent from Ubuntu Community Help Wiki Task 6: Wireshark Install Wireshark on the Ubuntu 22.04 VM labelled SERVER.\nWarning Firefox recently released an update the enables DNS over HTTPS by default. So, in order to use Firefox to request DNS packets that can be captured, you’ll need to disable DNS over HTTPS in Firefox. Alternatively, you can use dig to query DNS and capture the desired packets - this seems to be much easier to replicate easily.\nThen, using Wireshark, create screenshots showing that you captured and can show the packet content of each of the following types of packets:\nA DNS standard query for an A record for people.cs.ksu.edu A DNS standard query response for people.cs.ksu.edu HINT: It should respond with a CNAME record pointing to invicta.cs.ksu.edu A DNS standard query response for a PTR record for 208.67.222.222 (it will look like 222.222.67.208.in-addr.arpa) HINT: It should respond with a PTR record for resolver1.opendns.com An ICMP Echo (ping) request An encrypted SNMP packet showing cis527 or bootstrap as the username (look for the msgUserName field) HINT: Use the commands from Task 5 A DHCP Offer packet showing the Domain Name of \u003cyour ID\u003e.cis527.cs.ksu.edu HINT: Reboot one of your other VMs to force it to request a new IP address, or use the ipconfig (Windows) or dhclient (Ubuntu) commands to renew the IP address An HTTP 301: Moved Permanently or HTTP 302: Found redirect response HINT: Clear the cache in your web browser, then navigate to http://people.cs.ksu.edu/~sgsax (without a trailing slash). It should redirect to http://people.cs.ksu.edu/~sgsax/ (with a trailing slash). An HTTP Basic Authentication request, clearly showing the username and password in plaintext (expand the entries in the middle pane to find it). HINT: Visit http://httpbin.org/basic-auth/testuser/testpass and use testuser | testpass to log in Tip You’ll present those 8 screenshots as part of the grading process for this lab, so I recommend storing them on the desktop of that VM so they are easy to find. Make sure your screenshot clearly shows the data requested. –Russ\nResources Install and Use Wireshark on Ubuntu Linux from It’s FOSS Task 7: Make Snapshots In each of the virtual machines created above, create a snapshot labelled “Lab 3 Submit” before you submit the assignment. The grading process may require making changes to the VMs, so this gives you a restore point before grading starts.\nTask 8: Schedule A Grading Time Contact the instructor and schedule a time for interactive grading. You may continue with the next module once grading has been completed.\n",
    "description": "",
    "tags": null,
    "title": "Assignment",
    "uri": "/3-core-networking-services/05-assignment/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Core Networking Services - DNS Student Developer Pack from GitHub Education How To Install the Apache Web Server on Ubuntu 22.04 from DigitalOcean DNS Overview from DigitalOcean How to Point to DigitalOcean Nameservers from Common Domain Registrars from DigitalOcean Video Transcript Now that you’ve set up and configured your first cloud server, let’s discuss how to make that resource easily accessible to you and your organization.\nIn Module 3, we discussed the Domain Name System, or DNS, as the “phonebook of the internet.” It contains a list of domain names and the associated IP addresses for each one. We already have the IP address of our cloud server, now we must add our own entry to the domain name space as well.\nWhen you register for a domain, you’ll typically be registering for a second-level domain name, which is one step below a top-level domain, or TLD, such as .com, .org, and .net, among others. In this example, wikipedia is the second-level domain name below the .org TLD.\nTo see it a bit more clearly, here is a diagram showing the breakdown of a common web Uniform Resource Locator, or URL. A URL is written with the top-level domain at the end, and then working backwards to forwards you can see the full hierarchy of the address in the domain name space. At the front is the protocol used to access that resource. In our case, we’ll be registering a domain name, and then creating a few subdomains for our cloud resources.\nFor this lab assignment, you’ll be asked to register a domain name. I’ll be using Namecheap for my example website, since it is the domain registrar I’ve been using for my personal sites for some time. There are many other registrars out there on the internet, and each one offers different services and prices. You are welcome to use any registrar you like for this lab. Once again, the GitHub Student Developer Pack is an excellent resource, and one of the discounts offered is a free .me domain through Namecheap for one year. Honestly, if you don’t already have a personal domain name, now is a great time to register one!\nWhen registering a domain name, the most important thing to think about, first of all, is the name itself. Most domain registrars offer a search feature to see which domains are available and what the price is. You’ll also notice that the same domain name may be offered for wildly different prices under different TLDs. So, you’ll have to consider your choice of name and TLD carefully.\nIn addition, you can register the same second-level domain name under a variety of top-level domains, though each one comes at an addition cost, both in terms of price and management. Many large enterprises choose to do this to prevent “cybersquatting,” where another user registers a similar domain name, hoping to profit on it in some way. For example, Google owns many different domain names related to google.com, including gogle.com, goolge.com, and googlr.com in order to make sure that users who type the domain name slightly incorrectly still reach the correct website.\nOnce you’ve purchased your domain name, you’ll have a choice of how the DNS configuration is hosted. For many users, they are only planning on using the domain name with a particular website hosting service, such as WordPress or GitHub Pages. In that case, you can set up your domain to point to your host’s nameservers, and they’ll manage everything for you.\nHowever, you may want to use this domain name for a variety of uses. In that case, I highly recommend managing your DNS settings yourself. You can choose to use your registrar’s built-in DNS hosting, which is what I’ll do in this example. Many cloud providers, such as DigitalOcean, also offer a hosted DNS service that you can use for this feature. In general, I’ve always kept my domain names and cloud hosting separate, just to add a bit more resiliency in case one provider or the other has a problem.\nFinally, whenever you register a domain name, you are legally required to provide contact information, including your name, mailing address, and phone number, for the Whois service, a public service that provides information about all registered domains. In many cases, that would be your own personal information, which would be posted publicly along with the domain name by your registrar. For many individuals, that could create a major privacy and identity theft concern.\nThankfully, many domain registrars offer a privacy service that will replace your contact information with their own in the Whois database. In essence, they post their information publicly on your behalf, and then they will send any official communication to you directly. It helps prevent your personal information from being publicly available. In the case of Namecheap, their WhoisGuard service does just that, and as of this writing it is available free of charge for any users who register a domain name through their service. I highly recommend using one of these services to protect your private information when you register a domain name for your own personal use.\nLet’s take a look at how to register and configure a domain name. As I mentioned, I’ll be using Namecheap for this example, but there are many other registrars out there that you can use. First, I’ll use their domain name search tool to see if a particular domain name is available. Let’s search for cis527 and see what’s out there.\nAs you can see, that domain name is available on a variety of TLDs, and at a variety of prices as well, ranging from less than $1 to more than $60 per year. The price can fluctuate wildly depending on the demand and popularity of a particular TLD. Once you choose your domain name and TLD, you can work through your registrar to register and pay for the domain. You’ll also be asked to provide them with the appropriate information for the Whois service.\nOnce you’ve registered your domain name, you can usually configure it through your registrar’s website. Most importantly, you’ll be able to define where the nameservers for your domain are located. As we discussed above, I recommend using your registrar’s nameservers or the nameservers of your cloud provider so you can manage the DNS settings yourself. If you are using a particular website hosting service, they may have instructions for configuring this section to point to their nameservers.\nSince I’m using Namecheap’s DNS service, I can view the DNS settings right here as well. Looking at these settings, you’ll see that I have several A records already configured for this domain name. Each of these A records could point to a different cloud resource or server that I manage. However, you’ll notice that they all point to the same IP address. So, what’s going on here?\nOn my cloud server, I’m using the Apache web server. You’ll be using the same software on your own servers in the lab assignment. Thankfully, Apache, as well as most other web servers, has a unique feature that allows you to host multiple websites on the same IP address. This allows you make much more efficient use of your resources. Instead of having one droplet per website you manage, you can host many websites on the same droplet, and they can all share the same computing resource and bandwidth. If you are only hosting very small websites that don’t get much traffic, this is a great option.\nIn Apache, you can do this by configuring “virtual hosts” on the server. A virtual host defines a domain name and matches it with a folder storing the website’s files. When an incoming request comes to the server, it analyzes the domain name requested in the packet, and then finds the appropriate website to display. Because of this, it is very important to have your DNS settings set correctly for your domain. If you try to access this server by its IP address alone, you’ll generally only be able to see one of the websites it has available.\nTo configure your system to use virtual hosts, there are a couple of steps. First, in your DNS configuration, you’ll need to add new A records for each website you’d like to host. For this example, I’ll just use the names foo and bar. In each A record, I’ll set the IP address to be the public address of my web server.\nOnce I save my changes, that record will be updated in my registrar’s DNS servers. However, due to the large amount of caching and redundancy in the worldwide DNS network, it could take up to 24 hours for the changes to fully propagate. In general, you can avoid some of those issues by restarting your system to clear the DNS cache, using 3rd party DNS servers such as OpenDNS or Google DNS, and not querying this DNS entry right away so that the DNS servers don’t cache an invalid entry. However, in many cases you’ll simply have to wait until it starts working before you can continue, and there isn’t a whole lot you can do to make it go faster.\nNext, on my DigitalOcean droplet, I’ll install Apache if I haven’t already:\nsudo apt update sudo apt install apache2 and then create a folder to store the first website:\nsudo mkdir -p /var/www/foo/html To make it really simple to see which website is which, I’ll simply place a file that folder, giving the name of the site:\nsudo nano /var/www/foo/html/index.html Now, I’ll need to configure a virtual host file for that website:\nsudo nano /etc/apache2/sites-available/foo.russfeld.me.conf The /etc/apache2/sites-available/ directory stores all of the available site configuration files for Apache by convention. Of course, you’ll need to update the filename to match your domain name. In that file, I’ll place the following information, which I’ve adapted from the DigitalOcean guide on installing Apache linked in the resources section below this video:\n\u003cVirtualHost foo.russfeld.me:80\u003e ServerAdmin admin@russfeld.me ServerName foo.russfeld.me DocumentRoot /var/www/foo/html ErrorLog ${APACHE_LOG_DIR}/error.log CustomLog ${APACHE_LOG_DIR}/access.log combined \u003c/VirtualHost\u003e Then, I’ll save and close the file. Finally, I’ll need to deactivate the default website, and activate the new site:\nsudo a2dissite 000-default sudo a2ensite foo.russfeld.me Once I’ve activated it, I can check for any configuration errors:\nsudo apache2ctl configtest If it passes, I can restart the Apache service to reload my changes:\nsudo systemctl restart apache2 I’ll do these same steps for the other site, named bar, as well. The DigitalOcean guide for installing Apache includes much more in-depth information about this process, so I encourage you to read it carefully to learn even more about how to configure and work with Apache.\nOnce I’ve done that, I can test my virtual host configuration and see if it works. To do this, I’ll open a web browser and navigate to http://foo.russfeld.me and see if it takes me to the correct website. I can also try http://bar.russfeld.me to test the other website. As you can see, they both work correctly.\nThis is just a brief introduction to setting up your own domain name and configuring your cloud resources to use that domain. In the next few lab assignments, you’ll be configuring several virtual hosts similar to these. If you run into issues getting it to work, I encourage you to post a question in the course discussion forums.\n",
    "description": "",
    "tags": null,
    "title": "Domains \u0026 Virtual Hosts",
    "uri": "/5-the-cloud/05-domains-virtual-hosts/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello and welcome to the week nine announcements video for CIS 527 and CC 510 in fall 2023. This week Lab 4 is due tomorrow, so make sure you’re getting Lab 4 all wrapped up. Then you should also be starting on the content for Lab 5, which is some introduction to the cloud. And then a couple weeks from now you’ll have the Lab 5 content that’s actually due, so you should get started on Lab 5 sometime next week. So for grading on Lab 4, basically we want to see a few simple things. We want to see your Windows Server up and running with an Active Directory installed and a static IP address. We want to see that you’ve created a user and group and Active Directory, and we want to show that you’ve connected a Windows client to that Active Directory. Then we’ll shift over to Linux. We want to see that you’ve installed Open LDAP and you’ve created a user and group using LDAP Account Manager. I posted an announcements video yesterday with a video showing you how to install LDAP Account Manager, so hopefully that helps. I apologize for the bad audio quality on that video. It was recorded on my laptop, which doesn’t have a good microphone. I will re -record a more professional version of that video in the future. And then we want to see you have an Ubuntu client that logs into LDAP using the LDAP client. We also want to have an Ubuntu that logs into your Active Directory. Those could be two different VMs, two different snapshots of the same VM. It’s up to you how you want to handle that. But that’s really all we want to see with Lab 4. If everything’s working, it takes very little time to show us that it’s working. If you have trouble, feel free to work with Matt or I, and we’d be happy to help you debug it. We’re slowly getting better with debugging problems with Lab 4.\nSo some quick reminders to be successful in this class. Make sure you read the assignments very carefully. Take a look at any posted diagrams or resources that I have. A lot of those resources have good steps, tutorials. Generally, I try and post them in the order of usefulness. So typically the top one that I post is the one that you’re most interested in on the assignments. But if you’re stuck, don’t be afraid to ask questions. Don’t be afraid to come to office hours. The big thing is don’t spin your wheels. Don’t feel like you just keep working in a rut. If you’ve been working on something, just keep working on it. for half hour and can’t figure it out. Come talk to us, let us know. We’ve got some pretty good background in debugging and can do a lot to help you figure out what’s going on. So feel free to reach out.\nSo this week you should be moving on to Lab 5. Lab 5, we’re going to shift over to doing some stuff in the cloud for Lab. We won’t use the VMs for this lab, but you’ll still need them for Lab 6 and 7, so hold onto those. If you’re running out of storage space, let me know or let Matt know. We’ve got some tips we can give you to help resolve some of that. Basically what you’re going to do is set up two droplets on DigitalOcean’s cloud infrastructure. If you have experience with AWS and Azure, you’re welcome to work there. I don’t have a lot of experience with that or debugging anything there, so I really recommend DigitalOcean if possible, but you’re welcome to do something else. Just understand if it breaks, I may not be able to help you fix it and I may ask you to move to DigitalOcean. You’re going to set up an SSH in firewall, you’re going to configure the Apache web server and some simple websites on Apache. You’ll set up a working DNS name, and then we’re going to do a little bit with… Docker and you’ll set up a Docker reverse proxy to two different Docker containers just to get a little bit of experience working with Docker in lab five So one thing you can do is if you don’t have any access to digital ocean yet You can either register for the github education pack at this URL You can also go to try that digital ocean comm slash free trial offer. I apologize that got cut off Usually digital ocean has an offer where you can get anywhere from 100 to 200 dollars in free credit with a new account Same thing for name cheap dot me you can get a dot me domain for 99 cents If you need credits on digital ocean, let me know I’ve got referral credits. I can give you as well Overall for this class the total cost you should spend on this is about 11 dollars. It’s pretty cheap I think it’s it’s reasonable to do this on your own But most of these you can get for free or for 99 cents So hopefully it works should work out but at most you should pay no more than 11 dollars to complete some of this stuff\nSo finally a quick reminder, I’ve got some upcoming travel this week I’ll be out of the office Thursday through Sunday of this week because that responses be email and grading may be a bit delayed Matt is still available so you can reach out to Matt anytime if you have questions Please make sure you use the CIS 527 help email address that’s on the canvas page or post an ed discussion Those go to both Matt and I so we can help you out there Finally, of course, there’s opportunities to keep in touch. We have a discussion on discord. We have ed discussion boards We have time for one -on -one office hours Matt should be at the Thursday office hour time as well if you’d like to meet within there I’m working on scheduling the next discussions. Unfortunately. I’ve had to reschedule a couple of things So I’ll be working on that soon We’ll try and have at least two more discussions this semester My goal is to have three more but we’ll see if we can fit that all into the schedule But those will be getting posted very shortly as well So we’re getting up to the cloud It always reminds you of this great XKCD comic where the cloud is just somebody else’s computer I think this really describes it quite well. So if you haven’t seen this comic take a look at this XKCD comic I hope everything goes well with lab 5 if you have any questions, let us know and I will talk to you again in a couple weeks Good luck\n",
    "description": "",
    "tags": null,
    "title": "Fall '23 Week 9",
    "uri": "/y-announcements/week09/index.html"
  },
  {
    "content": "CIS 527 - Enterprise Systems Administration CC 510 - Computer Systems Administration Previous Versions This syllabus covers both courses. They are taught using the same content.\nInstructor Contact Information Instructor: Russell Feldhausen (russfeld AT ksu DOT edu)\nI use he/him pronouns. Feel free to share your own pronouns with me, and I’ll do my best to use them! Office: DUE 2213, but I mostly work remotely from Kansas City, MO Phone: (785) 292-3121 (Call/Text) Website: https://russfeld.me Virtual Office Hours: By appointment via Zoom . Schedule a meeting at https://calendly.com/russfeld Teaching Assistant Matt Schwartz (matt00 AT ksu DOT edu) - Office Hours by appointment via Calendly Preferred Methods of Communication: Email: Email is the official method of communication for this course. Any emails sent to the instructor regarding this course should be answered within one class day. Please use the official course email address: cis527-help@ksuemailprod.onmicrosoft.com Ed Discussion: For short questions and discussions of course content and assignments, Ed Discussion is preferred since questions can be asked once and answered for all students. Students are encouraged to post questions there and use that space for discussion, and the instructor will strive to answer questions there as well. Phone/Text: Emergencies only! I will do my best to respond as quickly as I can. Prerequisites CIS 527: CIS 300. CC 510: CC 310 or CIS 300. Students may enroll in CIS or CC courses only if they have earned a grade of C or better for each prerequisite to those courses.\nCourse Description Computer information systems form the backbone of many large organizations, and many students will be called upon in their careers to help create, manage and maintain these large systems. This course will give students knowledge and experience working with enterprise level computer systems including workstation management, file servers, web servers, networking devices, configuration management, monitoring, and more. We will mainly focus on the GNU/Linux and Microsoft Windows server software, and much of the learning will take place in hands-on lab activities working directly with these systems. In addition, students will be responsible for developing some technical documentation and communicating information about their systems in a variety of ways. Finally, throughout the course students will be exposed to a variety of information directly from system administrators across campus.\nStudent Learning Outcomes After completing this course, a successful student will be able to:\nUnderstand the major components of an enterprise level computer network and server system Design and implement a simple enterprise level server system and network, as well as provision workstations on that network quickly and easily Communicate information about enterprise systems clearly and effectively to users of all skill levels and interests Develop ways to increase efficiency by automating tasks whenever possible using scripting and configuration management tools Understand and describe security risks in any enterprise system and any ways that they can be mitigated Show how to monitor enterprise systems for problems and use that information to locate and fix any issues within the system Work with cloud technologies and describe how they can be integrated into an enterprise information technology setup Major Course Topics Configuration Management using Puppet Creating Secure Workstations (CIS 225 overview/review) Setting up an Enterprise Directory Service \u0026 Single Sign On Enterprise File Sharing Web \u0026 Application Servers Core Networking Services (DHCP, DNS, ICMP, etc.) System Monitoring \u0026 Maintenance Backup Strategies The Cloud \u0026 DevOps Course Structure This course is being taught 100% online and mostly asynchronous. There may be some bumps in the road. Students will work at their own pace through several modules, with due dates for completion of each module given. Material will be provided in the form of recorded videos, links to online resources, and discussion prompts. Each module will include a hands-on lab assignment, which will be graded interactively by the instructor or TAs. Assignments may also include written portions or presentations, which will be submitted online.\nThe course will also include a final project and presentation. More information about this can be found in the final project module on Canvas.\nThe Work There is no shortcut to becoming a great programmer or system administrator. Only by doing the work will you develop the skills and knowledge to make you a successful system administrator. This course is built around that principle, and gives you ample opportunity to do the work, with as much support as we can offer.\nLectures \u0026 Quizzes: Each module will include many lectures and quizzes. The goal is to introduce you to a new topic and provide ample background information, then check for your understanding of the core concepts through the quiz. Many lectures include links to additional resources that you are welcome to review if you want to dig deeper into a particular topic. Those additional resources may also be useful when completing the lab assignments.\nLab Assignments: Throughout the semester you will be building a non-trivial system architecture iteratively; every week a new lab assignment will be due. Each lab builds upon the prior lab’s infrastructure, so it is critical that you complete each lab in a timely manner! This process also reflects the way system administration is done in the real world - breaking large projects into more readily achievable milestones helps manage the development process.\nFollowing along that real-world theme, labs will mostly be graded on whether they achieve the goals as described in the lab assignment. You can think of each lab assignment as a directive given to you by your supervisor - if you meet those requirements, you are successful; however, if your system fails to meet those requirements, then it is not useful at all, even if it is partially complete. In practice, you may earn some partial credit for attempting a portion of a lab, but the majority of points will require full functionality.\nFinal Project: At the end of this course, you will design and evaluate a final project of your choosing to demonstrate your ability. This project can link back to your interest or other fields, and will serve as a capstone project for this course.\nGrading In theory, each student begins the course with an A. As you submit work, you can either maintain your A (for good work) or chip away at it (for less adequate or incomplete work). In practice, each student starts with 0 points in the gradebook and works upward toward a final point total earned out of the possible number of points. In this course, each assignment constitutes a portion of the final grade, as detailed below:\n70% - Lab Assignments* (7 labs, 10% each lab) 10% - Quizzes (15 quizzes, 0.66% each) 10% - Discussions (5 discussions, 2% each) 10% - Final Project All group work will include a REQUIRED peer evaluation component which can adjust that portion of the individual’s grade up to 50%. If a student should fail to contribute to a group assignment at all, their grade for that assignment will be reduced to a zero. Failure to complete the peer evaluation will result in a 10% grade deduction for that assignment.\nLetter grades will be assigned following the standard scale:\n90% - 100% → A 80% - 89.99% → B 70% - 79.99% → C 60% - 69.99% → D 00% - 59.99% → F Submission, Regrading, and Early Grading Policy As a rule, submissions in this course will not be graded until after they are due, even if submitted early. Students may resubmit assignments many times before the due date, and only the latest submission will be graded. For assignments submitted via GitHub release tag, only the tagged release that was submitted to Canvas will be graded, even if additional commits have been made. Students must create a new tagged release and resubmit that tag to have it graded for that assignment.\nOnce an assignment is graded, students are not allowed to resubmit the assignment for regrading or additional credit without special permission from the instructor to do so. In essence, students are expected to ensure their work is complete and meets the requirements before submission, not after feedback is given by the instructor during grading. However, students should use that feedback to improve future assignments and milestones.\nFor the project milestones, it is solely at the discretion of the instructor whether issues noted in the feedback for a milestone will result in grade deductions in a later milestones if they remain unresolved, though the instructor will strive to give students ample time to resolve issues before any additional grade deductions are made.\nLikewise, students may ask questions of the instructor while working on the assignment and receive help, but the instructor will not perform a full code review nor give grading-level feedback until after the assignment is submitted and the due date has passed. Again, students are expected to be able to make their own judgments on the quality and completion of an assignment before submission.\nThat said, a student may email the instructor to request early grading on an assignment before the due date, in order to move ahead more quickly. The instructor’s receipt of that email will effectively mean that the assignment for that student is due immediately, and all limitations above will apply as if the assignment’s due date has now passed.\nCollaboration Policy In this course, all work submitted by a student should be created solely by the student without any outside assistance beyond the instructor and TA/GTAs. Students may seek outside help or tutoring regarding concepts presented in the course, but should not share or receive any answers, source code, program structure, or any other materials related to the course. Learning to debug problems is a vital skill, and students should strive to ask good questions and perform their own research instead of just sharing broken source code when asking for assistance.\nThat said, the field of system administration requires the use of lots of online documentation and reference materials, and the point of the class is to learn how to effectively use those resources instead of “reinventing the wheel from scratch” in each assignment. Whenever content in an assignment is taken from an outside source, this should be noted somewhere in the assignment.\nLate Work Warning While my original intent was to have this course completely asynchronous and self-paced, I’ve found that students prefer having more strict deadlines than more flexibility, and many times they will perform better in the course when deadlines are enforced. Therefore, deadlines will be strictly enforced this semester. Read this late work policy very carefully! If you are unsure how to interpret it, please contact the instructors via email. Not understanding the policy does not mean that it won’t apply to you!\nDue to the asynchronous nature of this course, staying on task and keeping up with deadlines is very important. Therefore, all course work must be submitted, and all interactively graded materials must be graded with the instructor or TA, on or before the posted due date to receive full credit. For labs, it is not simply enough to contact the instructor/TA asking to schedule a grading time before the due date; the grading itself must be completed before the due date in order to be considered “on time”.\nAny work submitted and graded after the due date is subject to a deduction of 10% of the total points possible on the assignment for each class day that the assignment is late. For example, if an assignment is due on a Friday and is submitted the following Tuesday, it will be subject to a reduction of 20% of the total points possible, or 10% for each class day it was late (Monday and Tuesday in this example). Grading done on non-class days will be considered to have been submitted on the next available class day. Deductions for non-class days will still be automatically entered by Canvas - contact the instructor to have these deductions removed.\nThese deductions will only be applied to grades above 50% of the total points on the assignment. So, if you scored higher than 50%, your grade will be reduced by the late penalty down to a minimum grade of 50%. If you scored lower than 50% on the assignment, no deductions will be applied.\nAlso, note that several labs in this class require successful completion of previous labs. If you are behind and choose to skip a lab assignment to catch up, you may still have to make up some or all of that work in order to complete a later lab. You may contact the instructor to discuss options for obtaining model solutions to previous labs if needed.\nAll course work must be submitted, and all interactively graded materials must be graded with the instructor, on or before the last day of the semester in which the student is enrolled in the course in order for it to be graded on time. No late work will be accepted after that date.\nIf you have extenuating circumstances, please discuss them with the instructor as soon as they arise so other arrangements can be made. If you know you have upcoming events that will prevent you from completing work in this course, you should contact the instructor ASAP and plan on working ahead before your event instead of catching up afterwards. If you find that you are getting behind in the class, you are encouraged to speak to the instructor for options to catch up quickly.\nIncomplete Policy Students should strive to complete this course in its entirety before the end of the semester in which they are enrolled. However, since retaking the course would be costly and repetitive for students, we would like to give students a chance to succeed with a little help rather than immediately fail students who are struggling.\nIf you are unable to complete the course in a timely manner, please contact the instructor to discuss an incomplete grade. Incomplete grades are given solely at the instructor’s discretion. See the official K-State Grading Policy for more information. In general, poor time management alone is not a sufficient reason for an incomplete grade.\nUnless otherwise noted in writing on a signed Incomplete Agreement Form , the following stipulations apply to any incomplete grades given in this course:\nStudents will be given 6 calendar weeks from the end of the enrolled semester’s finals week to complete the course Students understand that access to instructor and TA assistance may be limited after the end of an academic semester due to holidays and other obligations If a student fails to resolve an incomplete grade after 6 weeks, they will be assigned an ‘F’ in the course. In addition, they will be dropped from any other courses which require the failed course as a prerequisite or corequisite. For CC courses only: Students may receive at most two incompletes in Computational Core courses throughout their time in the program. Any modules in a future CC course which depend on incomplete work will not be accessible until the previous course is finished For example, if a student is given an incomplete in CC 210, then all modules in CC 310 will be inaccessible until CC 210 is complete Recommended Texts \u0026 Supplies To participate in this course, students must have access to a modern web browser and broadband internet connection. All course materials will be provided via Canvas. Modules may also contain links to external resources for additional information, such as programming language documentation.\nThe online textbook for this course can be found at https://textbooks.cs.ksu.edu/cis527/ . All relevant pages from the textbook are also embedded into the appropriate Canvas modules.\nStudents in this course are expected to have access to a computer with virtual machine software (VMware, Virtual Box, Parallels, or other) installed and running. The computer should be capable of running multiple VMs simultaneously, which usually means having 8GB of RAM and a moderately powerful processor. Contact the instructor if you have questions or concerns.\nAll K-State Computer Science students have access to free software from Microsoft and VMWare. More information can be found on the K-State CS Support Website .\nSince this class covers such a wide range of material, no single textbook will suffice. Therefore, students who would like a textbook should refer to resources available through the K-State Library and other online resources. The O’Riley For Higher Education digital library contains an entire catalog of books published on that platform, and it is a great resource for this course.\nWe will also use several online resources as needed.\nThis book contains useful information for anyone thinking about pursuing a career in system administration or information technology in general:\n“The Practice of System and Network Administration” by Thomas Limoncelli, Christina Hogan and Strata Chalup.\nISBN 0321492668 - eBook Editions Available - Amazon Link Subject to Change The details in this syllabus are not set in stone. Due to the flexible nature of this class, adjustments may need to be made as the semester progresses, though they will be kept to a minimum. If any changes occur, the changes will be posted on the K-State Canvas page for this course and emailed to all students.\nStandard Syllabus Statements Info The statements below are standard syllabus statements from K-State and our program. The latest versions are available online here .\nAcademic Honesty Kansas State University has an Honor and Integrity System based on personal integrity, which is presumed to be sufficient assurance that, in academic matters, one’s work is performed honestly and without unauthorized assistance. Undergraduate and graduate students, by registration, acknowledge the jurisdiction of the Honor and Integrity System. The policies and procedures of the Honor and Integrity System apply to all full and part-time students enrolled in undergraduate and graduate courses on-campus, off-campus, and via distance learning. A component vital to the Honor and Integrity System is the inclusion of the Honor Pledge which applies to all assignments, examinations, or other course work undertaken by students. The Honor Pledge is implied, whether or not it is stated: “On my honor, as a student, I have neither given nor received unauthorized aid on this academic work.” A grade of XF can result from a breach of academic honesty. The F indicates failure in the course; the X indicates the reason is an Honor Pledge violation.\nFor this course, a violation of the Honor Pledge will result in sanctions such as a 0 on the assignment or an XF in the course, depending on severity. Actively seeking unauthorized aid, such as posting lab assignments on sites such as Chegg or StackOverflow, or asking another person to complete your work, even if unsuccessful, will result in an immediate XF in the course.\nThis course assumes that all your course work will be done by you. Use of AI text and code generators such as ChatGPT and GitHub Copilot in any submission for this course is strictly forbidden unless explicitly allowed by your instructor. Any unauthorized use of these tools without proper attribution is a violation of the K-State Honor Pledge .\nWe reserve the right to use various platforms that can perform automatic plagiarism detection by tracking changes made to files and comparing submitted projects against other students’ submissions and known solutions. That information may be used to determine if plagiarism has taken place.\nStudents with Disabilities At K-State it is important that every student has access to course content and the means to demonstrate course mastery. Students with disabilities may benefit from services including accommodations provided by the Student Access Center. Disabilities can include physical, learning, executive functions, and mental health. You may register at the Student Access Center or to learn more contact:\nManhattan/Olathe/Global Campus – Student Access Center accesscenter@k-state.edu 785-532-6441 K-State Salina Campus – Julie Rowe; Student Success Coordinator jarowe@k-state.edu 785-820-7908 Students already registered with the Student Access Center please request your Letters of Accommodation early in the semester to provide adequate time to arrange your approved academic accommodations. Once SAC approves your Letter of Accommodation it will be e-mailed to you, and your instructor(s) for this course. Please follow up with your instructor to discuss how best to implement the approved accommodations.\nExpectations for Conduct All student activities in the University, including this course, are governed by the Student Judicial Conduct Code as outlined in the Student Governing Association By Laws , Article V, Section 3, number 2. Students who engage in behavior that disrupts the learning environment may be asked to leave the class.\nMutual Respect and Inclusion in K-State Teaching \u0026 Learning Spaces At K-State, faculty and staff are committed to creating and maintaining an inclusive and supportive learning environment for students from diverse backgrounds and perspectives. K-State courses, labs, and other virtual and physical learning spaces promote equitable opportunity to learn, participate, contribute, and succeed, regardless of age, race, color, ethnicity, nationality, genetic information, ancestry, disability, socioeconomic status, military or veteran status, immigration status, Indigenous identity, gender identity, gender expression, sexuality, religion, culture, as well as other social identities.\nFaculty and staff are committed to promoting equity and believe the success of an inclusive learning environment relies on the participation, support, and understanding of all students. Students are encouraged to share their views and lived experiences as they relate to the course or their course experience, while recognizing they are doing so in a learning environment in which all are expected to engage with respect to honor the rights, safety, and dignity of others in keeping with the K-State Principles of Community .\nIf you feel uncomfortable because of comments or behavior encountered in this class, you may bring it to the attention of your instructor, advisors, and/or mentors. If you have questions about how to proceed with a confidential process to resolve concerns, please contact the Student Ombudsperson Office . Violations of the student code of conduct can be reported using the Code of Conduct Reporting Form . You can also report discrimination, harassment or sexual harassment , if needed.\nNetiquette Info This is our personal policy and not a required syllabus statement from K-State. It has been adapted from this statement from K-State Global Campus, and theRecurse Center Manual . We have adapted their ideas to fit this course.\nOnline communication is inherently different than in-person communication. When speaking in person, many times we can take advantage of the context and body language of the person speaking to better understand what the speaker means, not just what is said. This information is not present when communicating online, so we must be much more careful about what we say and how we say it in order to get our meaning across.\nHere are a few general rules to help us all communicate online in this course, especially while using tools such as Canvas or Discord:\nUse a clear and meaningful subject line to announce your topic. Subject lines such as “Question” or “Problem” are not helpful. Subjects such as “Logic Question in Project 5, Part 1 in Java” or “Unexpected Exception when Opening Text File in Python” give plenty of information about your topic. Use only one topic per message. If you have multiple topics, post multiple messages so each one can be discussed independently. Be thorough, concise, and to the point. Ideally, each message should be a page or less. Include exact error messages, code snippets, or screenshots, as well as any previous steps taken to fix the problem. It is much easier to solve a problem when the exact error message or screenshot is provided. If we know what you’ve tried so far, we can get to the root cause of the issue more quickly. Consider carefully what you write before you post it. Once a message is posted, it becomes part of the permanent record of the course and can easily be found by others. If you are lost, don’t know an answer, or don’t understand something, speak up! Email and Canvas both allow you to send a message privately to the instructors, so other students won’t see that you asked a question. Don’t be afraid to ask questions anytime, as you can choose to do so without any fear of being identified by your fellow students. Class discussions are confidential. Do not share information from the course with anyone outside of the course without explicit permission. Do not quote entire message chains; only include the relevant parts. When replying to a previous message, only quote the relevant lines in your response. Do not use all caps. It makes it look like you are shouting. Use appropriate text markup (bold, italics, etc.) to highlight a point if needed. No feigning surprise. If someone asks a question, saying things like “I can’t believe you don’t know that!” are not helpful, and only serve to make that person feel bad. No “well-actually’s.” If someone makes a statement that is not entirely correct, resist the urge to offer a “well, actually…” correction, especially if it is not relevant to the discussion. If you can help solve their problem, feel free to provide correct information, but don’t post a correction just for the sake of being correct. Do not correct someone’s grammar or spelling. Again, it is not helpful, and only serves to make that person feel bad. If there is a genuine mistake that may affect the meaning of the post, please contact the person privately or let the instructors know privately so it can be resolved. Avoid subtle -isms and microaggressions. Avoid comments that could make others feel uncomfortable based on their personal identity. See the syllabus section on Diversity and Inclusion above for more information on this topic. If a comment makes you uncomfortable, please contact the instructor. Avoid sarcasm, flaming, advertisements, lingo, trolling, doxxing, and other bad online habits. They have no place in an academic environment. Tasteful humor is fine, but sarcasm can be misunderstood. As a participant in course discussions, you should also strive to honor the diversity of your classmates by adhering to the K-State Principles of Community .\nDiscrimination, Harassment, and Sexual Harassment Kansas State University is committed to maintaining academic, housing, and work environments that are free of discrimination, harassment, and sexual harassment. Instructors support the University’s commitment by creating a safe learning environment during this course, free of conduct that would interfere with your academic opportunities. Instructors also have a duty to report any behavior they become aware of that potentially violates the University’s policy prohibiting discrimination, harassment, and sexual harassment, as outlined by PPM 3010 .\nIf a student is subjected to discrimination, harassment, or sexual harassment, they are encouraged to make a non-confidential report to the University’s Office for Institutional Equity (OIE) using the online reporting form . Incident disclosure is not required to receive resources at K-State. Reports that include domestic and dating violence, sexual assault, or stalking, should be considered for reporting by the complainant to the Kansas State University Police Department or the Riley County Police Department . Reports made to law enforcement are separate from reports made to OIE. A complainant can choose to report to one or both entities. Confidential support and advocacy can be found with the K-State Center for Advocacy, Response, and Education (CARE) . Confidential mental health services can be found with Lafene Counseling and Psychological Services (CAPS) . Academic support can be found with the Office of Student Life (OSL) . OSL is a non-confidential resource. OIE also provides a comprehensive list of resources on their website. If you have questions about non-confidential and confidential resources, please contact OIE at equity@ksu.edu or (785) 532–6220.\nAcademic Freedom Statement Kansas State University is a community of students, faculty, and staff who work together to discover new knowledge, create new ideas, and share the results of their scholarly inquiry with the wider public. Although new ideas or research results may be controversial or challenge established views, the health and growth of any society requires frank intellectual exchange. Academic freedom protects this type of free exchange and is thus essential to any university’s mission.\nMoreover, academic freedom supports collaborative work in the pursuit of truth and the dissemination of knowledge in an environment of inquiry, respectful debate, and professionalism. Academic freedom is not limited to the classroom or to scientific and scholarly research, but extends to the life of the university as well as to larger social and political questions. It is the right and responsibility of the university community to engage with such issues.\nCampus Safety Kansas State University is committed to providing a safe teaching and learning environment for student and faculty members. In order to enhance your safety in the unlikely case of a campus emergency make sure that you know where and how to quickly exit your classroom and how to follow any emergency directives. Current Campus Emergency Information is available at the University’s Advisory webpage.\nStudent Resources K-State has many resources to help contribute to student success. These resources include accommodations for academics, paying for college, student life, health and safety, and others. Check out the Student Guide to Help and Resources: One Stop Shop for more information.\nStudent Academic Creations Student academic creations are subject to Kansas State University and Kansas Board of Regents Intellectual Property Policies. For courses in which students will be creating intellectual property, the K-State policy can be found at University Handbook, Appendix R: Intellectual Property Policy and Institutional Procedures (part I.E.) . These policies address ownership and use of student academic creations.\nMental Health Your mental health and good relationships are vital to your overall well-being. Symptoms of mental health issues may include excessive sadness or worry, thoughts of death or self-harm, inability to concentrate, lack of motivation, or substance abuse. Although problems can occur anytime for anyone, you should pay extra attention to your mental health if you are feeling academic or financial stress, discrimination, or have experienced a traumatic event, such as loss of a friend or family member, sexual assault or other physical or emotional abuse.\nIf you are struggling with these issues, do not wait to seek assistance.\nKansas State University Counseling and Psychological Services offers free and confidential services to assist you to meet these challenges. Lafene Health Center has specialized nurse practitioners to assist with mental health. The Office of Student Life can direct you to additional resources. K-State Family Center offers individual, couple, and family counseling services on a sliding fee scale. Center for Advocacy, Response, and Education (CARE) provides free and confidential assistance for those in our K-State community who have been victimized by violence. For Kansas State Salina Campus:\nKansas State Salina Counseling Services offers free and confidential services to assist you to meet these challenges. The Kansas State Salina Office of Student Life can direct you to additional resources. The Kansas State Salina Campus offers several services for students, including health services, counseling, and academic assistance. For Global Campus/K-State Online:\nK-State Online students have free access to mental health counseling with My SSP - 24/7 support via chat and phone. The Office of Student Life can direct you to additional resources. University Excused Absences K-State has a University Excused Absence policy (Section F62) . Class absence(s) will be handled between the instructor and the student unless there are other university offices involved. For university excused absences, instructors shall provide the student the opportunity to make up missed assignments, activities, and/or attendance specific points that contribute to the course grade, unless they decide to excuse those missed assignments from the student’s course grade. Please see the policy for a complete list of university excused absences and how to obtain one. Students are encouraged to contact their instructor regarding their absences.\nCopyright Notice © The materials in this online course fall under the protection of all intellectual property, copyright and trademark laws of the U.S. The digital materials included here come with the legal permissions and releases of the copyright holders. These course materials should be used for educational purposes only; the contents should not be distributed electronically or otherwise beyond the confines of this online course. The URLs listed here do not suggest endorsement of either the site owners or the contents found at the sites. Likewise, mentioned brands (products and services) do not suggest endorsement. Students own copyright to what they create.\n",
    "description": "",
    "tags": null,
    "title": "Fall 2023 Syllabus",
    "uri": "/0-introduction/05-syllabus/index.html"
  },
  {
    "content": " Remote Connections (4) SSH to Linux RDP to Windows Ubuntu Static IP (4) IP \u0026 Subnet Default Gateway DNS Ubuntu DNS Server (15) Forward Zone File Reverse Zone File Forwarders ACL Ubuntu DHCP Server (15) Domain Name DNS Servers Routers Range Authoritative Ubuntu SNMP Daemon (4) snmp demonstration 3 Screenshots - before ping, pinging, after ping Ubuntu Firewall (-5 if not enabled or configured) Wireshark (8) DNS query people.cs.ksu.edu DNS Response people.cs.ksu.edu DNS PRT 208.67.222.222 ICMP Echo SNMP Packet DHCP Offer HTTP Redirect HTTP Basic Authentication showing username and password ",
    "description": "",
    "tags": null,
    "title": "Lab 3 Grading Checklist",
    "uri": "/z-instructor-resources/05-lab-3-grading-checklist/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Best System Monitoring Tools for Windows \u0026 Linux (Free \u0026 Paid) from Comparitech Beocat System Monitor Video Transcript One major part of any system administrator’s job is to monitor the systems within an organization. However, for many groups, monitoring is sometimes seen as an afterthought, since it requires additional time and effort to properly set up and configure a monitoring system. However, there are a few good reasons to consider investing some time and effort into a proper monitoring system.\nFirst and foremost, proper monitoring will help you keep your systems up and running by alerting you to errors as soon as they occur. In many organizations, systems are in use around the clock, but often IT staff are only available during the working day. So, if an error occurs after hours, it might take quite a while for the correct person to be contacted. A monitoring system, however, can contact that person directly as soon as an error is detected. In addition, it will allow you to more quickly respond to emergencies and cyber threats against your organization by detecting them and acting upon them quickly. Finally, a proper monitoring system can even save money in the long run, as it can help prevent or mitigate large system crashes and downtime.\nWhen monitoring your systems, there are a number of different metrics you may be interested in. First, you’ll want to be able to assess the health of the system, so knowing such things as the CPU, memory, disk, and network usage are key. In addition, you might want to know what software is installed, and have access to the log files for any services running on the system. Finally, many organizations use a monitoring system to help track and maintain inventory, as systems frequently move around an organization.\nHowever, beyond just looking at individual systems themselves, you may want to add additional monitoring to other layers of your infrastructure, such as network devices. You may also be involved in monitoring physical aspects of your environment, such as temperature, humidity, and even vibrations, as well as the data from security cameras and monitors throughout the area. In addition, many organizations set up external alerts, using tools such as Google Analytics to be alerted when their organization is in the news or search traffic suddenly spikes. Finally, there are many tools available to help aggregate these various monitoring systems together into a single, cohesive dashboard, and allow system administrators to set customized alert scenarios.\nOne great example of a system monitoring system is the one from K-State’s own Beocat supercomputer, which is linked in the resources section below this video. They use a frontend powered by Ganglia to provide a dashboard view of over 400 unique systems. I encourage you to check it out and see how much interesting data they are able to collect.\nOf course, there are many paid tools for this task as well. This is a screenshot of GFI LanGuard, one of many tools available to perform system monitoring and more. Depending on your organization’s needs, you may end up reviewing a number of tools such as this for your use.\nIn the next few videos, I’ll discuss some monitoring tools for both Windows and Linux. As part of your lab assignment, you’ll install a monitoring system on your cloud servers running on DigitalOcean, giving you some insight into their performance.\n",
    "description": "",
    "tags": null,
    "title": "Monitoring",
    "uri": "/7-backups-monitoring-devops/05-monitoring/index.html"
  },
  {
    "content": " YouTube Video Resources Windows PowerShell: Scripting Crash Course from Microsoft TechNet How to Write and Run Scripts in the Windows PowerShell ISE from Microsoft Windows PowerShell Scripting Tutorial for Beginners from Netwrix Video Transcript This video introduces the Windows PowerShell Integrated Scripting Environment (ISE) and covers the basics for creating and running your own scripts in Windows PowerShell.\nBefore we begin, we must change one system setting to allow unsigned PowerShell scripts to run. To do this, you’ll need to open a PowerShell window using the Run as Administrator option, and then enter the following command:\nSet-ExecutionPolicy Unrestricted However, be aware that this makes your system a bit more vulnerable. If you or a malicious program tries to run a malicious PowerShell script that is unsigned, the system will not try to stop you. It would be a very rare occurrence, but it is something to be aware of.\nNow, let’s open the PowerShell ISE to start writing scripts. To find it, simply search for PowerShell ISE on your Start Menu. It should be included by default on all versions of Windows 10.\nWhen you first open the PowerShell ISE, you may have to click the Script button in the upper-right corner of the window to view both the scripting pane and the command window. To write your scripts, you can simply write the text in the upper window, then save the file and run the program using the Run script button on the top toolbar. You can also run the script in the command window below.\nTo begin, let’s take a look at a simple Hello World script:\nWrite-Host \"Hello World\" return This script should be pretty self-explanatory. The Write-Host command simply displays text on the terminal, and the return line ends the script. As with any programming language, it is good practice to include a return at the end of your script, but it is not necessarily required.\nLet’s look at a more advanced script to see more features of the Windows PowerShell scripting language.\nParam( [string]$user ) if ( -not ($user)){ Write-Host \"Usage: batman.ps1 \u003cname\u003e\" return } if ($user.CompareTo(\"Batman\") -eq 0){ Write-Host \"Hello Mr. Wayne\" }else{ if ($user.CompareTo(\"Robin\") -eq 0){ Write-Host \"Welcome back Mr. Grayson\" }else{ Write-Host \"Intruder alert!\" } } return At the top of this script, there is a special Param section. In PowerShell, any parameters expected from the user must be declared here, with the data type in square brackets ([ and ]), followed by the variable name prefixed with a dollar sign $. In this script, we have declared one command line parameter $user of type string.\nBelow that, we have the first if statement:\nif ( -not ($user)){ Write-Host \"Usage: batman.ps1 \u003cname\u003e\" return } This will check to see if the $user parameter was provided. If it was not, it will print an error message and return to end the script.\nThe next if statement:\nif ($user.CompareTo(\"Batman\") -eq 0){ Write-Host \"Hello Mr. Wayne\" }else{ if ($user.CompareTo(\"Robin\") -eq 0){ Write-Host \"Welcome back Mr. Grayson\" }else{ Write-Host \"Intruder alert!\" } } performs a string comparison between the $user parameter and the string “Batman”, if the comparison returns 0, they are equal. This is very similar to other string comparison functions in C# and Java. One interesting item to note here is the use of -eq to denote equality. For some reason, PowerShell uses short textual comparison operators instead of the common symbols for boolean comparisons. I really don’t know why that particular design decision was made, but I encourage you to look at the documentation to see what options are available for comparison.\nHere is another simple script:\nParam( [string]$path ) $files = Get-ChildItem $path foreach($file in $files){ Write-Host $file.name } This is an example of a simple looping script. In this script, it will get a path from the user as an argument, then store a list of all the child items on that path in $files variable. Then, it will use a “foreach” loop to print out the name of each of those files. Thankfully, if you’ve done any programming in the .NET family of languages, most of this syntax will be very familiar to you.\nYou can also use PowerShell to create a simple menu for your script. This is a bit more involved than the example from the Linux Bash scripting video, but it is pretty straightforward:\n$title = \"Select Options\" $message = \"Choose an option to perform\" $build = New-Object System.Management.Automation.Host.ChoiceDescription \"\u0026Build\", \"Build the project\" $run = New-Object System.Management.Automation.Host.ChoiceDescription \"\u0026Run\", \"Run the project\" $clean = New-Object System.Management.Automation.Host.ChoiceDescription \"\u0026Clean\", \"Clean the project\" $quit = New-Object System.Management.Automation.Host.ChoiceDescription \"\u0026Quit\", \"Quit\" $options = [System.Management.Automation.Host.ChoiceDescription[]]($build, $run, $clean, $quit) $result = $host.ui.PromptForChoice($title, $message, $options, 0) switch ($result) { 0 { Write-Host \"You selected Build\" } 1 { Write-Host \"You selected Run\" } 2 { Write-Host \"You selected Clean\" } 3 { Write-Host \"You selected Quit\" } } At the top of the script, the first two variables define the text that will be shown as the title of the menu and the message displayed to the user before the menu options. The next four variables define the menu choices available. Looking at the first one, the \u0026Build gives the title of the option, with the ampersand \u0026 before the ‘B’ indicating which letter should be the shortcut to choose that option. The next string gives a longer description of the option. Finally, we put it all together in the $options variable as an array of available options, then use the PromptForChoice function to display the menu to the user, storing the user’s choice in the $result variable. Finally, we use a simple “switch” statement to determine which option the user chose and then perform that task.\nIf you are having trouble understanding what each part does, I recommend just running the script once and then matching up the text displayed in the menu with the script’s code. It may seem a bit daunting at first, but it is actually pretty simple overall. As a quick aside, if you run this on a system with a GUI, you may get an actual pop-up menu instead of a textual menu, but rest assured that it will display the textual version on system’s without a GUI.\nFinally, here is a quick script demonstrating how to get user input directly within the script:\n$name = Read-Host \"Input your name and press [ENTER]\" Write-Host \"Welcome $name!\" It’s as simple as that! Of course, that is just a very small taste of what PowerShell is capable of. Hopefully this introduction gives you some idea of what is available, but I encourage you to consult the online documentation for PowerShell to learn even more about it.\n",
    "description": "",
    "tags": null,
    "title": "PowerShell Scripting",
    "uri": "/x-extras/05-powershell-scripting/index.html"
  },
  {
    "content": " Puppet Learning VM Deprecated As of 2023, the Puppet Learning VM is no longer being maintained. The videos below demonstrate some of the features of Puppet, which can also be done on your Ubuntu VM after installing Puppet Agent. Unfortunately, it is not easily possible to simulate an enterprise Puppet setup without this VM, so I’ll keep these videos up for demonstration purposes. –Russ\nYouTube Video Resources Slides Core Types Cheat Sheet from Puppet Resource Type Reference from Puppet Video Script In this video, we will begin learning how to use Puppet to configure a system. Before creating our own Puppet scripts, called Manifest Files, we will discuss how Puppet actually views a system it is configuring.\nIn Puppet, a system is simply a set of resources. There are many different types of resources, such as files, user accounts, installed programs, and more. In addition to a type, each resource has a title, and a set of attributes giving additional information about the resource. The resources section below the video has links to the Puppet documentation for resource types.\nLet’s review some different resources using the Puppet Learning VM. You can also perform many of these same operations on your Windows and Linux computers with Puppet Agent installed. If you’d like to follow along, I’ll be working in the hello_puppet quest on the Puppet Learning VM.\nI have already performed the first task for the hello_puppet quest, so I’m now connected to one of the internal systems and installed the Puppet Agent on it. Now, I can start Task 2, where I review a file resource. Using Puppet, you can describe a file resource such as the following:\nsudo puppet resource file /tmp/test That should give you information about that file. Here you can see that the resource is of type file, and has its path for a title. Below that are the attributes of the file, given as parameter =\u003e value pairs. Since the file doesn’t exist, the only attribute visible is the ensure attribute, and it shows that the file is absent on the system.\nWe can easily create the file using this command:\ntouch /tmp/test Then we can use the same resource command to view it:\nsudo puppet resource file /tmp/test Now we can see many additional attributes of the file.\nWe can also use the puppet resource command to modify resource. For example, let’s add some content to that file:\nsudo puppet resource file /tmp/test content='Hello Puppet!' Once you run that command, you can view the contents of the file to confirm that it worked:\ncat /tmp/test There are many types of resources that can be viewed and modified in this way. For example, you can view information about a user account, such as the learning account on the current VM:\nsudo puppet resource user learning You can also find information about installed software packages, such as the Apache Webserver httpd:\nsudo puppet resource package httpd In this case, since the package is not installed on the system, the ensure attribute is set to purged, which is similar to absent.\nThe Puppet Learning VM quest describes how to see the inner workings of a Puppet Resource by breaking it. I’m not going to go over that process in detail, but I recommend you review that information on your own.\nAs with the file, we can configure attributes easily enough:\nsudo puppet resource package httpd ensure=present That command will install the latest version of the httpd package. Note that when it executes, the ensure value is changed to the current version. Later, as you define your Puppet manifests, you can use the ensure attribute to install the latest version using the present value, or provide a specific version number here if desired.\nThere are many different types of resources available in Puppet. I encourage you to review some of the documentation linked below this video before continuing, just to get an idea of what is available. The next video will describe how to create your own Puppet Manifest Files and apply them to a system directly using the Puppet Agent.\n",
    "description": "",
    "tags": null,
    "title": "Puppet Resources",
    "uri": "/2-configuration-management/05-puppet-resources/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Docker Tutorial for Beginners by TechWorld with Nana on YouTube Networking Containers from Docker Docker ARG, ENV, and .env - A Complete Guide by Vladislav Supalov Docker Storage Overview from Docker (contains information about choosing different storage types) Docker Bind Mounts from Docker Docker Volumes from Docker Video Transcript So far we’ve learned how to pull Docker images from a repository and instantiate them as containers, and also how to use Docker Compose to achieve the same result, but we really haven’t been able to do much with those containers. That’s because we haven’t given our containers any resources to work with. So, let’s do that now.\nThere are many different resources that can be assigned to containers. We can connect a network port from our host system to a network port on a container, allowing external access into the container from the network. We can also attach containers to various internal networks, allowing specific containers to talk with each other directly. In addition, we can set specific environment variables within a container, which are primarily used by the applications running within the container as configuration parameters, specifying things such as usernames, passwords, and other data needed by the container. Finally, we can attach a storage volume to a container, allowing us to store and persist data even after a container is stopped.\nLet’s start with the most common container resource - the networked port. This diagram shows a high-level overview of how this works. Inside of Docker, we might have two containers, one running a database server and the other hosting a web server. Since we want external users to be able to talk to the web server, we can map a network port from our host system, represented by the outer box, to a port on the web container. In this case, we are connecting the external port 8000 with the internal port 5000 on the web container. So, any incoming network traffic to our host on port 8000 will be forwarded to port 5000 on the web container.\nTo do this in Docker, we can use the -p flag on the docker run command to map a port from the host system to a container. The syntax of this command puts the external port first, then the port on the container. So, in this command, we are connecting port 8080 on our system to port 80 inside the Nginx container.\nBelow, we see the same system defined in Docker Compose as well. It adds a ports: entry below the service definition, and uses the same syntax as the docker run command. Of course, in both cases we can map multiple ports inside of the container by supplying additional -p entries in docker run or by adding additional elements below the ports: entry in Docker Compose.\nSo, let’s give this a try and then show that we can access the webserver from outside the container.\n[demo here]\nOnce we’ve mapped a port on a container, we can use the docker ps command to see the various port mappings present on a container. This is a great way to quickly confirm that we mapped the port correctly. I still sometimes get my ports reversed, so this is a quick and easy way to confirm it is set up properly when I’m debugging problems in a container.\nNext, lets go from individual ports to entire networks. We can expand the previous example by moving the db container to a separate network within Docker, isolating it from other containers and possibly even the outside world by making the new network internal to Docker.\n–\nTo do this using the Docker client, we can start by creating our container using the docker run command. This will connect the container to the default network in Docker - any container that is started without defining a network will be automatically connected to the default network.\nHowever, we can create a new network using the docker network command as shown here. This will create a new bridge network named redis_network, and it also specifies that it is an internal network, meaning that it won’t be connected to the outside world at all.\nOnce we’ve created the network, we can attach any existing containers to that network using the docker network connect command.\nWe can also use the --network flag in the docker run command automatically connect a container to a defined network when we create it. However, it is worth noting that this has two caveats:\nThe container will not be connected to the default Docker network We can only specify one network in the docker run command. If we want the container to be attached to multiple networks, we have to attach the rest manually using the docker network connect command. This Docker Compose file shows the same basic setup as the previous set of Docker commands We start by creating an nignx service that is connected to both the default and redis networks, and it has a port mapping to the outside world. Then, the redis service is only connected to the redis network. Finally, at the bottom of the file, we see a new top-level entry networks that lists the networks used by this configuration. Notice that we don’t have to include the default network here, since it is created for us automatically, though we can list it here if we want to configure it beyond the default settings.\nSo, let’s go ahead and apply this Docker Compose file to see it in action.\n[demo here]\nOnce we’ve got our containers running, we can use the docker network inspect command to inspect the various networks on our system. If we scroll through the output, we can find the containers connected to each network, as well as other information about how the network is configured. We can also test and make sure the containers can talk with each other, though we’ll leave that to a later example.\nAnother resource we can give to our Docker containers are environment variables. Many applications that are developed to run in Docker containers use environment variables for configuration. A great example of this is the MySQL Docker image - when we create a container from that image, we can provide a root password for the database as an environment variable. We can also use it to configure a second account and database, making it quick and easy to automate some of the basic steps required when setting up a database for an application. This diagram shows some of the places where environment variables are used in the process of building and instantiating a Docker container.\nSo, let’s see what that looks like in practice. Here is a set of docker commands that can be used to create two containers - one containing a MySQL database server, and another containing PHPMyAdmin, a program for managing MySQL databases. In both of the docker run commands, we are including an environment variable using the -e flag. For MySQL, we are setting a root password. Then, for the PHPMyAdmin container, we are configuring the hostname where it can find the MySQL server. Notice that the hostname we are giving it is the name of the MySQL container we created earlier, mysql1. This is one of the coolest aspects of networking in Docker! Docker actually handles an internal form of DNS for us, so each Docker container on a network is accessible by simply using the container name as the full hostname on the Docker network. This makes it super simple to connect Docker containers together in a network!\nThis slide shows the same basic setup in a Docker Compose file. Just like we specify ports and networks in the YML format, we can add another entry for environment that lists all of the environment variables. So, as before, we can create a docker-compose.yml file containing this content, and then use the docker compose up -d command to bring up this configuration in Docker. Let’s do that now to see how it works.\n[demo here]\nWe can also explore our running Docker containers to find the various environment variables that are available to that container. We can use the docker inspect command to inspect the configuration of each individual container. Of course, this presents a unique security vulnerability - notice here that we can easily see the root password for our MySQL server in our Docker environment. This is not good! If anyone has access to the Docker engine, they can potentially find out tons of sensitive information about our containers! So, we should think very carefully about what information we put in the environment variables, and who has access to the Docker engine itself. Thankfully, for orchestrating containers across the network, tools such as Kubernetes include a way to secure this information, so it is less of a concern there.\nNow that we have a running MySQL server, along with PHPMyAdmin, let’s take a minute to set up a database and store some information in it.\n[demo here]\nNow that we’ve stored some information in this database, let’s stop and restart the container and see what happens.\n[demo here]\nUh oh! Our information disappeared! This is because the transient read/write layer of a container is discarded when it is stopped, so any changes to the filesystem are not kept. While this is a great feature in many ways, it can also cause issues if we actually want to save that data!\nDocker includes two different ways we can store data outside of a container. The first is called a bind mount, which simply connects a directory on our host’s filesystem to a folder path in the container. Then, any data in the container that is written to that folder is actually stored in our host’s filesystem outside of the container, and it will be available even after the container is stopped.\nThe other method we can use is a Docker volume. A Docker volume is a special storage location that is created and managed by Docker itself. We can think of it like a virtual hard disk that we would use with VMWare - it is stored on the host filesystem, but it isn’t directly accessible outside of Docker itself.\nThere are many pros and cons for using both bind mounts and volumes in Docker - it really depends on how you intend to use the data. See the Docker documentation for a good discussion about each type of storage and the use-cases where it makes the most sense.\nTo use a bind mount in Docker, we must first make sure the folder exists in our host filesystem. For this, I’m just going to use the mkdir command to make the directory. We can then add a -v parameter to our docker run command. The first part of the volume entry is the path to the directory on the host system, and then following a colon we see the path within the container’s filesystem where it will be mounted. This is a very similar syntax to the port mappings we learned about earlier. At the bottom of this slide, we see the same configuration in Docker compose - we simply add a volumes entry and list the bind mount there.\nTo use a volume in Docker, the process is very similar. First, we can use the docker volume create command to make a volume in Docker. Then, in our docker run command, we use the name of the volume as the first part of the -v parameter, followed by the path inside of the container where it should be mounted.\nIn Docker Compose, the process is very similar. However, in this case, we don’t have to manually use the docker volume create command, as Docker Compose will handle creating the volume for us.\nOne thing to be aware of is where the volume is mounted within the container. Many Docker images, such as MySQL, include notes in the documentation about where data is stored in the container and which directories should be mounted as volumes to persist the data. So, in this case, the /var/lib/mysql directory in the container is the location given in the MySQL Docker Image documentation for persistent storage, so that’s why we are mounting our volume in that location.\nSo, let’s update our MySQL container to include a volume for storing data. Once we do that, we’ll show that it will properly store and persist data.\n[demo here]\nFinally, just like any other resource, we can use the docker inspect commands to see the volumes and bind mounts available on a container.\nThere we go! That’s a pretty in-depth overview of the various resources that we can add to a Docker container. We’ve learned how to map ports from the host system to ports within a container, connect containers together via various Docker networks, provide data to Docker containers in environment variables, and finally we can now persist data using bind mounts and Docker volumes. At this point, we should have enough information to really start using Docker effectively in our infrastructures. For the rest of this module, we’ll dive into various ways we can use Docker and more advanced configurations.\n",
    "description": "",
    "tags": null,
    "title": "Resources",
    "uri": "/5a-containers/05-resources/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello, and welcome to the week five Announcements video for CIS 527 in summer 2022. So this week, you should be wrapping up lab four, which is notoriously the hardest lab in this class, it is due today by 7pm. So make sure you get that done and schedule a grading time with me later today. So we can get that looked at. And then next week, you’re going to be shifting over to lab five, where we’re going to pivot from virtual machines on our system to actually working in the cloud. So Friday of the week, five quizzes are due and the next Monday, you’ll be turning in lab five, as well as the week five discussion.\nSo for lab four grading, it’s actually pretty simple. There’s not a whole lot that I need to see. But there’s a lot of stuff behind the scenes that need to work correctly. In order for this to work. Basically in lab for grading, I’m going to check your Windows Server, make sure it’s up and running, going to check your Windows Active Directory User and Group to see that you’re actually able to create your user. And then I want to see you actually log into the active directory using Windows. Likewise on Ubuntu, I want to see your open LDAP server working and actually see a user and group in PHP LDAP admin. And then I want to see your Ubuntu be able to log into LDAP. And then either that same VM on a different snapshot or a different VM log into Active Directory either way. So really, it takes only about five minutes to grade this if all your VMs are up and running. But it is a lot of complexity behind the scenes to get all that to work.\nSo again, just like last week to be successful, make sure you read things carefully. Make sure you look at the posted diagrams and the exercises, use the resources you have available to you I try and link some really good guidance for how to do this. But the big thing is, don’t be afraid to ask questions. If you’re spinning your wheels. If you get stuck and you’re not making progress for about an hour, feel free to ask questions. I’m available on Discord, you can email me you can join us for office hours, anytime that you want. Just let me know how I can help.\nSo this week’s Speaker I mentioned him last week is Kyle Hutson, one of the Beocat admins and K-State, he helps manage key state supercomputer that you probably seen in the engineering building right next to the computer science classrooms. He has a lot of experience working with very powerful hardware with very large on premises setups. And he talks a little bit about the difference between K-State hosting its own supercomputer versus using resources in the cloud, such as AWS, and why we would do something like that. And he also talks a little bit about scientific computing and how it differs from the general computing that you might work with at an industry itself.\nSo this week, you’re going to be working in lab five, where we’re going to move everything to the cloud. So the big thing with lab five is you’re going to create two droplets on Digital Ocean, which are two virtual servers that work up in the cloud, you’re going to set up SSH and a firewall on those so that you can connect to them remotely, you’re going to set up simple Apache sites on each one with a virtual hosts so that you can access websites on each one, you’ll also need to set up a DNS domain name and actually point those DNS names at those servers so that you can access them directly there. And you’ll also set up a Docker reverse proxy following the instructions in some of the new Docker videos this year. So lab five is kind of introducing all the cloud concepts. And then lab six and seven, we iterate on that a little bit more.\nSo a couple of things that you want to check out are the GitHub education pack, which is education github.com, it looks like this got cut off. But you can find DigitalOcean credits by going to try to digitalocean.com/freetrialoffer that will give you up to $100, credit and DigitalOcean. If you have a new account, if you go to nc.me, you can get a free .me domain for one year using your .edu email address when you sign up. If you have trouble getting access to either of those, please feel free to ask me, I’ve got referral credits, I can send you the free trial links, there’s lots of codes that you can use. So ideally, you shouldn’t have to pay anything for this, if you’ve never set up accounts on these services before. If you have set up accounts on these services at most, you may have to pay about 11 dollars to do this. So let me know, if you have any trouble getting this to work, I’m happy to share some referral credits or help you out in any way possible. But those are the two things you’ll need to sign up for in order to start working in the cloud.\nSo that’s all I got this week. As always, we have good discussions on this court. So you can always join me there, you can join me for tea time office hours, Tuesdays at 330. And Fridays at 1030. We’re always online in zoom for about an hour, you can schedule a one on one office hour with me using my Calendly link, you can always send me an email as well, I’m always happy to help. And remember the big thing in this class is anytime you get stuck, or you feel like you’re spinning your wheels, that’s a good time to take a step back and ask for help. Instead of trying to power forward, there are certain situations you can get into in this class where it’s very hard to make a forward progress. And so don’t be afraid to ask me questions. And I can help you either reset or restart or adjust things so that you can get it to work. So that’s what we’ve got going on. I really like this XKCD comic here where it talks about the cloud. I in my lecture, I talked a little bit about the cloud that it’s really just from a point of view, it’s really somebody else’s computer. And so you or I might think of as cloud is actually somebody else’s computer. So it’s really important to think about the cloud that way it’s kind of a good framing Advice to use for the cloud and so I’d like to end on this XKCD comic for the slideshow so as always if you have any questions let me know otherwise Best of luck working on lab five this week and I will see you next week\n",
    "description": "",
    "tags": null,
    "title": "Summer '22 Week 5",
    "uri": "/y-announcements/old/summer2022/week05/index.html"
  },
  {
    "content": " YouTube Video Resources Slides How to Get Windows 10 from Microsoft Azure Portal via CS Support How to Install VMware Tools from VMware Windows 10 Overview from Microsoft Microsoft Windows Version History from Wikipedia Windows 10 for Windows 7 Users from How-To Geek Windows 10 for Windows 8 Users from How-To Geek Windows 10 Help from Microsoft Windows 10 Wiki from Microsoft TechNet Windows 10 Architecture from Microsoft TechNet Windows 10 ISO Downloads from Microsoft Warning: May not work with keys from the Microsoft Imagine store! Use the downloads from the store instead. Operating System Market Share from Net Market Share How to Add Fast Forward Effect | Davinci Resolve 14 Tutorial from Chris’ Tutorials on YouTube Video Script This video introduces the Windows 10 operating system, and shows how to install it in a virtual machine.\nThe Windows family of operating systems has the largest market share of all desktop and laptop operating systems, with an estimated 88% of all personal computer systems using some form of Windows. Unsurprisingly, Windows 10 currently holds the largest market share overall, but there are still a large number of systems using Windows 7. Of those, most are either in enterprise organizations who have chosen not to upgrade or owned by home users who rarely purchase a new computer.\nThe Windows operating system has a long history. It originally was developed as a graphical “shell” for the MS-DOS operating system, which was popular in the early 1980s. Windows 1.0 was released in 1985, followed by Windows 2.0 and 3.0.\nWindows surged in popularity with the release of Windows 95, with 40 million copies sold during its first year. Many features of the modern Windows operating system were present even in Windows 95, such as the control panel, registry, and Internet Explorer web browser.\nAt the same time, development began on a version of Windows that was not dependent on an underlying DOS-based operating system. This process led to the release of Windows NT, followed by Windows 2000 and Windows XP. Windows XP was the dominant operating system on the market for nearly a decade until it was surpassed by Windows 7, which has been on top ever since.\nWindows 10 was released in 2015, and is the most current version of Windows. We’ll be using Windows 10 throughout much of this course.\nHere is a diagram showing the various versions of Windows and how they relate to one another. Notice that there are a few major families - the DOS versions at the top in red, starting with 1.0. The NT family near the bottom in blue, beginning with NT 3.1, as well as the server versions starting with Server 2003 in green. Finally, in the middle, there are a few mobile versions of Windows as well in yellow and orange, but they have been discontinued in recent years.\nNext, let’s see how to install Windows 10 in a virtual machine. At this point, I’m assuming you have already installed VMware Workstation or another virtualization software on your computer. If not, I recommend doing so before continuing.\nFirst, you’ll need to download the Windows 10 installation file and obtain a product key. Both of those can be found on the Microsoft Azure student portal, which is linked in the resources section below the video. This site now uses your K-State eID and password, making it even easier to access.\nOnce you log in, simply click Software on the left and search for Windows 10. In the list, find the entry for Windows 10 (consumer editions), version 1903 - DVD and download it. On the download page, you can also click this button to access your product key, which you’ll need to provide when you install it.\nAfter you have completed that task, you should have a large installation file in the .ISO file format and a 25 character product key available. You’ll need both when installing Windows.\nNext, let’s open VMware Workstation and create a new virtual machine. For this course, I recommend choosing “I will install the operating system later” to bypass the Easy Install feature of VMware. This will allow you to directly observe the installation process as it would be performed on a real computer.\nNext, we will select the type and version of the guest operating system. In this case, it will be Windows 10 x64. We can then give it a helpful name, and choose where it is stored on the computer. If you’d like to store it on a secondary hard disk, you can do so here. I recommend storing your virtual machines on the largest, fastest storage device you have available, preferably an SSD with at least 60 GB of free space.\nOn the following pages, you can choose the desired disk size and format. You should consult the Lab 1 assignment materials to make sure you choose the correct options here.\nFinally, you can click Finish to create the virtual machine. We’ll have to customize the hardware anyway, so we can come back to that once it is finished.\nOnce your virtual machine is created, you can click on the edit virtual machine settings button to customize the hardware. Again, make sure the hardware specification matches what is defined in the assignment for Lab 1. To install the operating system, we’ll have to tell the virtual machine where to find the .ISO file downloaded earlier. Click the CD/DVD option, then select the file and make sure it is enabled.\nIn addition, we’ll disable the network during installation so that we’ll be prompted to create a local account. Once we are done installing Windows 10, we can re-enable this option to connect to the internet and download updates.\nWhen you are ready to begin the installation process, click the button to power on the virtual machine. When it powers on, you may be prompted to press any key to install Windows. You’ll need to click somewhere inside of the VMWare window before pressing any key in order for the VM to recognize it.\nOnce it boots, you’ll be given the option to install Windows 10. Follow the prompts to install Windows 10 using the Lab 1 assignment as needed for configuration information. The virtual machine may reboot several times during the process.\nWhile installing, you may have to select options to confirm that you don’t have access to the internet, and would like to continue with limited setup. This is fine - Windows just really wants us to use a Microsoft account. You may also have to answer some security question - feel free to just make up answers if you want! They won’t be needed.\nYou can also disable many of the optional features, such as the digital assistant, location data, and targeted advertising. While they are helpful on personal computers, they won’t be used in this course.\nWhen you have successfully installed Windows 10, you’ll be ready to move on to configuring Windows 10. The next several videos will discuss that process.\nHowever, before going too far, I recommend installing VMware Tools in the Windows 10 virtual machine. This will allow you to have better control over the virtual machine. You can find instructions for doing that in the resources section below the video.\n",
    "description": "",
    "tags": null,
    "title": "Windows 10 Overview \u0026 Installation",
    "uri": "/1-secure-workstations/05-windows-overview-installation/index.html"
  },
  {
    "content": " YouTube Video Resources Join Windows 10 PC to a Domain from Windows 10 Forums Video Transcript Once you have completely set up your Active Directory Domain, you can begin to add Windows clients to the domain. This video will walk through some of that process, and I’ll discuss a few of the steps as we go.\nFor this example, I have my Windows 10 VM from the earlier labs. It is on the same network as the domain controller. Before I begin, I’ll need to set some static DNS settings on this computer. First, and most importantly, I’ll add a DNS entry for the domain controller first, and the second entry can be any of the other working DNS servers on the network. In this case, I’ll just use the VMware default gateway.\nNext, I’m going to right-click on the Start button and choose the System option. Before adding it to the domain, I need to make sure it is named correctly. If not, I’ll need to rename it and reboot before adding it to the domain. Once a computer is on a domain, it is very difficult to rename it.\nOnce I’m sure it is correct, I’m going to click the System Info link to get to the old System options in the Control Panel. Here I should be able to see the current workgroup. To add the system to a domain, click the Change settings button to the right of the computer name. Then, I’ll click the Change button to add it to the domain.\nOn this window, I’ll choose the Domain option, and enter the name of the domain, in this case cis527.local, and click OK. You may get a message about the NetBIOS name of the computer being too long, but you can safely ignore that message.\nIf everything works correctly, you should see a pop up asking you to enter the username and password of an account with permissions to add a computer to the domain. In this case, the only account we’ve set up with those permissions is the domain administrator account, so we’ll enter those credentials here. Once you enter it, it should welcome you to the domain, and prompt you to restart.\nIf your computer is unable to contact the domain at this step, it is usually a problem with your network configuration. To diagnose the problem, try to ping the domain name using the command-line ping utility and see what the response is. If it cannot find it, then check your network settings and DNS settings and make sure you can ping the domain controller as well. In many cases, this can be one of the more frustrating errors to debug if it happens to you.\nOnce the system reboots, you’ll be able to log in using any valid credentials for a domain user. As you experienced with the domain controller, you can enter the domain or computer name, followed by a slash, and then the user name to choose which user account to use. Thankfully, here you can always use .\\ to log in as the local computer. In this case, let’s use our domain user account.\nOnce you log in, let’s take a look at the Computer Management interface to see what changed about the users and groups on the system. If you open up the Administrators group, you’ll notice that it now includes an entry for CIS527\\Domain Admins, and likewise the Users group contains a new entry for CIS527\\Domain Users. That means, by default, any user on the domain can now log on to the computer as a user, and anyone on the domain with administrative privileges can log on to any computer on the domain and gain administrator permissions there as well. It is very important to understand these default permissions, so that you can assign them accordingly. For example, in many organizations, you may want to immediately remove the Domain Users entry from the Users group, and add a smaller group instead. You probably don’t need folks from the Accounting department logging on to systems in Human Resources, right?\nThat’s really all there is to it! You’ve successfully set up a Windows Active Directory Domain and added your first client computer to it. The next few videos will walk you through the same process on Ubuntu with OpenLDAP.\n",
    "description": "",
    "tags": null,
    "title": "Windows Client Configuration",
    "uri": "/4-directory-services/05-windows-client-configuration/index.html"
  },
  {
    "content": " YouTube Video Resources Windows Group Policy How to Share Files and Folders in Windows Server 2016 from Tactig How to Map Network Drives with Group Policy (Complete Guide) by Robert Allen on Active Directory Pro How to: Mapping Network Drives/Folders via Group Policy on Spiceworks Community Video Transcript In this video, I’ll briefly walk you through the steps of setting up a file server in Windows Server 2016, as well as how to access those resources from a Windows 10 client on the same network. Finally, I’ll discuss a bit of information about how to automatically map those resource as network drives on the client using Group Policy.\nFirst, let’s take a look at our server. I’m using the same VMs from Lab 4 to continue this example. Your server probably already has the File Server role installed, but if not you can install it following the same process used to install the Active Directory Domain Services role in Lab 4.\nIn the Server Manger, you can click on the File and Storage Services role to view information about your server. There, you can see information about the storage volumes available on your server, as well as the shared folders. You should already see two shared folders on your system, named NETLOGON and SYSVOL. These are created for the Active Directory Domain Controller role, as they store important information about the domain, such as Group Policy Objects, or GPOs, that should be replicated to other systems on the domain. So, you shouldn’t modify those shares!\nIf you’d like to create a new shared folder, the first step is to create that folder on your system. I’ll just create a folder in the root of the C:\\ drive called share. I’ll also create a simple text file in that folder, just so it isn’t empty. Next, I can either right-click the folder and configure the sharing options there, or I can share it through the wizard in the Server Manager. I’ll do the second option, since it gives me a bit more control over the configuration for the shared folder.\nIn that wizard, I’ll choose the “SMB Share - Quick” option since I don’t need to set any advanced settings. Next, I’ll set the location of the shared folder. In this case, since I’ve already created it, I can click the Browse button at the bottom to select that folder. On the following screen, I can set some basic information about the shared folder, including the name and description of the share.\nThere are a few additional settings you can configure for the share, such as hiding files based on a user’s permissions in the shared folder. I’ll make sure I checkmark the option to “Encrypt data access” to protect any remote connections to this shared folder.\nNext, you can set the permissions to access the share. To change them, click the Customize Permissions button. It is important to understand that a shared folder effectively has two sets of permissions - one set affecting access to the files and the folder itself, and another set, seen on the Share tab, that affects remote access to the files. In effect, you can limit who can access the files remotely, even if those users have permissions to access the files directly. I won’t make any changes at this point, but for one of the shares in the lab assignment you may need to update the permissions at this point.\nFinally, once everything is set correctly, I can click Create to create the shared folder. After it is created, I can see it in the Shares list inside the Server Manager.\nThere are a couple of ways to access the shared folder from a client on the same network. If network discovery is enabled, you can click the Network option on the left side of the Windows Explorer application to view servers on the network. However, in my experience, this option is usually the least successful, as Windows doesn’t have a great history of being able to easily locate shared resources on the network.\nAlternatively, you can always type two backslashes \\, followed by either the computer name or IP address in the address bar of Windows Explorer to view the shares available on that system. So, for my example setup, I would enter either \\\\cis527d-russfeld or \\\\192.168.40.42 to view the shares. I can then click on the shared folder name to view the files.\nHowever, doing so might be a bit of a hassle for your users, so thankfully there are a few ways to make this process simpler. First, you can right-click on any shared drive and select the Map Network Drive option to create a mapped drive on your computer. In effect, this creates a shortcut in Windows Explorer directly to the shared folder, and it will assign it a drive letter just like the local disks on your system. For many users, this is a very simple way to make those network resources available.\nHowever, you’d have to do this process manually for each user who would like to have the network drive mapped, and on each computer they would access it from. That seems very inefficient, right? Thankfully, there is an even better way to handle this using a Group Policy Object.\nIn the Group Policy editor, there is an option to create drive maps as part of a Group Policy Object. Here, I’ve configured a drive map to map that shared drive. Notice that I’m referencing it by IP address instead of the server name. This helps the system find the drive quickly, since that IP address should always work without needing to query the domain to find the server on the network.\nOnce I’ve created and enforced that Group Policy on the domain, I can switch back to the client and see if it works. One way to do so is to simply reboot the client and then log in again. When it reboots, it should receive the updated Group Policy Objects for the domain. However, if you’d like to test it immediately, you can open a Command Prompt or PowerShell window, and use the command gpupdate /force to force a Group Policy update from the Domain Controller.\nOnce you’ve updated the Group Policy, you should now see your newly mapped network drive in Windows Explorer. That’s all it takes! From there, you should be able to complete the Windows File Server portion of Lab 6. Make sure you pay special attention to the permissions for each shared folder. You may also want to review the information from Module 4 regarding Windows Group Policy for a quick refresher.\n",
    "description": "",
    "tags": null,
    "title": "Windows File Server",
    "uri": "/6-application-servers/05-windows-file-server/index.html"
  },
  {
    "content": "Moving from in-house to online.\n",
    "description": "",
    "tags": null,
    "title": "The Cloud",
    "uri": "/5-the-cloud/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Docker Tutorial for Beginners by TechWorld with Nana on YouTube Docker Get Started Tutorial from Docker Docker Sample Node Application from Docker on GitHub Docker Language Specific Guides from Docker GitHub CI Example (the sample site for this textbook’s theme) GitLab CI Example (this textbook) Video Transcript So far, we’ve explored how to download and use pre-built Docker images in our infrastructure. In many cases, that covers the vast majority of use cases, but what if we want to build our own Docker images? In this lesson, we’ll explore the process for creating our own Docker images from scratch. The process is pretty simple - we start with a special file called a Dockerfile that describes the image we want to create. In that file, we’ll identify a base image to build on, and then include information to configure that image, add our software and configure it, and then set some last details before building the new image. So, let’s see how that process works in practice!\nNormally, we’d probably have our own application that we want to include in the Docker container. However, since this is a course on system administration, we’re going to just use a sample program provided by Docker itself as the basis for our image. So, you can follow along with this entire tutorial by going to the Docker Get Started documentation that is at the URL shown on this slide. We’ll basically be following most of the steps in that tutorial directly.\nOur first step is to get a copy of the application code itself. So, on one of our systems that has Docker installed and configured, we can start by cloning the correct repository, and then navigating to the app directory inside of that repository. At this point, we may want to open that entire directory in a text editor to make it easy for us to make changes to the various files in the application. I’m going to open it in Visual Studio Code, since that is what I have set up on this system.\nNow that we have our application, we can create a Dockerfile that describes the Docker image that we’d like to create that contains this application. This slide shows the Dockerfile from the tutorial. Let’s go through it line by line to see what it does.\nFirst, we see a FROM line, which tells us what base image we want to build the image from. In this case, we have selected the node:12-alpine image. Based on the context, we can assume that this is a Docker image containing Node.js version 12, and that it was originally built using the Alpine Linux project, so the image is a very minimal image that may not have many additional libraries installed. This also tells us that we’ll need to use commands relevant the Alpine Linux project to install packages, so, for example, we’ll need to use apk add instead of apt install.\nThe next line is a RUN command, which specifies commands that should be run inside of the image. In this case, we are installing a couple of libraries using the apk add command. Since this command will change the filesystem of the image, it will end up generating a new layer on top of the node:12-alpine image.\nNext, we have a WORKDIR entry, which simply sets the working directory that the next commands will be run within. So, in this case, we’ll be storing our application’s code in the /app directory within the Docker image’s file system.\nAfter that we, see a COPY command. This one is a bit confusing, since the copy command in a Dockerfile operates in two different contexts. The first argument, . is interpreted within the host system, and references the current directory where the Dockerfile is stored. So, it will in effect copy all of the contents from that directory into the Docker image. The second argument, also a ., is relative to the Docker image itself. So, these files will be placed in the current directory in the Docker image, which is /app based on the WORKDIR entry above. As you might guess, this also creates a new layer in our created Docker image.\nOnce we’ve loaded our application files into the image, we see another RUN command. This time, we are using the yarn package manager to install any libraries required for our Node.js application to function. If you are familiar with more recent Node.js applications, this is roughly equivalent to the npm install command for the NPM package manager. This creates a third new layer in our resulting image.\nFinally, we see two entries that set some details about the final image itself. The first entry, CMD will set the command that is executed when a container is instantiated using this Docker image. Since we are building a Node.js application, we want to use the node command to run our server, which is stored in the src/index.js file. Notice that this command is also relative to the WORKDIR that was given above!\nFinally, we have an EXPOSE entry. This simply tells Docker that the port that should be made available from a container instantiated using this image is port 3000. However, by default this won’t actually create a port mapping to that port - we still have to do that manually! However, there are some tools that can use this information to automatically route data to the Docker container itself, and this EXPOSE entry helps it find the correct port to connect to.\nThere we go! That’s a basic Dockerfile - it may seem a bit complex, but when we break it down line by line, it is actually very simple and easy to understand.\nSo, once we’ve created a Dockerfile for our project, we can use the docker build command to actually build our Docker image. In that command, we can use the -t argument to “tag” the image with a simple to understand name so we can easily reference it later. We also need to specify the location of the Dockerfile, which is why we include the . at the end of the command.\nOnce our Docker image is built, we can use a simple docker run command to instantiate a container based on that image. We’ll also map port 3000 so we can access it from our local machine. If everything works, we can go to a web browser and visit http://localhost:3000 to see our application running!\nNow that we’ve built and run our Docker image, let’s make a quick update to the code and see how easy it is to rebuild it and run it again. First, we’ll make a quick edit in the app.js file to change a line of text. Then, we’ll use docker build to rebuild the Docker image. Take note of how long it takes to rebuild that image - we’ll try to make that faster a bit later!\nOnce it is built, we are ready to use it to instantiate a container. However, before we can run it, we must stop the running container that was built from the previous image using docker stop, and then we can use docker run to instantiate a new container.\nAt this point, we have a working Docker image that includes our application, but it is just stored on our own computer. If we want to share it with others, we have to upload it to a registry such as Docker Hub. If the project is hosted on GitHub or GitLab, we can use the various continuous integration and delivery features of those tools to automatically build our image and then host it on their registries as well. You can refer to the documentation for various registries if you are interested in hosting your image there. You can also look at some example CI/CD pipelines for both GitHub and GitLab by following the links at the top of this page.\nAnother task we may want to do after building our image is scanning it for vulnerabilities. Docker includes a quick scanning tool docker scan that will look at the image and try to find various software packages that are vulnerable. It is even able to look at things such as the package.json file for Node.js applications, or the requirements.txt file for various Python programs, among others.\nAs we can see on this slide, the current version of the Getting Started application actually includes a number of vulnerabilities introduced by an older version of a couple of Node.js libraries, so we may want to work on fixing those before we publish our image.\nLikewise, when we download an image from a registry, we may want to scan it first to see what vulnerabilities may be present in the image before we run it. Of course, this isn’t able to catch all possible vulnerabilities, but it is a good first step we can use when reviewing Docker images.\nFinally, let’s take a minute to look at the structure of our Dockerfile and see if we can do something to improve it a bit. Previously, our Dockerfile basically created three new layers on top of the base image - one to install packages in the operating system itself, one to add our application files, and a third to install packages for the Node.js environment that our application uses. Thankfully, Docker itself is very good about how it manages layers on an image, and if it is able to determine that a layer can be reused, it will pull that layer from the cache and use it instead of rebuilding it. So, as long as we start with the same base image and install the same software each time, Docker will be able to use that layer from the cache. So, our first layer can easily be loaded from the cache!\nHowever, each time we change any of the code in our application, we will end up having to rebuild the last two layers, and that can greatly slow down the process of building a new image. Thankfully, by changing our Dockerfile just a bit, as shown on this slide, we can take advantage of the caching capability of Docker to make subsequent builds much easier. In this case, we start by just copying the package.json and yarn.lock files to the image, and then run yarn install to install the libraries needed for our application. This will populate the node_modules folder, and it creates a new layer for our image. That layer can be cached, and as long as we don’t make any changes to either the package.json or yarn.lock files in our application, Docker will be able to reuse that cached layer!\nFrom there, we can copy the rest of our application files into the image and then we are good to go. However, since we’ve already installed our libraries, we don’t want to overwrite the node_modules folder. So, we can create a special file named .dockerignore to tell Docker to ignore various files when creating our image. It works the same way as a .gitignore file. So, we can just tell Docker to ignore the node_modules folder when it copies files into our image!\nTry it yourself! Build an image with this new Dockerfile, then make a change somewhere in the application itself and rebuild the image. You should now see that Docker is able to reuse the first two layers from the cache, and the build process will be much faster. So, by thinking a bit about the structure of our application and how we write our Dockerfile, we can make the build process much more efficient.\nDocker has some language-specific guides in their documentation that give Dockerfile examples for many common languages, so you can easily start with one of their guides whenever you want to build your own images. The Docker extension for Visual Studio Code also has some great features for building a Dockerfile based on an existing project.\nThat should give you everything you need to start building your own Docker images!\n",
    "description": "",
    "tags": null,
    "title": "Building Images",
    "uri": "/5a-containers/06-building/index.html"
  },
  {
    "content": " Note TODO The video below installs certbot from a PPA, but Ubuntu 22.04 includes certbot in the universe repositories. Refer to the instructions on the certbot website for updated instructions. –Russ\nYouTube Video Resources Slides Core Networking Services - Security Public Key Certificate from Wikipedia Certbot from the Electronic Frontier Foundation (EFF) How To Secure Apache with Let’s Encrypt on Ubuntu 22.04 from DigitalOcean Video Transcript So far, we’ve created a cloud server, a domain name, and connected the two so our users can access our cloud resources. However, we haven’t done anything to secure the connection between our users and our cloud system, which means that any information sent via HTTP to and from our server can be intercepted and read by a malicious third party. As you’ll recall, in Module 3 we showed how simple it is to do just that using Wireshark.\nTo secure our connections, we want to use HTTPS, which is simply HTTP using TLS to create a secured and encrypted tunnel. We’ve already discussed TLS a bit in Module 3, so feel free to refer to that video if you need a quick refresher of what TLS is and how it works.\nAs part of the TLS handshake, each system exchanges security information, such as a public key certificate, that is used to verify each other’s identity and construct a shared encryption key for the connection. It is very simple to create one of those certificates on your system, and then instruct your web server to use that certificate to create a secure connection. Those certificates are sometimes referred to as “SSL Certificates” or “TLS Certificates” as well.\nHowever, since you created that certificate yourself, it is known as a “self-signed” certificate, and really can’t be used to confirm your identity directly. For example, you could easily create a certificate that says this system is the server for amazon.com, but that wouldn’t be true. So, how can we use those certificates to confirm a system’s identity?\nThe solution is to use a “chain of trust” to verify the certificate. Each web browser and operating system has a set of root certificates installed which belong to several trusted entities, called certificate authorities, or CAs, from across the world. They, in turn, can issue intermediate certificates to others acting on their behalf. To validate that intermediate certificate, the root CA signs it using their own certificate. This signature can easily be verified to make sure it is genuine. Then, the intermediate certificates can be used to sign certificates for a website or cloud resource.\nWhen your web browser receives a certificate from a website, it can look at the signatures and verify the “chain of trust” that authenticates the certificate. In most web browsers, you can view this information by clicking on the lock icon next to the URL in the address bar. For example, the certificate for Wikipedia is currently signed by a certificate from “GlobalSign Organization Validation CA,” which in turn is signed by the “GlobalSign Root CA” certificate.\nIn essence, your web browser says “Well, I trust GlobalSign Root CA’s certificate, and they say they trust GlobalSign Organization Validation CA’s certificate, and the Wikipedia certificate was signed by that certificate, so it must be correct.”\nSo, as the administrator of resources in the cloud, it is in your best interest to not only use HTTPS to secure your web traffic, but you should also provide a valid certificate that has been signed by a trusted root CA. This helps ensure that your users can trust that the system they are connecting to is the correct one.\nUnfortunately, obtaining certificates could be expensive in the past, depending on your needs. Many times you would need to obtain these certificates through your domain registrar, or directly from a variety of certificate authorities on the internet. While those are still options, and in many cases for a large enterprise they are the best options, there is a better way for us to secure our websites.\nThe Internet Security Research Group created Let’s Encrypt, a free certificate authority that allows the owner of a domain name to request a security certificate free of charge. To make it even easier, the Electronic Frontier Foundation, or EFF, created Certbot, a free tool that helps you configure and secure your websites using Let’s Encrypt. So, let’s see how easy it is to do just that on our cloud server.\nI’m going to quickly walk through the DigitalOcean guide for using Let’s Encrypt on Apache, which is linked below this video in the resources section. Feel free to refer to that guide for additional information and discussion about this process.\nFirst, I’ll need to install Certbot by adding the appropriate APT repository and installing the package:\nsudo add-apt-repository ppa:certbot/certbot sudo apt install python-certbot-apache Next, I’ll need to make sure that the configuration file for my website is stored correctly and has the correct information. For example, if the website I would like to secure is foo.russfeld.me, my configuration file should be stored in /etc/apache2/sites-available/foo.russfeld.me.conf, and inside that file should be a line for ServerName foo.russfeld.me. Make sure that all three are correct before continuing.\nIn addition, you’ll need to make sure your firewall is configured to allow HTTPS traffic on port 443 through the firewall.\nFinally, you can use Certbot to request a certificate for your website. For my example site, the command would be this:\nsudo certbot --apache -d foo.russfeld.me If you’d like to create a certificate for more than one website, you can include them with additional -d flags at the end of this command.\nCertbot may ask you a few questions as it requests your security certificates, including your email address. At the end, it will ask you to choose if you’d like to have your site automatically redirected to HTTPS. I always recommend enabling this option, so that anyone who visits your site will be automatically secured.\nThat’s all there is to it! At this point, I recommend clearing the cache in your web browser to make sure it doesn’t have any cached information from that website. Then, you can visit your website in your browser, and it should automatically redirect you to the HTTPS protocol. You can then examine the certificate to make sure it is valid.\nThere is really no excuse in this day and age for having a website without a valid, authenticated security certificate, since it is so simple and easy to do. As a system administrator, you should strive to make sure every system you manage is properly secured, and this is one important part of that process.\n",
    "description": "",
    "tags": null,
    "title": "Certificates",
    "uri": "/5-the-cloud/06-certificates/index.html"
  },
  {
    "content": "Greetings! Here is the information you’ll need to know to get your lab graded.\nGrading Date/Time: \u003ctime\u003e Contact me ASAP if that time will not work for you for any reason and I will work with you to reschedule. Zoom Link: \u003clink\u003e If you haven’t used Zoom before, I recommend going to https://ksu.zoom.us/ first to download the Zoom client software. Then, when the time comes, click the link above to join the Zoom session. Your Setup: Before you join the Zoom session for grading, please have the following steps completed on your end. This helps things go as smoothly as possible: Use the fastest internet connection available to you. If you are using a laptop with a wireless connection, you may want to plug in to a hard-wired connection if available. If you can work from campus, that may be best, depending on your home internet connection. Make sure your audio input device (microphone) is working in Zoom. You are not required to have a webcam, but you are welcome to use one if you like. I will be using a webcam and microphone on my end. Confirm that all the VMs for this lab are booted and running on your system. We’ll be using the desktop sharing features of Zoom so I can see your VMs. So, make sure you close any windows or programs that are running in the background that shouldn’t be shared with me. Operations: Here are a few operations you may be asked to demonstrate: Connect remotely to Ubuntu via SSH Connect remotely to Windows via Remote Desktop Show Static IP Address Settings in Ubuntu DNS server settings: /etc/bind/named.conf.options, /etc/bind/named.conf.local and related zone files DNS server lookup test: dig ubuntu.cis527\u003cyour eID\u003e.cs.ksu.edu or nslookup ubuntu.cis527\u003cyour eID\u003e.cs.ksu.edu DNS server reverse test: dig -x xxx.xxx.xxx.41 or nslookup xxx.xxx.xxx.41 DHCP server settings: /etc/dhcp/dhcpd.conf DHCP server test: ipconfig /release and ipconfig /renew from Windows SNMP server test: snmp demonstration from Ubuntu Client SNMP screenshots (3) Wireshark screenshots (8) Finality Once you begin the grading process, no changes may be made to your system to fix any errors encountered. The grade you receive will reflect the state of your VMs at the start of the grading process. This is to ensure that grades are fairly determined and that no special consideration is given. That should cover everything. Please let me know if you have any questions prior to your scheduled grading time.\nGood luck!\nRuss\n",
    "description": "",
    "tags": null,
    "title": "Lab 3 Grading Email",
    "uri": "/z-instructor-resources/06-lab-3-grading-email/index.html"
  },
  {
    "content": " Puppet Learning VM Deprecated As of 2023, the Puppet Learning VM is no longer being maintained. The videos below demonstrate some of the features of Puppet, which can also be done on your Ubuntu VM after installing Puppet Agent. Unfortunately, it is not easily possible to simulate an enterprise Puppet setup without this VM, so I’ll keep these videos up for demonstration purposes. –Russ\nYouTube Video Resources Slides Configuration Management 101: Writing Puppet Manifests from DigitalOcean Language: Visual Index from Puppet Language: Basics from Puppet Video Script Now, let’s take a look at how we can use Puppet to create defined configurations to be applied to our systems. In Puppet, we refer to those defined configurations as manifest files. Each manifest file defines a set of resources to be configured on the system. Then, the Puppet Agent tool compares the configuration defined in the manifest file and the system’s current configuration, making changes as needed to the system until they match. In addition, each manifest file should be written in a way that it can be repeatedly applied to a system, since it is only defining the configuration desired and not the steps to accomplish it. This makes Puppet a very powerful tool for managing system configuration across a variety of systems and platforms.\nWhen you apply a manifest file, Puppet will go through a compilation process to convert the manifest file into a catalog. When working in a larger organization with a Puppet master server containing several manifest files, this allows the system to distill all of that information into a single place. We’ll talk a bit more about that when we look at a master/agent setup using Puppet.\nNow, let’s create our first Puppet Manifest files. Once again, I’m using the Learning Puppet VM during the hello_puppet quest, but you can follow along on your own Windows or Linux VM as well. I’ll use SSH to connect to the hello.puppet.vm node, then make sure the Puppet Agent is installed.\nTo make this process simpler, I’m also going to install the Nano text editor. For most beginners, I feel that Nano is the easiest command-line text editor to learn, though it may not be the most powerful. If you are familiar with Vim, feel free to use it as it is already installed on this system. Since the Learning Puppet VM is using CentOS as its Linux distribution of choice, the command to install packages is a bit different than on Ubuntu. To install Nano, type the following:\nsudo yum install nano Now, let’s open a new text file using Nano, called manifest.pp. In that file, we can define a file resource as follows:\nfile { '/tmp/testfile': ensure =\u003e 'file', content =\u003e 'Manifest File!', mode =\u003e '0644' } Once you have created that file, press CTRL+X, then Y, then ENTER to save and close the file in Nano. Then, you can apply the file using this command:\nsudo puppet apply manifest.pp It should successfully apply, and give you a message that the file was created. You can then use either\nsudo puppet resource file /tmp/testfile or\nls -l /tmp/testfile to verify that the file exists and has the correct permissions. You can also view the file’s contents using\ncat /tmp/testfile It’s that easy! Most of the attributes displayed when you use the puppet resource command are configurable in a manifest file, so you can use puppet resource to determine how a system is currently configured, then copy the desired attributes into a manifest file to define that configuration. Let’s add a few more things to our manifest:\npackage { 'nano': ensure =\u003e 'present' } user { 'cis527': ensure =\u003e 'present', shell =\u003e '/bin/zsh', home =\u003e '/home/cis527', managehome =\u003e true } Once you’ve edited a manifest file, you can verify that your syntax is correct using the following command:\npuppet parser validate manifest.pp If you have any errors in your manifest file, it will give you the approximate line and column number. Be careful about your use of colons, commas, quotes, and brackets. As with any programming language, Puppet is very particular about the correct use of syntax.\nNow, let’s apply that manifest and see what happens:\nsudo puppet apply manifest.pp Since the Nano package is already installed, it probably won’t do much at this point. However, it should create the cis527 user. We can verify that it worked by switching to that user account. First, we’ll need to set a password for it:\nsudo passwd cis527 Then, we can use that password and the switch user command to log in as that user:\nsu cis527 If all goes well, our terminal should change to show that we are logged in as the cis527 user. Of course, it is possible to define the desired password in the manifest file, but that is something you’ll have to figure out on your own as you complete Lab 2. (I can’t give everything away, can I?)\nBefore moving on to the next video, I encourage you to play around with this temporary manifest file a bit and see what other changes you can make to the resources we’ve defined, or what other resources you can use here.\n",
    "description": "",
    "tags": null,
    "title": "Puppet Manifest Files",
    "uri": "/2-configuration-management/06-puppet-manifest-files/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello, and welcome to the week six Announcements video for CIS 527 in summer 2022. So this week, you should be wrapping up lab five, which is due today at 7pm. So make sure you get that done. Most of lab grading is really simple. And we’ll talk about that in just a minute. You should also be wrapping up the week five discussion, which is also due tonight by midnight, so make sure you get that done. This week, you’ll be working on Week Six content. So the week six quizzes are due Friday. And then lab six and the week six discussions are due next Monday.\nSo for lab five grading today, the biggest thing I’m going to check is that you can SSH from your system to the front end. And that I can also use the grating SSH key to get into your front end. And then that we can SSH from your front end to your back end. We’ll also do things like checking the firewall, the date and time configuration, we’re going to check Apache DNS and make sure that HTTPS is working. So Apache is running on your back end. And then on your front end, you should have Docker with Docker reverse proxy setup. And so we should have three different websites we can access one on the back end using Apache and then two on the front end in Docker using some sort of reverse proxy, they should all be set up so that we can access them. So hopefully you’ve got all that working for lab five. If you’re having trouble, feel free to schedule a time with me, and I’d be happy to meet with you and get that checked out.\nSo the speaker that you’re going to look at this week is Sarah Allen. Sarah Allen is a system administrator from McCown-Gordon and a former CIS 527 student who’s now working in industry in System Administration. Her background is mainly in Help Desk and Frontline support for a large construction company. And so because of that, she deals with some unique challenges that a lot of other companies may not face. And her expertise is really interesting in that particular area of the field. So look forward to checking that video out of Sarah. And then of course, she’ll respond to a few prompts and ask a few questions of Sarah, at the end of that.\nSo this coming week, you’re going to work on lab six, I don’t have that posted right now. But I should have that posted by the end of today. Um, lab six is mainly focused on doing things around file servers, and drive mappings on Windows and Linux, we’re also going to look a little bit at application servers and web servers. And by the end of lab six, you’ll have a couple of working web apps. The plan is for lab six, you’ll do the windows stuff in your VMs. And you’ll also do the file server stuff and your VMs for Linux. And then in the cloud, you’re going to actually deploy some sort of a web app to Docker, I believe we’re going to do WordPress, but I need to look into that a little bit to make sure that that works the way I want it to before I actually post that lab.\nAlso, we’re getting toward the end of the semester. So now’s the time to start thinking of your final project. The goal for the final project is to build something new or fix something related to system administration. So you can think about systems you interact with on a daily basis that you’d like to change or something that you think you might want to build that has a system and registration component to it. Some things you could think of would be setting up a web resource for new startups. So how would you deploy their website? How would you host that? How would you maintain that? Automatically setting up a bunch of new laptops for school, designing some sort of central authentication and networking system for accompany discussing the differences between using thin clients and thick clients in a computer slab? The choice of antivirus clients the choice of using Chromebooks for web development. There’s lots of things that you can look at related to system administration. If you’re not sure you can chat with me for ideas. The biggest thing for this final project is to come up with something, propose it and then do a SWOT analysis of your proposal. And so I’m really curious about the SWOT analysis more than anything, where you look at the strengths, weaknesses, opportunities and threats. I will admit, in years past, most students have spent a lot of time on the design of the final projects, and then not a lot of time on the SWOT analysis. But the SWOT analysis is really more of the points of your final project. So make sure you leave plenty of time to do a deep analysis on it. And really try and present to me the pros and cons and show that you’ve thought through this very quickly.\nDon’t forget, if you need it, you can get the GitHub education pack, you can go to try digitalocean.com/freetrialoffer or go to nc.me to get access to DigitalOcean Name Cheap for super cheap, if not free. I also have some credits and both of those so if you need any help getting any of the resources for this class, just let me know.\nOtherwise, feel free to keep in touch. I still have discussions on Discord I host tea time every Tuesday at 330 and Friday 1030 It’s a great time to just hang out and chat while you work on stuff. You can also schedule a one on one office hours with me using my Calendly link that’s in my email signature in the syllabus. I’m always available, so don’t be afraid to ask me questions. Other than that we’re getting near the end. We’re on Week Six of eight. So just two more weeks to go. Make sure you keep up on the labs make sure you start thinking about your final project. If you have any questions, let me know otherwise. I look forward to seeing you again next week.\n",
    "description": "",
    "tags": null,
    "title": "Summer '22 Week 6",
    "uri": "/y-announcements/old/summer2022/week06/index.html"
  },
  {
    "content": " YouTube Video Resources File Server from Ubuntu Documentation Samba Server Guide from Ubuntu Community Help Wiki How to Set Up a Samba Share for a Small Organization on Ubuntu 16.04 from DigitalOcean (works for 20.04) Access User’s Home Folders via Samba on Ubuntu 17.04 from Website for Students (works for 20.04) Mount Windows Shares Permanently from Ubuntu Wiki How Do I Access Windows Shares from Bash on Ask Ubuntu Forums Use of pam-mount to Mount Home from Server on Ubuntu Forums Video Transcript In this video, I’ll walk you through the steps to set up a file server on Ubuntu using Samba. You’ll learn how to share a particular folder, and I’ll discuss what it takes to share the home folders for each user as well. Finally, I’ll show you a bit of the process for making those shared folders available to your users on an Ubuntu client.\nAs before, I’ll continue to use the VMs from Lab 4 for this example. Let’s start on the server VM. First, if you haven’t already, you’ll need to install the Samba server:\nsudo apt update sudo apt install samba Next, I’ll need to create a folder to share, as well as a file in that folder just so it isn’t empty:\nsudo mkdir /shared sudo touch /shared/file.txt Finally, since that folder should be accessible to everyone, I’ll set the permissions on that folder accordingly:\nsudo chmod -R 777 /shared Now that I have created my folder, I can work with Samba to share it with users on the network. There are a couple of different ways to accomplish this task. First, just like in Windows, you can right-click on the folder in Nautilus and access the sharing options there. For many users, this is the quickest and easiest way to share a folder on the network. However, for this example I’ll show you how to share the folder directly in the Samba configuration file, which will give you a bit more control over the configuration.\nSo, let’s open the Samba configuration file in Nano:\nsudo nano /etc/samba/smb.conf As you scroll through that file, you’ll see that there are many different settings that you can customize on your Samba server. Pay special attention to the section for sharing user home directories, as that will be very useful as you complete the lab assignment later.\nTo add a new share, I’m going to add a few lines at the bottom of the file:\n[shared] comment = Shared Files path = /shared browseable = yes guest ok = no read only = no create mask = 0755 Once you’ve made your edits, you should test your configuration file’s syntax:\ntestparm If everything looks as it should, then you can restart the Samba server to enable your new configuration:\nsudo systemctl restart smbd At this point, you may also need to adjust your firewall configuration to allow other systems on the network to access these shared resources. For this example, I have disabled my firewall for simplicity, but you’ll need to make sure it is configured properly to receive full credit on your lab assignment.\nFinally, we need to add users to our Samba server so they can access the resource remotely. Samba maintains a separate database of users since it cannot directly decrypt the password hashes stored in /etc/shadow, so it takes an extra step to enable our existing users to access shared resources via Samba. It is possible to configure Samba to use OpenLDAP for authentication, but the process is quite complex. I chose to leave that out of this exercise, but in an enterprise setting you may choose to do so.\nTo create and enable the cis527 user in Samba, you can use the following commands:\nsudo smbpasswd -a cis527 sudo smbpasswd -e cis527 The first command will ask you to enter a password for this user. This password can be different from the user’s password on the system. However, I recommend setting this to be the same password that the user would use to login via LDAP or locally on their system. This will allow the system to automatically mount that user’s home folder shared from Samba, as we’ll see later in this video.\nNow, let’s switch over to our Ubuntu client and see how to access a shared folder. First, in Nautilus, you can click the Other Locations option on the left to see the available servers on the network. If network discovery is working properly, you may see your Ubuntu server listed there and be able to click on it.\nIf not, you can search for the server just like you would on Windows. At the bottom of Nautilus, there should be a Connect to Server box. In that box, you would enter smb:// followed by the name or IP address of the system you’d like to connect to. So, in my example network, I would enter smb://192.168.40.41 to find my server.\nOnce you can see the shares on the server, you can double-click a shared folder to mount it on your system. It should pop up a window asking for a username and password. If your shared folder supports guest access, you can select the “Anonymous” option at the top to open the folder as a guest user. Otherwise, choose the “Registered User” option and enter your Samba username and password in the appropriate boxes. You can generally leave the Domain box alone unless you have a specific domain or workgroup configured on your Samba server.\nAlso, note that if you enter the incorrect username and/or password, you might get a strange error stating “Failed to mount Windows share: File Exists.” This error simply means that it was unable to access the shared resource, and most likely it is due to an authentication error. It isn’t really a helpful error message, is it?\nOnce you’ve connected to the shared folder, you should see a shortcut to that folder appear on your Ubuntu desktop. You can also access that folder via Terminal, but it is quite buried. The folder is typically mounted in /run/user/\u003cUID\u003e/gvfs/ where \u003cUID\u003e is the numerical user ID of your user on Ubuntu.\nThankfully, there are a few other ways to mount these shared folders on your system. First, using the Terminal, you’ll need to install the cifs-utils package:\nsudo apt update sudo apt install cifs-utils Then, you can mount that shared folder to the location of your choosing. I recommend first creating an empty folder in /mnt to act as a mount point:\nsudo mkdir /mnt/shared Then, you can use the mount command to mount the shared folder as a drive. For my example setup, I would use this command:\nsudo mount -t cifs -o username=cis527,dir_mode=0777,file_mode=0777 //192.168.40.41/shared /mnt/shared Of course, you’ll have to adjust the command to fit your setup. Now, I can access those shared files at /mnt/shared in Terminal as well.\nOnce I am finished, I can unmount that share using the Terminal as well:\nsudo umount /mnt/shared To further automate this process, you can add an entry to the /etc/fstab file that gives the details for a shared folder that should be mounted automatically for each user on the system. You’ll do just that for your lab assignment, so I won’t cover the specific details here. As long as you are able to mount it using the commands above, it should be pretty straight-forward to adapt those settings to work in the /etc/fstab file.\nFinally, you can also use libpam-mount to automatically mount drives on a per-user basis at login. This is especially useful if you want to automatically mount a user’s home folder from a Samba server directly into their own home folder locally. To start, you’ll need to install that library:\nsudo apt update sudo apt install libpam-mount Next, you can configure it by editing its configuration file:\nsudo nano /etc/security/pam_mount.conf.xml As you look through that file, you’ll see quite a few default options already in place. Unfortunately, since the file is in an XML format, it is a bit difficult to read. You’ll need to make a couple of changes. First, look for the entry:\n\u003cdebug enable=\"0\" /\u003e and change it to\n\u003cdebug enable=\"1\" /\u003e to enable debugging. By doing so, you’ll be able to see output in /var/log/syslog if this process doesn’t work, and hopefully you’ll be able to diagnose the error using that information.\nNext, look for the line:\n\u003c!-- Volume definitions --\u003e and, right below that line, you’ll add a line to define the shared folder you’d like to mount automatically. For my example setup, I would use the following definition:\n\u003cvolume fstype=\"cifs\" server=\"192.168.40.41\" path=\"homes\" mountpoint=\"/home/%(USER)/server\" /\u003e You’ll have to adjust the options in that line to match your particular environment. Once you’ve added your information, save and close the file.\nNext, we’ll need to make sure that the system is configured to use that module. To do so, examine the common-session configuration file:\ncat /etc/pam.d/common-session and look for this line in that file:\nsession optional pam_mount.so If it isn’t there, you’ll need to add that line to the bottom of that file.\nThat should do it! To test your configuration, simply log out and log in again. If it works, you should now be able to see that user’s home folder from the Samba server in the server folder in the home directory. It should also create shortcut on the Ubuntu desktop as well.\nIf it doesn’t work, you’ll want to review any error messages in the system log, The easiest way to find them is to search the system log for mount using the following command:\ncat /var/log/syslog | grep mount For the lab assignment, you’ll perform these steps for your environment in much the same way. It can be very tricky to get this working the first time, so be very careful as you edit these configuration files. If you aren’t able to get it working, please post in the course discussion forums on Canvas to get assistance.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu File Server",
    "uri": "/6-application-servers/06-ubuntu-file-server/index.html"
  },
  {
    "content": " YouTube Video Resources Getting Started with Windows PowerShell from Microsoft PowerShell Documentation from Microsoft Sample PowerShell Scripts from Microsoft PowerShell Community Blog from Microsoft Hey, Scripting Guy! Blog from Microsoft (older content) Video Script Before digging too deep into Windows, let’s take some time to learn about one of its most powerful, and least used, features, the PowerShell command line interface. PowerShell is an update to the old Command Prompt terminal, which was a very limited version of the DOS terminal from decades ago.\nPowerShell is built on top of the .NET framework, and can leverage many features of that programming environment to perform powerful tasks within a simple interface. While we won’t go too deep into PowerShell in this class, it is a very useful thing to learn.\nOn your Windows 10 Virtual Machine, you can open PowerShell by searching for it on the Start Menu.\nThe PowerShell interface is very simple, with just a small terminal that you can use to enter commands. Most commands take the form of a verb, followed by a hyphen, and then a noun. Let’s try a few to see how they work:\nGet-Location - get the current directory Set-Location \u003cpath\u003e - change directory Get-ChildItem - list files New-Item \u003cname\u003e (-ItemType \u003c\"file\" | \"directory\"\u003e -Value \u003cvalue\u003e) - make a new directory or file Get-Content \u003cname\u003e - get file contents Remove-Item \u003cname\u003e - remove a directory or file Copy-Item \u003cname\u003e - copy a directory or file Move-Item \u003cname\u003e - move a directory or file Select-String -Pattern \u003cpattern\u003e -Path \u003cpath\u003e - search for a string in a file or group of files Get-Command - get a list of commands Get-Help - get help for a command Get-Alias - get a list of aliases for a command Out-File - output to a file If you want to use the output of one command as input to another command, you can use the vertical pipe character | between commands. Here are some examples.\nSelect-String - search for a string in output Measure-Object \u003c-Line | -Character | -Word\u003e - get line, word or character count Sort-Object - sort output by a property Where-Object - filter output by a property You can also use PowerShell to write and execute scripts of commands. It is very similar to writing code in a programming language, but it is outside of the scope of this class. If you are interested in learning more, there are many great resources linked below the video to get you started. You can also check out the crash-course on PowerShell scripting in the Extras module.\n",
    "description": "",
    "tags": null,
    "title": "Windows 10 PowerShell",
    "uri": "/1-secure-workstations/06-windows-powershell/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Group Policy on Wikipedia Group Policy Processing and Precedence from Microsoft Docs Group Policy Fundamentals in Active Directory by Troy Thompson in Redmond Magazine Tutorial: The Joys of Windows Server’s Group Policies by J. Peter Bruzzese on InfoWorld What is Group Policy in Windows from How-To Geek The 10 Windows Group Policy Settings you Need to Get Right from CSO Online In the Trenches: Eight Tips-n-Tricks for Microsoft Windows Group Policy by Mark Mizrahi on Global Knowledge Top 10 Most Important Group Policy Settings for Preventing Security Breaches by Abhishek Rai on Lepide Blog Group Policy Best Practices by Robert Allen on Active Directory Pro Optimizing Group Policy Performance by Darren Mar-Elia on Microsoft TechNet Group Policy Design Best Practices by Darren Mar-Elia on ITPro Today 10 Windows Group Policy Settings you Need to Tweak by Benjamin Roussey on TechGenix Video Transcript One of the major features of Microsoft’s Active Directory is the ability to use Group Policy Objects, or GPOs, to enforce specific security settings on any Windows client added to the domain. For large enterprises, this is a major tool in their arsenal to ensure that Windows PCs are properly configured and secured in their organization.\nGroup Policy can be set at both the local computer level, as well as via Active Directory. The settings themselves are hierarchical in nature, so you can easily browse through related settings when using the Group Policy Editor tool. In addition, GPOs are inheritable, and multiple GPOs can be applied to a group or OU in a domain.\nWhen group policies are applied, they follow this order. First, any policies set locally on the machine are applied, then policies from the Active Directory site, then the global domain itself, and finally any policies applied to the OU containing this computer or user are applied. At each level, any previous settings can be overridden, so in essence, the policies applied at the OU level take precedence over others.\nSome examples of things you can control via Group Policy are shown here. You can create mapped drives, which we will see in a later module. You can also set power options, which could be used in an organization to save money by reducing energy usage. In addition, you can use group policy to enforce password restrictions, and even install printers and printer drivers as well.\nLet’s take a look at one quick demonstration, setting a password policy for the domain. This tutorial is from Infoworld, and is linked in the resources section below the video.\nHere I have opened my Windows 2016 Server VM as configured for Lab 4. First, I’ll open the Group Policy Management Console, which is available on the Tools menu in Windows Server Manager. Next, I’ll find my domain, and right-click it to create a GPO in this domain. I’ll also give the GPO a helpful name, and click OK to create it.\nNext, I can right-click the policy and choose Edit. In the policy editor, I’ll need to dig down the following path:\n\u003cname\u003e\\Computer Configuration\\Policies\\Windows Settings\\Security Settings\\Account Policies\\Password Policy In that window, we’ll right-click the “Password Must Meet Complexity Requirements” option, and choose Properties. Then, we’ll enable the setting by checkmarking the box and clicking Enable. You can find more information about the policy by clicking the Explain tab at the top.\nFinally, once the policy is configured, you’ll need to right-click on it once again, and select the Enforced option to enforce it on the domain itself.\nThat’s all there is to it! Now, any new password created on this domain must meet those complexity requirements. While you won’t have to work with Group Policy for this lab assignment, we’ll come back to it in a later module as we work with application and file servers.\n",
    "description": "",
    "tags": null,
    "title": "Windows Group Policy",
    "uri": "/4-directory-services/06-windows-group-policy/index.html"
  },
  {
    "content": " YouTube Video Resources Sysinternals Suite from Microsoft 11 Useful Tools for Windows SysAdmins from Anturis Blog Best System Monitoring Tools for Windows \u0026 Linux (Free \u0026 Paid) from Comparitech 20 Top Windows SysAdmin Tools You Should Know from Power Admin The Top 20 Free Network Monitoring and Analysis Tools for SysAdmins from GFI Tech Talk Top System Monitoring Tools for Windows Environments from PCWDLD Video Transcript In this video, I’m going to review some of the tools you can use to help monitor the health and performance of your Windows systems. While some of these tools support collecting data remotely, most of them are designed for use directly on the systems themselves. There are many paid products that support remote data collection, as well as a few free ones as well. However, since each one is unique, I won’t be covering them directly here. If you’d like to use such a system, I encourage you to review the options available and choose the best one for your environment.\nFirst, of course, is the built-in Windows Task Manager. You can easily access it by right-clicking on the taskbar and choosing it from the menu, or, as always, by pressing CTRL+ALT+DELETE. When you first open the Task Manager on a system, you may have to click a button to show the more details. The Task Manager gives a very concise overview of your system, listing all of the running processes, providing graphs of system performance and resource usage, showing system services and startup tasks, and even displaying all users currently logged-in to the system. It serves as a great first tool to give you some quick insight into how your system is running and if there are any major issues.\nFrom the Performance tab of the Task Manager, you can access the Resource Monitor. This tool gives more in-depth details about a system’s resources, showing the CPU, memory, disk, and network usage graphs, as well as which processes are using those resources. Combined with the Task Manager, this tool will help you discover any processes that are consuming a large amount of system resources, helping you diagnose performance issues quickly.\nThe Windows Performance Monitor, which can be found by searching for it via the Start Menu, also gives access to much of the same information as the Resource Monitor. It also includes the ability to produce graphs, log files, and remotely diagnose another computer. While I’ve found that it isn’t quite as useful as the Resource Monitor, it is yet another tool that could be used to help diagnose performance issues.\nFinally, the Windows Reliability Monitor, found on the Control Panel as well as via the Start Menu, provides a combined look at a variety of data sources from your system. Each day, it rates your system’s reliability on a scale of 1 to 10, and gives you a quick look at pertinent log entries related to the most recent issues on your system. This is a quick way to correlate related log entries together and possibly determine the source of any system reliability issues.\nThe Sysinternals Suite of tools available from Microsoft also includes several useful tools for diagnosing and repairing system issues. While I encourage you to become familiar with the entire suite of tools, there are four tools in particular I’d like to review here.\nFirst is Process Explorer. It displays information about running processes on your system, much like the Task Manager. However, it provides much more information about each process, including the threads, TCP ports, and more. Also, Process Explorer includes an option to replace Task Manager with itself, giving you quick and easy access to this tool.\nNext is Process Monitor. We’ve already discussed this tool way back in Module 1. It gives you a deep view into everything your Windows operating system is doing, from opening files to editing individual registry keys. To my knowledge, no other tool gives you as much information about what your system is doing, and by poring over the logged information in Process Monitor, you can discover exactly what a process is doing when it causes issues on your system.\nAnother tool we’ve already discussed is TCPView, which allows you to view all TCP ports open on your system, as well as any connected sockets representing ongoing network connections on your system. If a program is trying to access the network, this tool is one that will help you figure out what is actually happening.\nLastly, Sysinternals includes a tool called Autoruns, which allows you to view all of the programs and scripts that run each time your system is started. This includes startup programs in the Start Menu, as well as programs hidden in the depths of the Windows Registry. I’ve found this tool to be particularly helpful when a pesky program keeps starting with your system, or if a malicious program constantly tries to reinstall itself because another hidden program is watching it.\nWindows also maintains a set of log files that keep track of system events and issues, which you can easily review as part of your monitoring routine. The Windows logs can be found by right-clicking on the Start Button, selecting Computer Management, then expanding the Event Viewer entry in the left-hand menu, and finally selecting the Windows Logs option there. Unlike log files in Ubuntu, which are primarily text-based, the Windows log files are presented in a much more advanced format here. There are a few logs available here, and they give quite a bit of useful information about your system. If you are fighting a particularly difficult issue with Windows, the log files might contain useful information to help you diagnose the problem. In addition, there are several tools available to export these log files as text, XML, or even import them into an external system.\nFinally, you can also install Wireshark on Windows to capture network packets, just as we did on Ubuntu in Module 3. Wireshark is a very powerful cross-platform tool for diagnosing issues on your network.\nThis is just a quick overview of the wide array of tools that are available on Windows to monitor the health and performance of your system. Hopefully you’ll find some of them useful to you as you continue to work with Windows systems, and I encourage you to search online for even more tools that you can use in your work.\n",
    "description": "",
    "tags": null,
    "title": "Windows Monitoring",
    "uri": "/7-backups-monitoring-devops/06-windows-monitoring/index.html"
  },
  {
    "content": " YouTube Video Resources How to Assign a Static IP Address in Windows 7, 8, 10, XP, or Vista from How-To Geek How to Set a Static IP Address on Windows 10 from Pureinfotech 4 Common Windows Network Utilities Explained from Make Tech Easier TCPView from Windows Sysinternals Video Transcript Now, let’s look at how to manage and configure a network connection in Windows 10.\nTo begin, I’m working in the Windows 10 VM I created for Lab 2, with the Puppet Manifest files applied.\nFirst, let’s take a quick look at how our networking is configured in VMware. This will be very important as you complete Lab 3. To view the virtual networks in VMware Workstation, click the Edit menu, then choose Virtual Network Editor. On VMware Fusion, you can find this by going to the VMware Fusion menu, selecting Preferences, then the Network option.\nHere, we can see the virtual networks available on your system. Right now, there are two networks on my system, one “Host-only” network, and one “NAT” network. For this lab, we’ll be working with the “NAT” network, so let’s select it.\nFirst, let’s look at the network type, listed here. For this module, it is very important to confirm that the network type is set to “NAT” and not “Bridged.” If you use a “Bridged” network for this lab, you could easily break the network that your host computer is connected to, and in the worst case earn yourself a visit from K-State IT staff as they try to diagnose the problem. So, make sure it is set correctly here!\nWe can also see lots of information about the network’s settings. For example, we can see the subnet IP here, and the subnet mask here. By clicking on the NAT Settings button, I can also find the gateway IP. The gateway IP is the IP of the router, which tells your system where to direct outgoing internet traffic. You’ll want to make a note of all three of those, as we’ll need them later to set a static IP in Windows. You can also click the DHCP Settings button to view the settings for the DHCP server, including the range of IP addresses it uses, which can also be very helpful. If you want to change any of these settings, you can click the Change Settings button at the bottom. You’ll need Administrator privileges to make any changes.\nNext, let’s confirm that our VM is using that virtual network. To do so, click the VM menu, then select Settings, then choose the Network Adapter. Make sure the network connection is set to “NAT” here as well.\nOk, now let’s look at the network configuration in Windows 10. First, you can see information about the available network adapters in the Device Manager. You can access that by right-clicking on the Start button and choosing Device Manager from the list. On that window, expand the section for Network Adapters. Here, you’ll see that this VM has a single network adapter, as well as a couple of Bluetooth devices available.\nNext, let’s look at the network settings for our network adapter. You can find these by right-clicking the Start button once again, and choosing Network Connections, or by right-clicking the Network icon in the notification area and selecting Open Network \u0026 Internet Settings. This will bring you to the Network Status window in the Settings app. While you can find quite a bit of information about your network settings here, I’ve found it is much easier to click the option below for Change adapter options to get to the classic Network Connections menu on the Control Panel.\nHere, I can see all of the available network adapters on my system, as well as a bit of information about each one. Let’s right-click on the Ethernet0 adapter, and select Status. This window will show the current connection status, as well as some basic statistics. You can click the Details button to view even more information, such as your IP address, MAC address, and more.\nIf you’d like to set a static IP address for this system, you’ll need to click the Properties button at the bottom, then select the Internet Protocol Version 4 (TCP/IPv4) option, and finally Properties below that. On this window, you can set a static IP address for your system. To do so, I’ll have to enter an IP address, subnet mask, and default gateway. For the IP address, I’ll just make sure that it isn’t in use on the network by picking one outside the DHCP range used by VMware. The subnet mask and default gateway should be the same as the ones you found in the VMware network settings earlier. Finally, we’ll need to enter some DNS servers. Typically, you can just enter the same IP address as your default gateway, as most routers also can act as DNS resolvers as well. You can also use other DNS servers, such as those from OpenDNS or Google, as described in the Lab 3 assignment. Finally, I’ll click OK to save and apply those settings. If everything is successful, I should still be able to access the internet. Let’s open a web browser, just to be sure. For this lab assignment, you won’t be setting a static IP on Windows 10, but you will use this process in the next module when you configure your first Windows server. The process is very similar.\nWindows includes a number of tools to help troubleshoot and diagnose issues with your network connection. First off, the network troubleshooter available in the Network \u0026 Internet Settings menu is pretty good, and can help you figure out many simple issues with your network connection.\nBeyond that, there are a few command-line tools that you should be familiar with on Windows. So, let’s open a PowerShell window. The first command to use is ipconfig. This tool has been available in Windows since the earliest versions, and it can give you quite a bit of information about your network connection. Running it without any additional options will give you your IP address, subnet mask, default gateway, and other basic information for each of your network adapters.\nYou can also run ipconfig /all to see all the available information about all network adapters on your system. It gives quite a bit more information, including your MAC address and DHCP lease information.\nThat command also allows you to manage your DHCP client. For example, you can use ipconfig /release to release all DHCP addresses, then ipconfig /renew to request a new DHCP address. This is very handy if you have recently reset or reconfigured your network router, as you can tell Windows to just request a new IP address without having to reboot the system.\nIt can also help manage your DNS cache. Windows maintains a cache of all DNS requests and the responses you receive, so that multiple requests for the same DNS name can be quickly resolved without needing to query again. You can use ipconfig /displaydns to view the cached DNS entries, and ipconfig /flushdns to clear the cache. This is very handy when you are trying to diagnose issues with DNS on your system. Of course, DNS caching could create a privacy concern, as the DNS cache will contain information about all websites you’ve visited on this system. In fact, some anti-cheat programs for online video games have been found to check the Windows DNS cache, looking for entries from programs known to interfere with their games.\nFinally, Windows includes a couple of really handy troubleshooting tools. First, you can use the ping command to send a simple message to any server on the internet. It uses the Internet Control Message Protocol, or ICMP, which allows it to send a simple “echo” request to the server. Most servers will respond to that request, allowing you to confirm that you are able to communicate with it properly across the internet. While that may seem like a very simple tool, it can actually be used in very powerful ways to diagnose a troublesome internet connection. Similarly, the tracert command will use a series of ICMP “echo” messages to trace the route across the internet from your computer to any other system. See the video on troubleshooting in this module for more information on how to troubleshoot connections using these tools.\nThe Windows Sysinternals suite of tools also includes one helpful tool, called TCPView, which allows you to view all of the active TCP connections on your system. This will show all open ports as well as any established connections. As you are working with networked programs, you can use TCPView to get a good idea of what connections are happening on your system. It can also help you diagnose some problems with programs and your firewall configuration. You can also use the netstat command in PowerShell to find similar information, but I prefer this graphical view.\nThat’s all for configuring Windows networking. Stay tuned for information about configuring networking in Ubuntu!\n",
    "description": "",
    "tags": null,
    "title": "Windows Network Configuration",
    "uri": "/3-core-networking-services/06-windows-network-configuration/index.html"
  },
  {
    "content": " YouTube Video Resources Windows 10 WSL Installation Guide from Microsoft Windows Subsystem for Linux Documentation from Microsoft Video Transcript In this video, I will briefly introduce the Windows Subsystem for Linux, or WSL, which was one of the most highly anticipated features added to Windows 10 in the last couple of years. WSL allows you to install a full Linux distribution right inside your Windows 10 OS, giving you terminal access to some of your favorite Linux programs and tools. You can even run services such as sshd, MySQL, Apache, and more directly in WSL.\nFirst, you must enable the feature on Windows. The simplest way to do this is to open a PowerShell window using the Run as Administrator option, then enter the following command:\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux Once it is complete, you will need to reboot your computer to enable to feature.\nOnce you have rebooted, you can go to the Windows Store and install the distribution of your choice. There are many to choose from, and you can even have multiple distributions installed at the same time. For this example, I’ll choose Ubuntu.\nOnce it has finished installing, you must open it once to complete the initialization. During that process, you’ll be asked to create a username and password for your Linux system. It doesn’t have to be the same as your Windows user account, and it will not be synchronized with your Windows password either.\nAs soon as it is complete, you’ll be taken to your Linux distribution’s shell. Pretty neat, right?\nSo, what can you do from here? Pretty much anything! Here are a few things I suggest doing right off the bat.\nFirst, you can update your system just like any other Linux system:\nsudo apt update sudo apt upgrade You can also set up SSH keys so you can use SSH from WSL instead of dealing with PuTTY or other Windows-based terminal programs. Refer to the Extra - SSH page for more information on how to do that.\nOn WSL, you can find your normal Windows drives at /mnt by doing:\nls /mnt To get easy access to your Windows files, you can add a symbolic link to your Windows user folder, or any folder you choose. For example, if your Windows username is cis527 you can use the following command to add a shortcut in your Windows home folder within your WSL home folder:\nln -s /mnt/c/Users/cis527/ ~/cis527 You can verify that it worked using this command:\nls -l I’ve used this to create a few useful shortcuts to allow me to easily access my Windows files.\nFinally, since I am a big fan of using Git on command line, I usually install Git on my WSL to allow me to easily manage my repositories just like I normally do on my Linux based development systems.\nI hope you enjoy working with WSL as much as I have. If you have any suggestions of cool uses for WSL that I missed, feel free to send them to me. You just might get some extra credit points and see your idea featured here in a future semester!\n",
    "description": "",
    "tags": null,
    "title": "Windows Subsystem for Linux",
    "uri": "/x-extras/06-windows-subsystem-for-linux/index.html"
  },
  {
    "content": "All the fun of virtual machines without all the extra baggage!\n",
    "description": "",
    "tags": null,
    "title": "Containers",
    "uri": "/5a-containers/index.html"
  },
  {
    "content": "Putting your resources to work.\n",
    "description": "",
    "tags": null,
    "title": "Application Servers",
    "uri": "/6-application-servers/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Docker Tutorial for Beginners by TechWorld with Nana on YouTube Docker in Visual Studio Code from Visual Studio Code Developing Inside a Container from Visual Studio Code Remote Development in Containers Tutorial from Visual Studio Code GitHub CI Example (the sample site for this textbook’s theme) GitLab CI Example (this textbook) Video Transcript Docker is obviously a very useful tool for system administration, as we’ve seen throughout this module. In this lesson, however, let’s take a minute to talk about how we can use Docker in the context of software development. Depending on the application we are developing and how it will eventually be packaged and deployed, Docker can be used at nearly every stage of the development process.\nFirst, for applications that may be packaged inside of a Docker image and deployed within a container, we may wish to create a Dockerfile and include that as part of our source code. This makes it easy for us to use Docker to run unit tests in a CI/CD pipeline, and we can even directly deploy the finalized image to a registry once it passes all of our tests.\nThis slide shows a sample Dockerfile for a Python application written using the Flask web framework. It is very similar to the Node.js example we saw earlier in this module. By including this file in our code, anyone can build an image that includes our application quickly and easily.\nAnother great use of Docker in software development is providing a docker-compose.yml file in our source code that can be used to quickly build and deploy an environment that contains all of the services that our application depends on. For example, the Docker Compose file shown in this slide can be included along with any web application that uses MySQL as a database. This will quickly create both a MySQL server as well as a PHPMyAdmin instance that can make managing the MySQL server quick and easy. So, instead of having to install and manage our own database server in a development environment, we can simply launch a couple of containers in Docker.\nIn addition, this file can be used within CI/CD pipelines to define a set of services required for unit testing - yet another way we can use the power of Docker in our development processes.\nA more advanced way to use Docker when developing software is to actually perform all of the development work directly within a Docker container. For example, if we are using a host system that has one version of a software installed, but we need to develop for another version, we can set up a Docker container that includes the software versions we need, and then do all of our development and testing directly within the container. In this way, we are effectively treating it just like a virtual machine, and we can easily configure multiple containers for various environments quickly and easily.\nThe Visual Studio Code remote extension makes it easy to attach to a Docker container and run all of the typical development tools directly within a container. For more information about how that works, see the links in the resources section at the top of this page.\nFinally, as I’ve alluded to many times already, we can make use of Docker in our continuous integration and continuous delivery, or CI/CD, pipelines. Both GitHub and GitLab provide ways to automate the building and testing of our software, and one of the many tasks that can be performed is automatically creating and uploading a Docker image to a registry. This slide shows an example of what this looks like using GitHub actions. All that is required is a valid Dockerfile stored in the repository, and this action will do the rest.\nSimilarly, GitLab allows us to create runners that can automate various processes in our repositories. So, this slide shows the same basic idea in a GitLab pipeline. In fact, this pipeline will be run within a Docker container itself, so we are effectively running Docker in Docker to create and push our new Docker image to a registry.\nAt the top of this page are links to a couple of sample repositories that contain CI/CD pipelines configured in both GitHub and GitLab. In each repository, the pipeline will create a Docker image and publish it to the registry attached to the repository, so we can easily pull that image into a local Docker installation and run it to create a container.\nThis is just a small sample of how we can use Docker as a software developer. By doing so, we can easily work with a variety of different services, automate testing and packaging of our software, and make it easy for anyone else to use and deploy our applications within a container. So, I highly encourage you to consider integrating Docker into your next development project.\n",
    "description": "",
    "tags": null,
    "title": "Developing with Containers",
    "uri": "/5a-containers/07-developing/index.html"
  },
  {
    "content": " YouTube Video Resources Resources to Learn Git from GitHub Git Tutorial from Git Documentation Git Homepage Become a Git Guru from Atlassian Git Tutorial from Tutorialspoint Git Tutorial Slides by Russell Feldhausen GitHub Desktop from GitHub Git Kraken from Axosoft Gitignore Documentation from Git Documentation Gitignore Templates from GitHub Video Transcript This video provides a brief introduction to the Git source control program used by many system administrators. While this video is not intended to provide a full introduction to Git as a programmer would use it, it should serve as a quick introduction to the ways that system administrators might come across Git in their workloads.\nFor this example, I’ll be working with the command-line git program. There are many graphical programs that can be used to interact with Git as well, such as GitHub Desktop and Git Krakken\nIf you haven’t already, you’ll need to install the Git command-line tool. You can do so using apt:\nsudo apt update sudo apt install git Also, if you have not used Git on this system, you’ll want to configure your name and email address used when you create commits:\ngit config --global user.name \"Your Name\" git config --global user.email username@emailprovider.tld Now, let’s create a new Git repository on our computer. To do that, I’m going to create a new folder called git-test and add a couple of files to it:\nmkdir ~/git-test cd ~/git-test echo \"File 1\" \u003e file1.txt echo \"File 2\" \u003e file2.txt Next, let’s initialize an empty Git repository in this folder. This is the first step to adding an existing folder to Git. You can also use this command in an empty folder before you begin adding content.\ngit init Once you’ve done that command, let’s look at the directory and show all the hidden files:\nls -al As you can see, there is now a folder .git in this directory. That folder contains the information for this Git repository, including all of the history and changes. DO NOT MODIFY OR DELETE THIS FOLDER! If you change this folder, it may break your Git repository.\nTo see the status of a Git repository, you can use the git status command:\ngit status Here you’ll see that there are two files that are not included in the repository, which are the files we added earlier. To add a file to a Git repository, there are three steps. First, you must create or modify the file on the filesystem inside a git repository, which will cause Git to see the file as modified or “dirty.” Then, any changes you’d like to add to the Git repository will need to be “staged” using the git add command. Finally, those changes which are staged are “committed” using the git commit command.\nSo, to perform this process on the two files currently in our directory, we’ll do this command to stage them:\ngit add file1.txt file2.txt Once they are staged, we can check the status to make sure they are ready to commit:\ngit status If everything looks correct, we can commit those changes:\ngit commit -m \"First Commit\" The -m flag on the git commit command allows us to specify a commit message on the command line. If you omit that option, you’ll be taken to your system’s default text editor, usually Nano or Vim, and be able to write a commit message there. Once you are done, just save and close the file as you normally would, and the git command will proceed to commit the changes.\nFinally, once you have committed your changes, you can review the status of your repository again:\ngit status You can also review the Git log to see a log of your commits:\ngit log Now that we’ve created a Git repository and added our first commit, let’s talk about remotes. A remote in Git is a remote copy of a repository, which can be used as a backup for yourself, or as a way to share a repository with other developers. Most commonly, your remote will be an online service such as GitHub or GitLab, but there are many others as well. For this example, I’ll show you how to work with the GitLab instance hosted by K-State CS. The instructions are very similar for GitHub as well.\nFirst, you’ll need to navigate to http://gitlab.cs.ksu.edu and sign-in with your K-State CS username and password. Once you’ve logged in, go to your account settings, and look for the SSH Keys option. In order to authenticate your system with GitLab, you’ll need to create an SSH key and copy-paste the public key here. See the SSH video in the Extras module for detailed instructions on creating those keys. I’ll perform the steps quickly for this example as well. Once you are done, you can click the logo in the upper-left to go back to the dashboard.\nOn the dashboard, you can click the New Project button to create a new project. To make it simple, I’m going to give it the same name as my folder, git-test. I can also give it a description, and I’ll need to set the visibility level for this project. I’m going to use the Private option for now, so I’ll be the only one able to see this project.\nOnce your project is created, it will give you some handy instructions for using it. We’re going to do the last option, which is to use an existing Git repository. So, at any time in the future, you can easily follow those instructions to get everything set up correctly.\nBack on the terminal, we’ll need to add a remote to our Git repository. The command for this is:\ngit remote add origin \u003curl\u003e Then, we’ll need to push our repository to the remote server. Since we don’t have any locally created branches or tags, we can just use this command:\ngit push -u origin master If you have already created branches or tags in this repository, you can push them to the remote server using these commands:\ngit push -u origin --all git push -u origin --tags There you go! Your Git repository is set to be tracked by a remote server. If you open the GitLab page, you can now see your files here as well. You can now use that remote server to share this repository across multiple computers, or even with multiple developers. For example, let’s see how we could get a copy of this repository on another computer, make changes, and share those changes with both systems.\nFirst, on the other computer, you’ll need to also create an SSH key and add it to your account on GitLab. I’ve already done so for this system.\nNext, we’ll need to get a copy of the repository from the remote server using this command:\ngit clone \u003curl\u003e That will create a copy of the repository in a subfolder of the current folder. Now, we can open that folder and edit a file:\ncd git-test echo \"Some Changes\" \u003e\u003e file1.txt As you make edits, you can see all of the changes since your last commit using a couple of commands. First:\ngit status will give you a list of the files changed, created, or deleted since the last commit. You can see the details of the changes using:\ngit diff HEAD which will show you all of the changed files and those changes since the last commit.\nNow, we can stage any changes using:\ngit add . which will automatically add any changed, added, or removed files to the index. This command is very handy if you want to add all changed files to the repository at once, since you don’t have to explicitly list each one. Then, we can commit those changes using:\ngit commit -m \"Modified file1.txt\" and finally, we can upload those changes to the remote server using:\ngit push If we view the project on GitLab, we can see the changes there as well. Finally, on our original computer, we can use this command to download those changes:\ngit pull If you have made changes on both systems, it is a good idea to always commit those changes to the local repository before trying to use git pull to download remote changes. As long as the changes don’t conflict with each other, you’ll be in good shape. If they conflict, you’ll have to fix them manually. I’ll discuss how to do that a bit later as we deal with branching and merging.\nAnother major feature of Git is the ability to create branches in the repository. For example, you might have a really great idea for a new feature as you are working on a project. However, you are worried that it might not work, and you don’t want to lose the progress you have so far. So, you can create a branch of the project and develop your feature there. If it works, you can merge those changes back into the master branch of your project. If it doesn’t, you can just switch back to the master branch without merging, and all of your original code is just as you left it.\nTo create and switch to a new branch, you can use these commands:\ngit branch \u003cbranch_name\u003e git checkout \u003cbranch_name\u003e It should tell you that you switched to your new branch. You can also run\ngit status at any time to see what branch you are currently on. Now, let’s make some changes:\necho \"File 12\" \u003e file1.txt echo \"More Changes\" \u003e\u003e file2.txt Here, you’ll note that I am overwriting the contents in file1.txt since I only used one \u003e symbol, while I am adding a third line of content to file2.txt since I used \u003e\u003e to append to that file. You can always see the changes using:\ngit diff HEAD Now, let’s commit those changes:\ngit add . git commit -m \"Branch Commit\" git push -u origin \u003cbranch_name\u003e Notice that the first time you push to a new branch, you’ll need to provide the -u \u003cbranch_name\u003e option to tell Git which branch to use. Once you’ve done that the first time, you can just use git push in the future.\nNow, let’s switch back to the master branch and make some changes there as well:\ngit checkout master echo \"File 13\" \u003e file1.txt git add . git commit -m \"Master Commit\" git push At this point, let’s try to merge the changes from my new branch back into the master branch. To merge branches, you’ll need to switch to the destination branch, which I’ve already done, then use this command:\ngit merge \u003cbranch_name\u003e If none of the changes cause a conflict, it should tell you that it was able to merge the branches successfully. However, in this case, we have created a conflict. This is because we have modified file1.txt in both branches, and Git cannot determine how to merge them together in the best way. While Git is very smart and able to merge modified files in many cases, it isn’t able to do it when the same lines are changed in both files and it can’t determine which option is correct. So, it will require you to intervene and make the changes.\nSo, let’s open the file:\nnano file1.txt Here, you should see content similar to this\n\u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD File 13 ======= File 12 \u003e\u003e\u003e\u003e\u003e\u003e\u003e new_branch The first section, above the line of equals signs =, is the content that is in the destination branch, or the branch that you are merging into. The section below that shows the content in the incoming branch. To resolve the conflict, you’ll need to delete all of the lines except the ones you want to keep. So, I’ll keep the change in the incoming branch in this case:\nFile 12 and then I’ll save and close the file using CTRL+X, then Y, then ENTER. Once I’ve resolved all of the conflicts, I’ll need to commit them:\ngit add . git commit -m \"Resolve Merge Conflicts\" git push Congratulations! You’ve now dealt with branching and merging in Git.\nFinally, there are a couple of handy features of Git that I’d like to point out. First is the tagging system. At any time, you can create a tag based on a commit. This is typically used to mark a particular version or release of a program, or maybe even an assignment in a course. It makes it easy to find that particular commit later on, in case you need to jump back to it. To create and push a tag, you can do the following commands:\ngit tag -a \u003ctag_name\u003e -m \u003cmessage\u003e git push You can see the tags on GitLab too. It is a very handy feature when working with large projects.\nAlso, if you have some files that you don’t want Git to track, you can use a .gitignore file. It is simply a list of patterns that specify files, folders, paths, file extensions, and more that should be left out of Git’s index. I encourage you to read the documentation for this feature, which is linked in the resources section below the video. There are also some great sample .gitignore templates from GitHub linked there as well.\nI hope this video has given you a brief introduction of Git and how you could use it as a system administrator. If you’d like to learn even more about Git, I encourage you to review the links in the resources section below this video for even more information and examples of how you can use Git.\n",
    "description": "",
    "tags": null,
    "title": "Git",
    "uri": "/x-extras/07-git/index.html"
  },
  {
    "content": " Install Windows Server 2019 Standard (3) Static IP DNS server point to itself Install AD DS (10) Domain name: ad.\u003ceID\u003e.cis527.cs.ksu.edu Domain user Windows 10 Client on AD (5) Log in with domain user Install OpenLDAP (15) Domain Name: dc=ldap,dc=\u003ceID\u003e,dc=cis527,dc=cs,dc=ksu,dc=edu Install phpLDAPadmin LDAP OUs: users and groups LDAP groups: admin LDAP user TLS (test client side with ldapwhoami -x -Z ldap.\u003ceID\u003e.cis527.cs.ksu.edu returning anonymous) Ubuntu Client on LDAP (5) Log in with LDAP user Interoperability (10) Add Ubuntu to Windows AD LDAPSearch (2) Screenshots Ubuntu Firewall enabled and configured (-5 if not) ",
    "description": "",
    "tags": null,
    "title": "Lab 4 Grading Checklist",
    "uri": "/z-instructor-resources/07-lab-4-grading-checklist/index.html"
  },
  {
    "content": " Puppet Learning VM Deprecated As of 2023, the Puppet Learning VM is no longer being maintained. The videos below demonstrate some of the features of Puppet, which can also be done on your Ubuntu VM after installing Puppet Agent. Unfortunately, it is not easily possible to simulate an enterprise Puppet setup without this VM, so I’ll keep these videos up for demonstration purposes. –Russ\nYouTube Video Resources Language: Relationships and Ordering from Puppet Video Script Now that we’ve seen Puppet Manifest files, let’s discuss some of the caveats of such a system. On the Learning Puppet VM, during the hello_puppet quest as described in the earlier videos, I’m going to continue to create manifest files on the hello.puppet.vm node.\nUsing Nano, create the following manifest file:\nfile { '/tmp/test1': ensure =\u003e 'file' } file { '/tmp/test2': ensure =\u003e 'file' } file { '/tmp/test3': ensure =\u003e 'file' } notify { \"First Notification\": } notify { \"Second Notification\": } notify { \"Third Notification\": } Once that is done, apply the manifest:\nsudo puppet apply test.pp When you do, you may see things applied out of order. Since a Puppet Manifest file only defines the desired configuration, it is up to the Puppet Agent tool to determine which steps are necessary and in what order. This is because Puppet considers each resource definition to be atomic, meaning that it is independent of all other resources. In some cases, however, we want to make sure that some resources are configured before others. Thankfully, Puppet gives us many ways to do so.\nThe first uses the before and require keywords. Here is an example from the Puppet documentation linked in the resources section below the video:\npackage { 'openssh-server': ensure =\u003e present, before =\u003e File['/etc/ssh/sshd_config'], } file { '/etc/ssh/sshd_config': ensure =\u003e file, mode =\u003e '0600', source =\u003e 'puppet:///modules/sshd/sshd_config', require =\u003e Package['openssh-server'], } Each of these elements creates the same relationship. You only need to include either the before or require keyword in one of the resources, but both are not required.\nSimilarly, the notify and subscribe keywords create a similar relationship. From the documentation:\nfile { '/etc/ssh/sshd_config': ensure =\u003e file, mode =\u003e '0600', source =\u003e 'puppet:///modules/sshd/sshd_config', notify =\u003e Service['sshd'], } service { 'sshd': ensure =\u003e running, enable =\u003e true, subscribe =\u003e File['/etc/ssh/sshd_config'], } Again, only one or the other is required. However, unlike with the before and require keywords, in this case the second item will only be refreshed if the prior resource has changed. In this example, if the sshd_config file is changed, then the sshd service will be restarted so it will read the newly changed file. This is a very powerful tool for making changes to configuration files and making sure the services immediately restart and load the new configuration.\nFinally, the third method is through the use of chaining arrows. This tells the Puppet Agent to apply resources in the order they are written in the manifest. One of the most powerful and common uses of this is to create a “Package, File, Service” chain. Here is the example from the Puppet documentation:\n# first: package { 'openssh-server': ensure =\u003e present, } -\u003e # and then: file { '/etc/ssh/sshd_config': ensure =\u003e file, mode =\u003e '0600', source =\u003e 'puppet:///modules/sshd/sshd_config', } ~\u003e # and then, if the previous file was updated: service { 'sshd': ensure =\u003e running, enable =\u003e true, } This manifest will ensure the openssh-server package is installed first. Then, once it is installed, it will place the desired configuration file on the system. The first arrow with a simple hyphen creates a before/require relationship. Then, if that file is modified in any way, the second arrow, using a tilde ~ character, will enforce a notify/subscribe relationship and cause the sshd service to refresh.\nAs you work on your Puppet Manifest files for Lab 2, it is helpful to keep in mind that some resources may need to be chained together to work properly. However, do not try to chain together your entire manifest file, as that defeats much of the flexibility of Puppet itself.\n",
    "description": "",
    "tags": null,
    "title": "Puppet Resource Ordering",
    "uri": "/2-configuration-management/07-puppet-resource-ordering/index.html"
  },
  {
    "content": " YouTube Video Resources Slides An Introduction to Droplet Metadata from DigitalOcean How to Resize Droplets from DigitalOcean How to Automate the Scaling of Your Web Application on DigitalOcean Ubuntu 16.04 Droplets from DigitalOcean Load Balancers from DigitalOcean What is High Availability? from DigitalOcean AWS Auto Scaling from Amazon Web Services Netflix Case Study Netflix: What Happens When You Press Play? from High Scalability Blog Scryer: Netflix’s Predictive Auto Scaling Engine from Netflix Technology Blog Completing the Cloud Migration from Netflix DevOps Case Study: Netflix and the Chaos Monkey from DevOps Blog at CMU Software Engineering Institute Chaos Engineering Upgraded from Netflix Technology Blog Video Transcript One of the major selling points of the cloud is its rapid elasticity, or the ability of your cloud resources to grow and shrink as needed to handle your workload. In this video, you’ll learn a bit more about how to configure your cloud resources to be elastic, as well as some of the many design decisions and tradeoffs you’ll face when building a cloud system.\nFirst, let’s talk a bit about scalability. If you have a cloud system, and find that it does not have enough resources to handle your workloads, there are generally two ways you can solve that problem. The first is to scale vertically, which involves adding more CPU and RAM resources to your cloud systems. Effectively, you are getting a bigger, more powerful computer to perform your work. The other option is to scale horizontally by configuring additional cloud systems to increase the number of resources you have available. In this instance, you are just adding more computers to your organization. In many cases, your decision of which way to grow may depend on the type of work you are performing as well as the performance limitations of the hardware. If you are dealing with a large number of website users, you may want to scale horizontally so you have more web servers available. If you are performing large calculations or working with big databases, it may be easier to scale vertically in order to keep everything together on the same system.\nIn the case of DigitalOcean, you can scale both horizontally and vertically with ease. To scale horizontally, you’ll simply add more droplets to your cloud infrastructure. You may also need to add additional items such as a load balancer to route incoming traffic across multiple droplets. For vertical scaling, you can easily resize your droplets via the DigitalOcean control panel or their API. Unfortunately, to resize a droplet it will need to be rebooted, so you’ll have to deal with a bit of downtime unless you have some existing infrastructure for high availability. We’ll discuss that a bit later in this video.\nUnfortunately, DigitalOcean does not support any automated scaling features at this time. However, they do offer a tutorial online for how to build your own scripts to monitor your droplet usages and provision additional droplets if you’d like to scale horizontally. This diagram shows what such a setup might look like. It has a frontend resource with a load balancer as well as a script to manage scaling, and a number of backend web servers to handle the incoming requests. If you are interested in learning about scaling in DigitalOcean, I encourage you to check out the tutorial linked in the resources section below the video.\nOne major feature that sets Amazon Web Services apart from DigitalOcean is the ability to perform automatic scaling of AWS instances. Through their control panel, it is very simple to set up a scaling plan to optimize your use of AWS resources to match your particular needs in the cloud. Since AWS is primarily targeted at large enterprise customers, it has many of these features available to help them manage large workloads in the cloud. I’ve also linked to information about Auto Scaling in AWS in the resources section below the video if you’d like to learn more.\nFinally, beyond scaling, you should also consider how to design your systems for high availability when dealing with the cloud. In many organizations, your overall goal is to make your cloud resources available all of the time, without any noticeable errors or downtime. To do that, however, requires quite a bit of planning and an advanced architecture to make it all work properly. Here is a simple example setup from DigitalOcean, showing how you can use six cloud resources to build a simple, highly available system.\nFirst, there are two load balancers. One is acting as the primary, and is assigned to a floating IP address. The secondary load balancer has a connection to the primary, allowing it to monitor the health of the system. If the secondary load balancer detects an error with the primary, it will switch the floating IP to point to itself to handle incoming requests. This change can happen almost automatically, so the users will not experience much downtime at all. Behind the load balancer is two application servers. The load balancers can forward requests to either application server, but they will, of course, detect if one server is down and route all requests to the other server instead. Finally, each application server is attached to a backend database server, each of which are replicated from the other to maintain data consistency. With this setup, as long as both systems of the same type don’t fail at the same time, the application should always be available to the users.\nIn the updated assignment for Lab 5, you are asked to create a load balancer between your DigitalOcean droplets to split HTTP traffic between the frontend and backend droplets. Once that is properly set up and configured, you should be able to visit the IP address of the load balancer and see the homepage of one of the two droplets. Then, if you constantly refresh that page, it should swap between the two servers as shown here.\nUnfortunately, due to the way we have configured other parts of this lab, it is prohibitively difficult to configure this load balancer to properly handle HTTPS traffic. This is mainly because we are using an external registrar for our domain name so that DigitalOcean cannot manage the domain, and the certificates we are getting from certbot are tied to the actual domain name and not a wildcard. In a production system, we would probably change one of these two things to allow us to send properly secured HTTPS traffic through the load balancer. But, for now, we won’t worry about that.\nTo put it all together, let’s look at a quick case study for how to build an effective computing architecture in the cloud. Netflix is one of the pioneers in this area, and arguably has one of the most advanced and robust cloud infrastructures on the internet today.\nAs you know, Netflix has grown by leaps and bounds over the past decade. This graph shows their total monthly streaming video hours from 2008 through 2015. Since that time, it has continued to grow at an even faster rate. In fact, today Netflix is the platform of choice for viewing TV content among most Americans, beating out basic cable and broadcast TV. As Netflix moved into the streaming video arena, they suffered a few major setbacks and outages in their data centers in 2008, prompting them to move to the cloud. As of January 2016, their service is hosted entirely from the cloud, primarily through Amazon Web Services and their own content delivery network, named Netflix Open Connect.\nOf course, moving to the cloud brings its own challenges. This graph shows the daily traffic for five days across Netflix’s systems. As you can see, the traffic varies widely throughout the day, peaking and then quickly dropping. To handle this level of traffic, Netflix has a couple of options. They could, of course, scale their system to handle the highest peaks of traffic, and let it set idle during the dips. However, since the cloud should be very elastic, that is a very inefficient use of resources and could end up costing the company a fortune.\nIn addition, since the traffic peaks and dips so quickly, a reactive scaling approach may not work. According to their technology blog, it can take up to 45 minutes to provision a new cloud resource in their infrastructure, so by the time it is ready to go the traffic may have increased even more. In short, they’d never be able to catch up.\nSo, Netflix developed Scryer, a predictive scaling tool for their cloud infrastructure. Scryer analyzes traffic patterns and builds a prediction of what the traffic will be in the future, allowing Netflix to proactively scale their resources up before the increase in traffic happens, allowing them to instantly be available when they are needed.\nThis graph shows the workload predicted by Scryer for a single day as well as the scaling plan that came from that prediction. Netflix has used this to not only improve their performance, but reduce the costs as well.\nOf course, scaling is just one piece of the puzzle when it comes to handling large cloud workloads. Many large websites employ high levels of caching, as well as the use of a content distribution network, to lessen the load on their actual cloud infrastructures and reduce the need for scaling. In the case of a content distribution network, or CDN, those websites store their data closer to the users, sometimes directly in the datacenters of internet service providers across the globe. Netflix is no different, and in many cases Netflix has stored content representing 80% or more of its workload directly in the networks of local ISPs. So, while Netflix has all of the data stored on their cloud systems, those systems are usually more involved in sending data to the local content distribution centers than actual individual users.\nFinally, Netflix was a pioneer in the area of chaos engineering, or building their systems to expect failure. As they moved into the cloud, they developed tools such as “Chaos Monkey,” “Latency Monkey,” and even “Chaos Gorilla,” all part of their “Simian Army” project, to wreak havoc on their systems. Each of those tools would randomly cause issues with their actual production cloud systems, including shutting down a node, introducing artificial latency, or, in the case of Chaos Gorilla, even cutting off an entire zone. By doing so, Netflix essentially forced itself to build systems that were highly tolerant of failures, to the point that consumers wouldn’t even notice if an entire zone went offline.\nIn fact, these graphs show just such an event, as simulated by Chaos Gorilla. The top shows global traffic, while the bottom shows traffic during the same time period for both the eastern and western US zones. During the test, traffic to the western zone was blocked, resulting in a large amount of traffic being rerouted to the eastern zone. Looking at the graphs, you can clearly see the switchover in the smaller graphs below, but the global graph stayed steady, meaning that users worldwide wouldn’t have even noticed a blip in service.\nWhile this may be an extreme example of planning for failures, it goes to show the depth to which Netflix has designed their cloud infrastructure to be both highly scalable to handle fluctuating demand, and highly available to mitigate failures and outages. I’ve included a whole section of related reading for this Netflix case study in the resources section below the video if you’d like to know more about these topics.\nSo, as you move forward and continue designing systems for the cloud, here are a few design considerations I feel that you should think about. First, do you plan on scaling out, or scaling up? In addition, is it better to scale predictively, or reactively. Also, does your system need to be designed for high availability, or is it better to save money and simplify the design at the expense of having a bit of downtime once in a while? Finally, no matter what design you choose, you should always be planning for the inevitable failures and outages, and testing any failover procedures you have in case they do happen. Just like the fire drills you might remember from school, it’s much better to practice for emergency situations that never happen than to have to deal with an emergency you haven’t prepared for in the first place.\nThat’s all for Module 5! In Module 6, you’ll continue building both cloud systems as well as enterprise networks as we deal with application servers. In the meantime, you should have everything you need to complete Lab 5. As always, if you have any questions, feel free to post in the course discussion forums to get help. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Scaling \u0026 High Availability",
    "uri": "/5-the-cloud/07-scaling-high-availability/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello, and welcome to the week seven announcements video for CIS 527 In summer 2022. So we’re getting toward the end of the semester. So this week, don’t forget you got lab six due tonight by 7pm. So make sure you get that submitted, I’ve got most grading times available for students. But if you need more help or needed writing time, just let me know. It’s also available on Calendly. Like always, there’s a week six discussion that’s due tonight by 1159. So make sure you watch the video, respond to it and post some questions for our guest speaker. This week on Friday, you’ve got the week, seven quizzes that are due. And you also have your final project proposal that’s due, we’ll talk a little bit about the final project and the second. And then next Monday, lab seven, the last seven, the last lab in this class is due. And there’s also one more discussion that is due next Monday as well. I will be posting lab seven, hopefully later today. So you can get started on that.\nSo for lab six grading, we’re really looking at four things, you should have a Windows File Server setup with the Group Policy, so that automatically mounts those network drives via Active Directory, you should have an Ubuntu file server setup with some work in fs tab and some other files to actually get the files to mount correctly, you should have a Windows web application running in IIS on your Windows Server. And then you should have a cloud web application running either directly on your back end and front end droplets or running it through Docker. Those are the things we’re going to be looking for. For lab six. The nice thing is if it works, it takes all of five minutes to grade it. If it doesn’t work, we’ll try and dig into it a little bit and figure out exactly what’s going on. So that hopefully we can get it fixed. So lab seven is kind of a grab bag of all these other topics that I really want to cover in this class, but they don’t fit nicely into a single lab. So we’ll spend some time talking about backups and restore. And we’ll have you do a practice Backup and Restore on an Active Directory server will talk a bit about monitoring and give you a chance to set up some monitoring systems on your droplets on the web. We’ll also talk a bit about DevOps and we’ll play around with web hooks and a few things like that. So that’s really what we’re doing in lab seven.\nThe speaker for this week is Hunter Guthrie. Hunter Guthrie is a plant system administrator for Evergy, specifically at the Wolf Creek generating plant. He’s a former student of mine in this class. And he’s really unique because he works in a very highly secure industry working at a nuclear power plants. So you know, Sarah had her own concerns and things working in a construction industry where it’s much more about usability and working out in the field. Hunter is kind of the other side of that where he works in a very highly secure industry. So everything has to be very carefully vetted and protected. And so he works on kind of an entirely different setup than what Sarah has. And it’s really interesting to see those two different sides of the IT coin.\nSo you should be starting to think about your final project. The proposal is due this Friday. The goal of the final project is to demonstrate what you’ve learned in this class by proposing to either build something or fix something or change something related to it and System Administration. Some ideas that you can think of would be designing a web resource for new startups. So how would you host their website? How would you ensure high availability? How would you deal with high traffic loads? Looking at things like scalability and the design of that system? You can think about setting up laptops for a school if a school gets a donation of 50 laptops or 100 laptops? How could you use tools like puppet or chef or Ansible to set up and manage those laptops, you could think about central authentication. If you’ve got a small company that’s growing, how could you help them build out a central authentication system using a Windows Server and Active Directory and getting all the clients connected, maybe looking at things like VPNs, so that you can have multiple offices connected together on the same network or, you know, today thinking about work from home and how people can securely access work resources from home. You can also discuss things like thin clients versus thick clients, specifically in a lab setting. So you know, here in K-State, we have a couple of computer labs that have thin clients that just remote desktop into a server. We also have some labs with thick clients or traditional desktops. And so there’s pros and cons on that, and that’d be really good. The bottom line was this final project, hopefully, you’ve got an idea in mind. If not, feel free to chat with me, I’m happy to kind of dig into things you’re interested in and try and pitch you an idea of something that I think would be a really useful final project.\nSo the final project itself, it really has a few different deliverables. The first big thing is you’ll need to have a written report, I highly recommend using the template that I pointed out in the final project. Be aware of the template each paragraph is listed as bullet points. Your report should not be bullet points. The bullet points are simply saying what sorts of paragraphs what sorts of ideas you should address in those sections. So do not submit a report full of bullet points. It should be a written report with paragraphs. In the written report, you’re going to spend some time researching, proposing and doing a SWOT analysis of your project. You can include graphics and data, anything that’s needed to help me understand your proposal and your SWOT analysis. The big thing to focus on is the SWOT analysis. You need to do enough work on your proposal so that it’s understandable that you’ve proposed it We understand the parameters that you’re using. But you really should focus a bit on the SWOT analysis. A lot of students usually tend to make the SWOT analysis at the last minute. And because of that, they really haven’t taken enough time to analyze specifically the weaknesses and the threats related to their project, I really don’t want to see a SWOT analysis that says that any proposal has no weaknesses or no threats, that’s simply not the case, you just have to think a little bit bigger, be a little bit more malicious towards your project so that you can come up with those weaknesses and threats. Once you’ve got all that done, you’re going to schedule a live presentation where you will present your written report to me, in usually the presentation is about 15 to 30 minutes, a lot of times it covers about the same structure and content is the written report. The goal of this live presentation is to convince me that you have analyzed your proposal Well, a good way to think of this is I am your CIO, you are working for me, and you’re proposing this as a new technology project press to undertake. So you should convince me as your CIO, that this is a good idea that you’ve done your analysis that you’ve done your research, and that we can proceed with this. So your presentation is about 15 to 30 minutes, you can present any style you want. If you want to use PowerPoint, if you just want to scroll through your report, if you want to do something different, that is fine. The last part of your final project is a small prototype, I want you to take one small part of what you’re proposing and build a small prototype of it. The prototype should be a minor portion of this, maybe spend about two to four hours on it. But things like if you’re doing the laptop setup for a school, maybe play around with Ansible and compare it to puppet a little bit. If you’re doing a single sign on for a company look at maybe building another Active Directory server and connecting up some different clients in different ways. If you’re building a website for high availability, maybe spin up a couple more droplets on DigitalOcean and play around with things like their load balancers or their automatic scalability. Again, the prototype should be a small part of your project, maybe spend no more than two or four hours on the prototype. But it really wants you to build at least a small portion of what you’re proposing. So that I can see that you’ve done some research and gotten hands on with something that you’re actually playing around with.\nSo the presentation itself, you have two options, you can either give it live to me via zoom, or you can pre record a video. Even if you pre record the video, we still need to have a scheduled time on Zoom. So I can do some q\u0026a. So make sure you leave time in the schedule for that. It should be presented on or before the Friday of finals week. So that is a week from this Friday, July 29. You can go ahead and schedule your time right now. So if you know that there’s a time that works for you, get it on my schedule. If you work during the days, and you want to schedule something after 5pm, email me so that I can get you on the schedule, my schedule does tend to fill up pretty quickly that week. So don’t be afraid to do that. But like I said, you can either present it live via zoom, and we’ll do q\u0026a Right after or you can pre record a video. And then I will watch the video and we will schedule a zoom time where I can do q\u0026a with you after that. So submitting a video that was recorded Friday night that doesn’t cover the q\u0026a, you’re going to lose points for that. So make sure that you get that in early enough so that we can schedule some q\u0026a time as well.\nThat’s really all I’ve got this week. Hopefully that helps you understand the final project and what’s coming up. As always, you can keep in touch. We’ve got discussions on Discord, I’ve got Tea Time office hours, Tuesdays at 330 Fridays at 1030. I do one on one office hours via Calendly you can get in touch with me a lot of different ways. But hopefully you’re getting toward the end of the semester and I’m sure that I will see a few PowerPoints in this class. So make sure you’d be thinking about your final projects and how you want to present it and make it dynamic. As always if you have any questions, let me know and I will see you once again next week.\n",
    "description": "",
    "tags": null,
    "title": "Summer '22 Week 7",
    "uri": "/y-announcements/old/summer2022/week07/index.html"
  },
  {
    "content": " YouTube Video Note PHPLDAPAdmin no longer works on Ubuntu 22.04 since it has several incompatibility issues with PHP 8+. The assignment is being updated to use the new LDAP Account Manager (LAM) software instead. The video above will be updated soon to include LAM instead of PHPLDAPAdmin\nResources How to Install OpenLDAP on Ubuntu Server 22.04 from TechRepublic How To Install and Configure OpenLDAP and phpLDAPadmin on Ubuntu 16.04 from DigitalOcean (works for 22.04 as well, but PHPLDAPAdmin is not working on PHP 8.1 - use the new LDAP Account Manager instead) LDAP \u0026 TLS from the Ubuntu Server Guide How Does HTTPS Work from kubucation on YouTube (a good overview of CAs and certificates) Core Networking Services - Security The Cloud - Certificates Video Transcript As we did earlier with setting up Active Directory on Windows, let’s take a look at the steps required to install and configure OpenLDAP on Linux. The goal of this video isn’t to show you all the steps of the process, but provide commentary on some of the more confusing steps you’ll perform.\nFor this example, I’m using the Ubuntu VM labelled Server from Lab 3. I have chosen to disable the DHCP server on this system, and instead have replaced it with the VMware DHCP server. In practice, that change should not affect any of this process. Also, I’ll generally be following the guide from DigitalOcean, which is linked in the resources section below the video.\nAt this point, I have also already performed the first steps of the guide, which is to install the slapd and ldap-utils packages on this system. Now, I’ll step through the configuration process for slapd and discuss the various options it presents.\nsudo dpkg-reconfigure slapd First, it will ask you if you want to omit the OpenLDAP server configuration. Make sure you select \u003cNo\u003e for that option. You can use the arrow keys (← and →) to move the red-shaded selector to the option you’d like to select, then press ENTER to confirm it.\nNext, it will ask for your domain name. For this example, I’ll just use ldap.cis527russfeld.cs.ksu.edu. You’ll need to adjust these settings to match the required configuration for the lab assignment. This matches the DNS configuration I made in Lab 3, which is very important later on in this process.\nNext, it will also ask for your organization name. I’ll just use cis527 here.\nThen, it will ask for your Administrator password. This is the equivalent of the root account within the domain. It is able to change all domain settings, and therefore the password for this account should be very complex and well protected. For this example, I’ll just use the same password we are using for everything else, but in practice you’ll want to carefully consider this password.\nThe next option is whether you want to have the database removed if you purge the slapd package. Selecting \u003cyes\u003e on this option would delete your LDAP database if you ever chose to reinstall the slapd package. For most uses, you’ll always want to select \u003cno\u003e here, which is what I’ll do.\nSince there are some existing configuration files in place, the installer asks if you’d like those to be moved. Unless you are reinstalling slapd on an existing server, you can select \u003cyes\u003e for this option.\nThat should complete the configuration for the OpenLDAP server. For this example, I have disabled the firewall on this system, but for your lab assignment you’ll need to do a bit of firewall configuration at this point.\nNext, I’m going to configure phpldapadmin on this server. It is a useful web interface for managing your OpenLDAP server. We can install it using apt:\nsudo apt install phpldapadmin Once it has been installed, we’ll just need to configure it by editing its configuration file.\nsudo nano /etc/phpldapadmin/config.php In that file, you’ll need scroll down to around line 286 to find the configuration for your LDAP servers. You’ll need to set the server name, host address, and login information. You can roughly follow the first guide from DigitalOcean linked below this video for these changes as well. For my system, I’ll configure the following lines (you may need to uncomment some of them as well):\n$servers-\u003esetValue('server', 'name', 'CIS 527 LDAP'); $servers-\u003esetValue('server', 'host', '127.0.0.1'); $servers-\u003esetValue('server', 'base', array('dc=ldap,dc=cis527russfeld,dc=cs,dc=ksu,dc=edu')); $servers-\u003esetValue('login', 'bind_id', 'cn=admin,dc=ldap,dc=cis527russfeld,dc=cs,dc=ksu,dc=edu'); At last, you can load the website by navigating to http://localhost/phpldapadmin in your web browser.\nOnce we’ve loaded the interface, we’ll need to add a few items to make this server fully useable. The second guide linked below this video gives the steps for adding some organizational units, groups, and users to the server. I’ve already performed the steps to add the users and groups organizational units, as well as the admin group. For this video, I am going to discuss a few issues related to creating LDAP users.\nTo begin, I’ll click on the “ou=users” link on the left, then the “Create a child entry” link on the right. I’ll choose the “Generic: User Account” template next. On this page, I’ll enter the information for my user account. Note that the “Common Name” field must be unique, so it is a good idea to input the user’s username here, instead of the default of their first and last name. I’ll leave the home directory as the default, choose the “admin” group, and the Bash login shell. Finally, I’ll enter a password, and click the “Create Object” button to create it. Once it is created, I’ll edit the UID number to be 10000 instead of 1000. This is very important, because by default accounts on Ubuntu start at UID 1000, so this could create a UID conflict.\nFinally, let’s add some encryption to this server to protect the information shared across the network. This step was not required in previous versions of this lab, but it has always been a good idea. Now that Ubuntu uses SSSD, or System Security Services Daemon, for authentication, it requires us to provide TLS encryption on our LDAP server. So, let’s do that now.\nFor these steps, we’ll be closely following the Ubuntu Server Guide linked below this video. We’ll be adjusting a few of the items to match our configuration, but most of the commands are exactly the same.\nFirst, we’ll need to install a couple of packages to allow us to create and manipulate security certificates\nsudo apt update sudo apt install gnutls-bin ssl-cert Next, we need to create our own certificate authority, or CA. In a production system, you would instead work with an actual CA to obtain a security certificate from them, but that can be time consuming and expensive. So, for this example, we’ll just create our own. Also, contrary to what your browser may lead you to believe, using your own CA certificates will result in an encrypted connection, it just may not be “trusted” since your browser doesn’t recognize the CA certificate.\nIf you want to know more about CAs and certificates, check out the handy YouTube video I’ve linked below this video. It gives a great description of how certificates work in much more detail. I’ve also included links to the lectures in Module 3 and Module 5 that deal with TLS certificates.\nSo, we’ll first create a private key for our CA:\nsudo certtool --generate-privkey --bits 4096 --outfile /etc/ssl/private/mycakey.pem Then, we’ll create a template file that contains the options we need for creating this certificate. Notice here that I’ve modified this information from what is provided in the Ubuntu Server Guide document to match our setup:\nsudo nano /etc/ssl/ca.info and put the following text into that file.\ncn = CIS 527 ca cert_signing_key expiration_days = 3650 Then, we can use that template to create our self-signed certificate authority certificate:\nsudo certtool --generate-self-signed --load-privkey /etc/ssl/private/mycakey.pem --template /etc/ssl/ca.info --outfile /usr/local/share/ca-certificates/mycacert.crt This will create a certificate called mycacert.crt that is stored in /usr/local/share/ca-certificates/. This is our self-signed CA certificate. So, whenever we want any system to trust one of our self-signed certificates that we make from our CA, we’ll need to make sure that system has a copy of this CA certificate in its list of trusted certificates.\nTo do that, we simply place the certificate in /usr/local/share/ca-certificates/, and then run the following command to add it to the list of trusted certificates:\nsudo update-ca-certificates If that command works correctly, it should tell us that it has added 1 certificate to our list.\nBefore we move on, let’s make a copy of our mycacert.crt file in our cis527 users’s home directory. We’ll use this file in the next part of this lab when we are configuring our client system to authenticate using LDAP.\ncp /usr/local/share/ca-certificates/mycacert.crt ~/ Now that we’ve created our own self-signed CA, we can use it to create a certificate for our LDAP server. So, we’ll start by creating yet another private key:\nsudo certtool --generate-privkey --bits 2048 --outfile /etc/ldap/ldap01_slapd_key.pem Then, we’ll create another template file to give the information about the certificate that we’d like to create:\nsudo nano /etc/ssl/ldap01.info and place the following tex in that file:\norganization = CIS 527 cn = ldap.cis527russfeld.cs.ksu.edu tls_www_server encryption_key signing_key expiration_days = 365 Notice that in that file, we entered the fully qualified domain name for our LDAP server on the second line. This is the most important part that makes all of this work. On our client machine, we must have a DNS entry that links this domain name to our LDAP server’s IP address, and then the certificate the server uses must match that domain name. So, if we don’t get this step right, the whole system may not work correctly until we fix it!\nFinally, we can use this long and complex command to create and sign our server’s certificate:\nsudo certtool --generate-certificate --load-privkey /etc/ldap/ldap01_slapd_key.pem --load-ca-certificate /etc/ssl/certs/mycacert.pem --load-ca-privkey /etc/ssl/private/mycakey.pem --template /etc/ssl/ldap01.info --outfile /etc/ldap/ldap01_slapd_cert.pem Once the certificate has been created, we just need to do a couple of tweaks so that it has the correct permissions. This will allow the LDAP server to properly access the file:\nsudo chgrp openldap /etc/ldap/ldap01_slapd_key.pem sudo chmod 0640 /etc/ldap/ldap01_slapd_key.pem Finally, at long last, we are ready to tell our LDAP server to use the certificate. To do that, we’ll create an LDIF file:\nnano ~/certinfo.ldif and in that file we’ll put the information about our certificate:\ndn: cn=config add: olcTLSCACertificateFile olcTLSCACertificateFile: /etc/ssl/certs/mycacert.pem - add: olcTLSCertificateFile olcTLSCertificateFile: /etc/ldap/ldap01_slapd_cert.pem - add: olcTLSCertificateKeyFile olcTLSCertificateKeyFile: /etc/ldap/ldap01_slapd_key.pem Once that file is created, we can use the ldapmodify tool to import it into our server:\nsudo ldapmodify -Y EXTERNAL -H ldapi:/// -f certinfo.ldif Whew! That was a lot of work! Now, if we did everything correctly, we should be able to query this LDAP server using TLS. The best way to check that is by using the following command:\nldapwhoami -x -ZZ -h ldap.cis527russfeld.cs.ksu.edu The -ZZ part of that command tells it to use TLS, and if it works correctly it should return anonymous as the result. If this step fails, then you may need to do some troubleshooting to see what the problem is.\nThat should do it! In the next video, I’ll show how to configure another Ubuntu VM to use this LDAP server for authentication.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu LDAP Installation",
    "uri": "/4-directory-services/07-ubuntu-ldap-installation/index.html"
  },
  {
    "content": " YouTube Video Resources 20 Command Line Tools to Monitor Linux Performance from TecMint 80 Linux Monitoring Tools from Server Density Blog System Monitoring Tools for Ubuntu from AskUbuntu Best System Monitoring Tools for Windows \u0026 Linux (Free \u0026 Paid) from Comparitech The Top 20 Free Network Monitoring and Analysis Tools for SysAdmins from GFI Tech Talk Seamless Infrastructure Monitoring from DigitalOcean Monitoring Quickstart from DigitalOcean How to Set Up Monitoring Alerts from DigitalOcean Video Transcript Ubuntu also has many useful tools for monitoring system resources, performance, and health. In this video, I’ll cover some of the more commonly used ones.\nFirst and foremost is the System Monitor application. It can easily be found by searching for it on the Activities menu. Similar to the Task Manager on Windows, the System Monitor allows you to view information about all the running processes, system resources, and file systems on Ubuntu. It is a very useful tool for discovering performance issues on your system.\nOf course, on Ubuntu there is an entire universe of command-line tools that can be used to monitor the system as well. Let’s take a look at a few of them.\nFirst is top. Similar to the System Monitor, top allows you to see all running processes, and it sorts them to show the processes consuming the most CPU resources first. There are commands you can use to change the information displayed as well. In addition, you may also want to check out the htop command for a more advanced view into the same data, but htop is not installed on Ubuntu by default.\nIn addition to top, you may also want to install two similar commands, iotop and iftop. As you might expect, iotop shows you all processes that are using IO resources, such as the file systems, and iftop lists the processes that are using network bandwidth. Both of these can help you figure out what a process is doing on your system and diagnose any misbehavior.\nTo view information about memory resources, you can use either the vmstat or free commands. The free command will show you how much memory is used, and vmstat will show you some additional input and output statistics as well.\nAnother command you may want to install is iostat, which is part of the sysstat package. This command will show you the current input and output information for your disk drives on your system.\nIn addition, you can use the lsof command to see which files on the system are open, and which running process is using them. This is a great tool if you’d like to figure out where a particular file is being used. I highly recommend using the grep tool to help you filter this list to find exactly what you are looking for.\nIn Module 3, we discussed a few network troubleshooting tools, such as the ip and ss commands, and Wireshark for packet sniffing. So, as you are working on monitoring your system, remember that you can always use those as well.\nFinally, you can also use the watch command on Ubuntu to continuously run a command over and over again. For example, I could use watch tail /var/log/syslog to print the last few lines of the system log, and have that display updated every two seconds. This command is very handy when you need to keep an eye on log files or other commands as you diagnose issues. If you are running a web server, you may also want to keep an eye on your Apache access and error logs in the same manner.\nThere are also a few programs for Ubuntu that integrate several different commands into a single dashboard, which can be a very useful way to monitor your system. The two that I recommend using are glances and saidar. Both can easily be installed using the APT tool. Glances is a Python-based tool that reads many different statistics about your system and presents them all in a single, unified view. I especially like running it on systems that are not virtualized, as it can report information from the temperature sensors built-in to many system components as well. It even has some built-in alerts to help you notice system issues quickly. Saidar is very similar, but shows a slightly different set of statistics about your system. I tend to use both on my systems, but generally I will go to Glances first.\nAs I mentioned a bit earlier, Ubuntu stores a plethora of information in log files stored on the system. You can find most of your system’s log files in the /var/log directory. Some of the more important files there are auth.log which tracks user authentications and failures, kern.log which reports any issues directly from the Linux kernel, and syslog which serves as the generic system log for all sorts of issues. Some programs, such as Samba and Apache, may create their own log files or directories here, so you may want to check those as well, depending on what your needs are. As with Windows, there are many programs that can collect and organize these log files into a central logging system, giving you a very powerful look at your entire infrastructure.\nLastly, if you are running systems in the cloud, many times your cloud provider may also provide monitoring tools for your use. DigitalOcean recently added free monitoring to all droplets, as long as you enable that feature. You can view the system’s monitoring output on our DigitalOcean dashboard under the Graphs option. In addition, you can configure alert policies to contact you when certain conditions are met, such as sustained high CPU usage or low disk space. I encourage you to review some of the available options on DigitalOcean, just to get an idea of what is available to you.\nThat should give you a quick overview of some of the tools available on an Ubuntu system to monitor its health and performance. There are a number of tools available online, both free and paid, that can also perform monitoring tasks, collect that data into a central hub, and alert you to issues across your system. As part of the lab assignment, you’ll configure either Munin or Ganglia to discover how those tools work.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Monitoring",
    "uri": "/7-backups-monitoring-devops/07-ubuntu-monitoring/index.html"
  },
  {
    "content": " Note This video was recorded on Ubuntu 18.04, but nothing significant has changed in Ubuntu 20.04. –Russ\nYouTube Video Resources How to Configure Static IP Address on Ubuntu 20.04 Focal Fossa Desktop/Server from LinuxConfig.org Network Configuration from Ubuntu Server Guide ‘ip’ Command Cheat Sheet (Command Line Reference) from The Geek Diary ip Command Cheat Sheet from Red Hat Even though it is for Red Hat, most of these commands should also work in Ubuntu, and honestly, it is just a really handy cheat sheet to have! 12 ip Command Examples for Linux Users from Linuxtechi How to Use IPRoute2 Tools to Manage Network Configuration on a Linux VPS from DigitalOcean How to Use Traceroute and MTR to Diagnose Network Issues from DigitalOcean An Introduction to the ss Command from The Linux Foundation 10 Examples of Linux ss Command to Monitor Network Connections from BinaryTides Video Transcript Now, let’s look at how to manage and configure a network connection in Ubuntu 18.04.\nTo begin, I’m working in the Ubuntu VM I created for Lab 2, with the Puppet Manifest files applied.\nFirst, let’s take a quick look at how our networking is configured in VMware. This information is also covered in the video on Windows networking, but it is relevant here as well, since this will be very important as you complete Lab 3. To view the virtual networks in VMware Workstation, click the Edit menu, then choose Virtual Network Editor. On VMware Fusion, you can find this by going to the VMware Fusion menu, selecting Preferences, then the Network option.\nHere, we can see the virtual networks available on your system. Right now, there are two networks on my system, one “Host-only” network, and one “NAT” network. For this lab, we’ll be working with the “NAT” network, so let’s select it.\nFirst, let’s look at the network type, listed here. For this module, it is very important to confirm that the network type is set to “NAT” and not “Bridged.” If you use a “Bridged” network for this lab, you could easily break the network that your host computer is connected to, and in the worst case earn yourself a visit from K-State IT staff as they try to diagnose the problem. So, make sure it is set correctly here!\nWe can also see lots of information about the network’s settings. For example, we can see the subnet IP here, and the subnet mask here. By clicking on the NAT Settings button, I can also find the gateway IP. The gateway IP is the IP of the router, which tells your system where to direct outgoing internet traffic. You’ll want to make a note of all three of those, as we’ll need them later to set a static IP in Ubuntu. You can also click the DHCP Settings button to view the settings for the DHCP server, including the range of IP addresses it uses, which can also be very helpful. If you want to change any of these settings, you can click the Change Settings button at the bottom. You’ll need Administrator privileges to make any changes.\nNext, let’s confirm that our VM is using that virtual network. To do so, click the VM menu, then select Settings, then choose the Network Adapter. Make sure the network connection is set to “NAT” here as well.\nOk, now let’s look at the network configuration in Ubuntu. In this video, I’ll discuss how to view and update the network settings using the GUI. There are, of course, many ways to edit configuration files on the terminal to accomplish these tasks as well. However, I’ve found that the desktop version of Ubuntu works best if you stick with the GUI tools.\nYou can access the network settings by clicking the Activities button and searching for Settings, then selecting the Network option. As with Windows, you can right-click the networking icon in the notification area, then select the network connection you wish to change, and choosing the appropriate settings option in that menu.\nOnce in the Settings menu, click the Gear icon next to the connection you’d like to configure. The Details tab will show you the details of the current connection, including the IP address, MAC address, default gateway, and any DNS servers. On the Identity tab, you’ll see that you can edit the name of the connection, as well as the MAC address.\nTo change the network settings, click the IPv4 tab. Here, you can choose to input a manual IP address. If I select that option, I’ll have to enter an IP address, subnet mask, and default gateway. For the IP address, I’ll just make sure that it isn’t in use on the network by picking one outside the DHCP range used by VMware. The subnet mask and default gateway should be the same as the ones you found in the VMware network settings earlier. Finally, we’ll need to enter some DNS servers. Typically, you can just enter the same IP address as your default gateway, as most routers also can act as DNS resolvers as well. You can also use other DNS servers, such as those from OpenDNS or Google, as described in the Lab 3 assignment.\nOnce you have made your changes, click the green Apply button in the upper-right corner to apply your changes. If everything is successful, I should still be able to access the internet. Let’s open a web browser, just to be sure.\nUbuntu includes a number of tools to help troubleshoot and diagnose issues with your network connection. Most of these are accessed via the command line. So, let’s open a Terminal window. First, you can view available network devices using the networkctl command. You can also view their status using networkctl status.\nIf you’ve worked with Linux in the past, you’re probably familiar with the ifconfig command. However, in recent years it has been replaced with the new ip command, and Ubuntu 18.04 is the first LTS version of Ubuntu that doesn’t include ifconfig by default. So, we’ll be using the newer commands in this course.\nThe first command to use is the ip address show command. This command will show you quite a bit of information about each network adapter on your system, including the IP address. You can also find the default gateway using ip route show. The new ip command has many powerful options that are too numerous to name here. I highly recommend reviewing some of the resources linked below the video to learn more about this powerful command.\nIn addition, Ubuntu includes a ping command, very similar to the one included in Windows. It uses the Internet Control Message Protocol, or ICMP, which allows it to send a simple “echo” request to the server. Most servers will respond to that request, allowing you to confirm that you are able to communicate with it properly across the internet. While that may seem like a very simple tool, it can actually be used in very powerful ways to diagnose a troublesome internet connection. On Ubuntu, note that by default the ping command will continuously send messages until you stop the command using CTRL+C. You can also specify the number of messages to send using the -c \u003cnumber\u003e option, such as ping 192.168.0.1 -c 4 to send 4 messages to that IP address.\nSimilarly, the mtr command will use a series of ICMP “echo” messages to trace the route across the internet from your computer to any other system. This is similar to the tracert command on Windows, and, in fact, there is a similar traceroute command which can be installed on Ubuntu. However, it has been deprecated in favor of mtr in recent versions of Ubuntu. See the video on troubleshooting in this module for more information on how to troubleshoot connections using these tools.\nFinally, Ubuntu also has a tool that can be used to examine TCP sockets. Previously, you would use the netstat command for this purpose, but it has been replaced by the new ss command, short for “socket statistics.” For example, using just the ss command will get a list of all sockets, much like what TCPView will show on Windows. You can find just the listening TCP sockets by using ss -lt. As with the other commands, there are many different uses for this command. Consult the resources linked below this video for more information on how to use it.\nWith that information in hand, you should be able to complete Task 2 of this lab assignment, which is to set a static IP address on your Ubuntu VM acting as the server. If you run into any issues, please post in the course discussion forums to get help. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Network Configuration",
    "uri": "/3-core-networking-services/07-ubuntu-network-configuration/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Web Server on Wikipedia LAMP (Software Bundle) from Wikipedia Nginx from Wikipedia Web Server Survey from Netcraft Video Transcript Another common type of application server in many enterprises is a web server. A web server is the software that is used to make websites available on the World Wide Web. So, it is responsible for listening and responding to HTTP requests. The software is typically designed so it can handle many simultaneous connections, and even serve multiple websites as we saw in Lab 5. Additionally, many web servers support features for caching and server-side scripting languages, making them a truly versatile platform for serving both web pages and web-based applications.\nThere are a three web servers that are currently the most commonly used on the internet today. We’ll be looking at two of them in this module. The first is Apache. It was one of the earliest web servers, and because it was free and open source, it drove much of the early expansion of the web in the 1990s and 2000s. While Apache is technically available for a number of different platforms, it is most commonly found running on Linux systems. As of September 2018, it runs 23% of all websites on the internet, and 34% of all domain names use Apache, according to Netcraft. While it has been declining in market share over the past several years, Apache is still one of the most commonly used web servers on the internet today.\nOn many systems, Apache is typically part of the larger LAMP software stack. LAMP typically stands for “Linux, Apache, MySQL, PHP” but other databases and scripting languages also fit the initialism. As part of this lab’s assignment, you’ll install a LAMP stack and a web application running on that platform.\nThe next web server we should discuss is Nginx (pronounced “engine-x”). Nginx was developed as a successor to Apache, with a focus on high throughput and the ability to handle a large number of simultaneous connections. According to the documentation, it is able to handle 10,000 inactive connections on just 2.5 MB of RAM. While Nginx doesn’t support as many scripting languages and plugins as Apache, it’s high performance makes it a very popular choice for websites expecting a large amount of traffic. As of September 2018, Nginx powered 19% of websites and 23% of all domains on the internet, and it has been slowly climbing the ranks since its release.\nThe last web server we’ll work with during this lab assignment is Microsoft’s Internet Information Services, or IIS. IIS is included in all versions of Windows, though typically it is not installed by default on the consumer versions of the operating system. Like Apache, IIS is able to support web applications written in the .NET family of languages. As of September 2018, IIS powers 36% of websites on the internet, making it the most popular web server in that regard. However, it is only present on 26% of domains, putting it behind Apache in that metric.\nAs mentioned earlier, both Apache and IIS, as well as Nginx through the use of some additional software, are able to support a large range of scripting languages to power interactive web applications. Some of the more commonly used languages are listed here, from PHP and Python to the .NET family of languages, and even Java and JavaScript. As you work on this lab assignment, you’ll get to learn a bit about how to work with web applications written in a couple of these languages.\nThe next few videos will cover the steps needed to complete the rest of this lab assignment. First, you’ll need to make sure the web server is installed and configured properly. You’ll also have to configure the domain name for the website, and handle installing a security certificate. Finally, you’ll be able to install an application that uses one of the scripting languages supported by the server, and see how it all works together to present a web application to your users.\n",
    "description": "",
    "tags": null,
    "title": "Web Servers Overview",
    "uri": "/6-application-servers/07-web-servers-overview/index.html"
  },
  {
    "content": " YouTube Video Resources Slides How to Manage Microsoft and User Accounts in Windows 10 from Windows Central How to Set Up and Configure User Accounts on Windows 10 from How-To Geek The Geek’s Way of Managing User Accounts and Groups in Windows from Digital Citizen Video Script Once you have installed Windows 10, the next step is to configure the user accounts and groups needed on the system. There are three major methods for configuring user accounts, and each give you access to different sets of features. They are the Settings menu, the Control Panel, and the Administrative Tools. Let’s look at each one.\nFirst, the Settings menu. To find it, go to the Start Menu and select Settings. On the Settings menu, click Accounts. From here, you can click the Sign-in Options link to change your own password and other options. You can also click the Family \u0026 other users option to add additional users to the computer. However, by default it will ask you to provide the phone number or email address for a Microsoft account. To add a local account, you must first click the “I don’t have this person’s sign-in information” link, then “Add a user without a Microsoft account” to get to the correct screen. From there, you can simply set a username and password for that account.\nOnce the account is created, you can change the account type. Through the Settings menu, you are only given the option to create standard users and administrators.\nNow, let’s look at using the Control Panel to manage user accounts. To find the Control Panel, search for it on the Start Menu. You can also right-click the Start Button to access the Control Panel. Once there, click the User Accounts option twice to get to the User Accounts screen. From here, you can change your account name, something you aren’t able to do from the Settings menu. However, it does not change the username, just the name displayed on the screen. You can also change your own account type here.\nHowever, for most options such as creating a new account, it will refer you back to the Settings menu. There are a few options on the left side of the User Accounts screen that are difficult to find elsewhere, but they are rarely used. The most important one is the ability to create a password reset disk, which is a flash drive that can be used to reset a lost password. However, as we’ll see later in the semester, resetting Windows passwords is trivial provided the disk isn’t encrypted.\nFinally, let’s look at using the Administrative Tools to manage user accounts. The easiest way to access these is to right-click on the Start Button and select Computer Management. On that window, you’ll see Local Users and Groups in the tree to the left, under System Tools. Expanding that option will give you access to two folders: Users and Groups. In the Users folder, you’ll see all available user accounts on the system.\nYou’ll notice that there are a few more accounts listed here than anywhere else, because Windows 10 includes several disabled accounts by default. The first, Administrator, is the actual built-in administrator account on the system. You can roughly compare it to the “root” account on a Linux computer. It has full access to everything on the system, but is disabled by default. Also, it has no password by default. This can create a major security hole, as it could be accidentally (or intentionally) enabled, giving anyone full access to the system. Most organizations choose to set a password on the account as a precaution, but leave it disabled.\nThe second, DefaultAccount, is simply a dummy account which stores the default profile. A copy of this account is made when each new user account is created. Using some tools available from Microsoft, it is possible to customize this account to give each user on the system a set of default settings, such as a desktop background.\nThe last default account, Guest, is the built-in guest account. It can be enabled to allow guests to access the computer. They can run programs, surf the internet, and store files in their folder, but generally cannot access any other user’s folders, install software, or change any system configuration. If you must allow guest access to a computer, this is probably one of the better ways to do so.\nGoing back to Windows, you can create a new user account here very easily. However, there are a few things to be aware of when doing so. First, you can give it a name, username, and password. At the bottom, you’ll see several checkboxes. The first one forces the user to change her or his password when first logging on to the system. I generally recommend not enabling this option, because that will also force the user’s password to expire after a set time. Instead, I recommend unchecking that box, and checking the third one to set the password to never expire. If you would like to enforce password expiration, it is much better to do so using a central directory service such as Active Directory, which we’ll cover in more detail in Module 4.\nOnce you have created a user account, you can right-click on it to edit a few properties. Notice that by right-clicking any account, you are given the option to set the account’s password. This is helpful if a user has forgotten the account password and does not have a reset disk available. However, if the user has made use of Windows file encryption or some other security features of Windows, it may irrevocably destroy their ability to access that information. So, I only recommend using this feature as a last resort.\nLooking at a user’s properties, you can see tabs at the top to configure group memberships and profile information. On the Member Of tab, you can add users to different groups. Notice by default that users are only added to the Users group. If you’d like a user account to be an administrator, you’ll have to manually add that user to the Administrators group here.\nThe profile tab gives additional options for the user account, such as defining a custom location for the user’s profile, home folder, or scripts. However, I recommend not configuring these options on a local computer account. Instead, you can manage these from Active Directory. We’ll cover that in Module 4.\nNext, let’s look at the Groups folder. Here you’ll see several of the groups listed that are included in Windows 10 by default. The three groups to pay attention to are Administrators, Users, and Guests. They correspond to administrator accounts, normal user accounts, and guest accounts. By adding an account to one of those groups, you’ll give it those permissions on the system. There are several other groups available, most of which are not used directly. If you’d like to know more about these groups, consult the relevant Windows documentation. Of course, you can create groups and manage group members from this window as well.\nLastly, let’s briefly look at the User Account Control, or UAC, feature of Windows. You’ve probably seen this pop up from time to time if you use Windows regularly. UAC is used to prevent accounts from changing system settings without getting explicit confirmation of the change from someone with administrator privileges. I recommend leaving UAC at the default setting, as it will prevent malicious programs from making changes to your computer without alerting you first. On earlier versions of Windows, this was a major problem. You can compare this feature to how Mac and Linux computers constantly ask for an administrator account’s password whenever software is installed or system settings are modified.\nFinally, one other thing to look at is the Local Group Policy Editor on Windows. You can find it by searching for it on the Start Menu. This allows you to view and edit the local security policy of your system. For example, let’s go to Windows Settings \u003e Security Settings \u003e Account Policies \u003e Password Policy. Here, I could set policies regarding how often passwords must be changed, how long they must be, and whether they should meet certain complexity requirements. However, as I mentioned before, it is best to leave the local group policy alone, and instead configure group policies using Active Directory. We’ll cover that in Module 4.\nWith this information, you should be able to start Lab 1, Task 2 - Configuring Windows 10. The next pages will continue giving you the information needed to complete that task.\n",
    "description": "",
    "tags": null,
    "title": "Windows 10 User Management",
    "uri": "/1-secure-workstations/07-windows-user-management/index.html"
  },
  {
    "content": "Automatic deployment \u0026 99.9999% uptime.\n",
    "description": "",
    "tags": null,
    "title": "Backups, Monitoring \u0026 DevOps",
    "uri": "/7-backups-monitoring-devops/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Data Loss on Wikipedia 10 Common Causes of Data Loss from Consolidated Technologies Backup Types Explained: Full, Incremental, Differential, Synthetic, and Forever-Incremental from Nakivo Understanding RPO and RTO from Druva What is High Availability? from DigitalOcean Law Firm Retools its Backup Scenario from Network World Postmortem of Database Outage of January 31 from GitLab Video Transcript Backups are an important part of any system administrator’s toolkit. At the end of the day, no amount of planning and design can completely remove the need for high quality, reliable backups of the data that an organization needs to function. As a system administrator, you may be responsible for designing and executing a backup strategy for your organization. Unfortunately, there is no “one size fits all” way to approach this task, as each organization’s needs are different. In this video, I’ll discuss some different concepts and techniques that I’ve acquired to help me think through the process of creating a backup strategy.\nTo begin, I like to ask myself the classic questions that any journalist begins with: “Who? What? When? Where? Why? How?” Specifically, I can phrase these to relate to the task of designing a backup strategy: “Who has the data? What data needs to be stored? When should it be backed up? Where should those backups be stored? Why can data loss occur in this organization? How should I create those backups?” Let’s look at each question in turn to see how that affects our overall backup strategy.\nFirst, who has the data? I recommend starting with a list of all of the systems and people in your organization, and getting a sense of what data each has. In addition, you’ll need to identify all of the servers and network devices that will need analyzed. Beyond those systems, there may be credentials and security keys stored in safe locations around the organization that will need to be considered. Finally, especially when dealing with certain types of data, you may also need to consider ownership of the data and any legal issues that may be involved, such as HIPAA for health data, or FERPA for student data. Remember that, storing data for your customers doesn’t mean that you have any ownership of that data or the intellectual property within it.\nNext, once you’ve identified where the data may be stored, you’ll need to consider the types of data that need to be stored. This could include accounting data and personnel files to help your organization operate smoothly, but also the web assets and user data that may be stored on your website. Beyond that, there is a plethora of data hiding on your systems themselves, such as the network configuration, filesystems, and even the metadata for individual files stored on the systems. At times, this can be the most daunting step once you start to consider the large amount of decentralized data that most organizations have to deal with.\nOnce you have a good idea of the data you’ll need to back up, the next question you’ll have to consider is when to make your backups. There are many obvious options, from yearly all the way down to instantaneously. So, how do you determine which is right for your organization?\nOne way to look at it is to consider the impact an issue might have on your organization. Many companies use the terms Recovery Point Objective, or RPO, and Recovery Time Objective, or RTO, to measure these items. RPO refers to how much data may be lost when an issue occurs, and RTO measures how long the systems might be unavailable while waiting for data and access to be restored. Of course, these are goals that your organization will try to meet, so the actual time may be different for each incident.\nThis timeline shows how they are related. In this example, your organization has created a backup at the point in time labelled “1” in this timeline. Then, some point in the future, an incident occurs. From that point, your organization has an RPO set, which is the maximum amount of data they would like to lose. However, the actual data loss may be greater, depending on the incident and your organization’s backup strategy. Similarly, the RTO sets a desired timeline for the restoration of data and service, but the actual time it takes to restore the data may be longer or shorter. So, when creating a backup strategy, you’ll need to choose the frequency based on the RPO and RTO for your organization. Backups made less frequently could result in a higher RPO, and backups that are less granular or stored offline could result in a higher RTO.\nYou will also have to consider where you’d like to store the data. One part of that equation is to consider the type of storage device that will be used. Each one comes with its own advantages and disadvantages, in terms of speed, lifetime, storage capacity, and, of course, cost. While it may seem counter-intuitive, one of the leading technologies for long-term data storage is still the traditional magnetic tape, as it already has a proven lifetime of over 50 years, and each tape can hold several terabytes of data. In addition, tape drives can sustain a very high read and write speed for long periods of time. Finally, for some very critical data, storing it in a physical, on-paper format, may not be a bad idea either. For passwords, security keys, and more, storing them completely physically can prevent even the most dedicated hacker from ever accessing them.\nBeyond just the storage media, the location should also be considered. There are many locations that you could store your backup data, relative to where the data was originally stored. For example, a backup can be stored online, which means it is directly and readily accessible to the system it came from in case of an incident. Similarly, data can be stored near-line, meaning that it is close at hand, but may require a few seconds to access the data. Backups can also be stored offline, such as on a hard disk or tape cartridge that is disconnected from a system but stored in a secure location.\nIn addition, data can be stored in a variety of ways at a separate location, called an offsite backup. This could be as simple as storing the data in a secure vault in a different building, or even shipping it thousands of miles away in case of a natural disaster. Lastly, some organizations, such as airlines or large retail companies, may even maintain an entire backup site, sometimes called a “disaster recovery center,” which contains enough hardware and stored backup data to allow the organization to resume operations from that location very quickly in the event of an incident at the primary location.\nWhen storing data offsite or creating a backup site, it is always worth considering how that site is related to your main location. For example, could both be affected by the same incident? Are they connected to the same power source? Would a large hurricane possibly hit both sites? Do they use the same internet service provider? All of these can be a single point of failure that may disable both your primary location and your backup location at the same time. Many companies learned this lesson the hard way during recent hurricanes, such as Katrina or Sandy. I’ve posted the story of one such company in the resources section below this video.\nOptimization is another major concern when considering where to store the data. Depending on your needs, it may be more cost-effective or efficient to look at compressing the backup before storing it, or performing deduplication to remove duplicated files or records from the company-wide backup. If you are storing secure data, you may also want to encrypt the data before storing it on the backup storage media. Finally, you may even need to engage in staging and refactoring of your data, where backup data is staged in a temporary location while it is being fully stored in its final location. By doing so, your organization can continue to work with the data without worrying about their work affecting the validity of the backup.\nAnother important part of designing a backup strategy is to make sure you understand the ways that an organization could lose data. You may not realize it, but in many cases the top source of data loss at an organization is simply user error or accidental deletion. Many times a user will delete a file without realizing its importance, only to contact IT support to see if it can be recovered. The same can happen to large scale production systems, as GitLab experienced in 2017. I’ve posted a link to their very frank and honest post-mortem of that event in the resources section below this video.\nThere are many other ways that data could be lost, such as software or hardware failure, or even corruption of data stored on a hard drive itself as sectors slowly degrade over time. You may also have to deal with malicious intent, either from insiders trying to sabotage a system, all the way to large-scale external hacks. Lastly, every organization must consider the effect natural disasters may have on their data.\nFinally, once you’ve identified the data to be backed up, when to create the backups, and where to store them, the last step is to determine how to create those backups themselves. There are many different types of backups, each with their own features and tradeoffs. First, many companies still use an unstructured backup, which I like to think of as “just a bunch of CDs and flash drives” in a safe. In essence, they copy their important files to an external storage device, drop it in a safe, and say that they are covered. While that is definitely better than no backup at all, it is just the first step toward a true backup strategy.\nUsing software, it is possible to create backups in a variety of ways. The first and simplest is a full backup. Basically, the system makes a full, bit by bit, copy of the data onto a different storage device. While this may be simple, it also requires a large amount of storage space, usually several times the size of the original data if you’d like to store multiple versions of the data.\nNext, software can help you create incremental backups. In this case, you start with a full backup, usually made once a week, then each following day’s backup only stores the data that has changed in the previous day. So, on Thursday, the backup only stores data that has changed since Wednesday. Then, on Friday, if the backup needs to be restored, the system will need access to all backups since the full backup on Monday in order to restore the data. So, if any incremental backup is lost, the system may be unrecoverable to its most recent state. However, this method requires much less storage than a full backup, so it can be very useful.\nSimilarly, differential backups can also be used. In this case, each daily backup stores the changes since the most recent full backup. So, as long as the full backup hasn’t been lost, the most recent full backup plus a differential backup is enough to restore the data. This requires a bit more storage than an incremental backup, but it can also provide additional data security. It is also possible to keep fewer full backups in this scenario to reduce the storage requirements.\nFinally, some backups systems also support a reverse incremental backup system, sometimes referred to as a “reverse delta” system. In this scenario, each daily backup consists of an incremental backup, then the backup is applied to the existing full backup to keep it up to date. Then, the incremental backups are stored as well, allowing those changes to be reversed if needed. This allows the full backup to be the most recent one, making restoration much simpler than traditional incremental or differential backups. In addition, only the most recent backup is needed to perform a full recovery. However, as with other systems, older data can be lost if the incremental backups are corrupted or missing.\nLastly, there are a few additional concerns around backup strategies that we haven’t discussed in this video. You may need to consider the security of the data stored in your backup location, as well as how to go about validating that the backup was properly created. Many organizations perform practice data recoveries often, just to make sure the process works properly.\nWhen creating backups, your organization may have a specific backup window, such as overnight or during a planned downtime. However, it may be impossible to make a backup quickly enough during that time. Alternatively, you may need to run backups while the system is being used, so there could be significant performance impacts. In either case, you may need to consider creating a staging environment first.\nAlso, your organization will have to consider the costs of buying additional storage or hardware to support a proper backup system. Depending on the metric used, it is estimated that you may need as much as 4 times the storage you are using in order to create proper backups. Lastly, as we discussed earlier, you may need to consider how to make your systems available in a distributed environment, in case of a large-scale incident or natural disaster.\nOf course, don’t forget that backups are just one part of the picture. For many organizations, high availability is also a desired trait, so your system’s reliability design should not just consist of backups. By properly building and configuring your infrastructure, you could buy yourself valuable time to perform and restore backups, or even acquire new hardware in the case of a failure, just by having the proper system design.\nHopefully this overview of how to create a backup strategy has given you plenty of things to consider. As part of the lab assignment, you’ll work with a couple of different backup scenarios in both Windows and Ubuntu. I also encourage you to review your own personal backup plan at this point, just to make sure you are covered. Sometimes it is simple to analyze these issues on someone else’s environment, but it is harder to find our own failures unless we consciously look for them.\n",
    "description": "",
    "tags": null,
    "title": "Backups",
    "uri": "/7-backups-monitoring-devops/08-backups/index.html"
  },
  {
    "content": " YouTube Video Resources Slides 5 Tips for Managing Database Servers in Production from Cloud 66 10 Essential Performance Tips for MySQL by Baron Schwartz on InfoWorld 10 Easy Tips for Better SQL Server Performance from ITPro Today What Kind of Disks Should You Use with Microsoft SQL Server 2016? from Heroix Blog Database Encryption on Wikipedia Encrypting Data at Rest on Servers: What Does it Get You? from HLN Consulting Video Transcript One of the other most commonly used application servers in an enterprise is a database server. You may also see the term “Database Management System” or “DBMS” used in some areas to refer to the same thing. A database server is typically the backbone of your organization’s data storage, as it can be used to store and retrieve a large amount of data very efficiently. Depending on the server software, the data may be stored in large files on the file system, or the database server may work directly with a block-level storage device for even more performance.\nOf course, since a database server is constantly reading and writing data, it has some very unique performance and storage needs. We’ll discuss a few of those at the end of this video. In addition, as a system administrator, you’ll definitely be tasked with creating backups, and you may also handle replication across multiple database servers. We’ll spend a bit of time discussing backups in Module 7.\nUnfortunately, working with database servers can be one of the most complex tasks a system administrator handles, and, in fact, many organizations use a different title, “database administrator” or “DBA,” to refer to staff who are primarily responsible for working with database servers. Because it is such a complex topic, we really won’t be spending much time working directly with database servers other than performing basic setup and configuration. If you are interested in working with a particular database server, I encourage you to read some of the information in the resources section below the video to learn more about this topic.\nThere are a few different database systems that are commonly used in industry today. Unlike web servers, where the vast majority of organizations use one of just three systems, there are many more database servers widely in use today. I’ll briefly talk about a few of the most common that you’ll come across today.\nFirst is MySQL, and its fork, MariaDB. MySQL was originally developed in the late 1990s as an open source project, freely licensed under the GPL. As with Apache, it gained widespread use and acceptance among Linux enthusiasts, and was a commonly used database in many open source projects such as WordPress and Drupal, as well as major websites such as Flickr, YouTube, and Twitter. In 2010, MySQL was acquired by Oracle, and while it still retains its open source status at this time, many developers feared that Oracle may eventually move the software to a proprietary license. So, MariaDB was created as a fork of MySQL to maintain a public license. MariaDB maintains full compatibility with MySQL, and the two systems can be used pretty much interchangeably in practice. As part of this lab assignment, you’ll install and work with MySQL in the cloud.\nAnother commonly used database system is Postgres. Postgres is very similar to MySQL in terms of licensing and features, but it aims to be as “standards compliant” as possible. Because of this, many professionals prefer Postgres due to the fact that it meets some of the standards that MySQL does not.\nMicrosoft also has their own database server, named Microsoft SQL Server. This is typically used along with the .NET family of programming languages on Windows systems. While we won’t work directly with Microsoft SQL Server, as part of the lab assignment you’ll install a .NET web application which most likely uses a local database that is similar to Microsoft SQL Server.\nOracle, of course, is another major database system in use today. They are well known for providing enterprise-level database software, and have been doing so since the 1970s. Because of this, many older organizations have been using Oracle’s database software for some time, including K-State. Most of K-State’s central systems, including KSIS and HRIS, are built on top of Oracle database products.\nLastly, there are a number of new database systems that include features such as document or object storage and “NoSQL” schemas. One of the most popular of those is MongoDB. These systems are most commonly used with web applications and some big data analytics packages.\nAs I mentioned earlier, working with a database server presents some very interesting and unique performance considerations that you may have to deal with as a system administrator. First, you’ll definitely be concerned with the amount of RAM that the system has available. Ideally, you’d like to have plenty of RAM available for the system so that it can hold a large amount of data and indexes directly in memory, making requests as fast as possible. You’ll also want to prevent paging of data if at all possible. If the server is unable to store everything it needs directly in RAM, it can make your server up to 50 times slower as it handles paging.\nYour CPU speed is also a factor, as that impacts how quickly the system can perform indexing and querying of the data, especially if many complex queries are needed. In addition, another major concern is the read and write speeds of the filesystem. You may want to consider using high performance SSDs in your database server, coupled with a RAID configuration using RAID 1+0 or RAID 6 to gain additional performance and data security. There is a great discussion of these storage considerations linked in the resources section below the video.\nBeyond the hardware, you may also have to deal with issues such as the network speed and throughput to your database server. In some cases, you may even need to install multiple network interfaces for a database system if you find that the network interface is constantly saturated but the database server itself is not running near capacity. Also, many database servers maintain a separate log file from the data, which is used to verify that data updates are made properly. For large systems, it is generally recommended to store that log file in a separate location, so that updates to both the data and the log file can be done in parallel. Finally, as with any system, you’ll want to set up plenty of monitoring and alerts on your database server, so you can quickly respond to problems as they arise. We’ll discuss a bit of information about monitoring in Module 7.\nYou’ll also need to spend some time thinking about the security of your database server. Just like with any other system, you’ll want to make sure any network connections to and from this server are properly secured. Most database servers support using TLS to secure the connection between a database server and the application using it, but in many cases it must be configured and is not enabled by default. In general, it is also recommended to prevent external access to your database server. This is generally done through a firewall that only allows incoming connections from the application servers or a select number of internal IP addresses.\nAnother major concern with database servers is encryption. Depending on the type of data you are working with, there may be two different levels of encryption to consider. First, you should consider encrypting the data when it is at rest, or stored on the disk itself. Typically this is handled through full-disk encryption, file system encryption, or even encryption of the database tables themselves through the database server software. Each option comes with tradeoffs in terms of performance and security, so you should carefully research the options to determine the best choice for your environment.\nEncrypting the data at rest, unfortunately, does not protect the data if a malicious user manages to gain access to the database system itself while it is running. So, you may also want to encrypt data stored in the tables themselves so that it is only readable by the application using the data. Some examples of data you may want to encrypt are usernames, email addresses, phone numbers, and any other personal information for your users. The details of how to do this properly are definitely outside the scope of this course, but there are many great resources online and elsewhere to learn how to secure data stored in an application.\nLastly, as a system administrator, you may want to keep an eye out for data being copied from your database server in an unexpected way. Typically, when a malicious user gains access to a database server, the first thing she or he will do is try to get a copy of that data on their own systems. This is called “exfiltration,” and is a major concern for enterprises. As part of your security and monitoring of your database server, you may want to watch for unexpected network connections leading outside of your organization, especially if they are coupled with an unusually large amount of network traffic. It could be a sign that someone is trying to get a copy of your databases.\nAs I mentioned earlier, this is just a brief introduction to working with database servers. You’ll get a bit of experience with MySQL in this lab assignment, but if you are interested in learning more, I encourage you to take courses in database systems and seek additional resources online. Database administrators are always in demand, and it is a great career path to pursue.\n",
    "description": "",
    "tags": null,
    "title": "Database Servers",
    "uri": "/6-application-servers/08-database-servers/index.html"
  },
  {
    "content": "Greetings! Here is the information you’ll need to know to get your lab graded.\nGrading Date/Time: \u003ctime\u003e Contact me ASAP if that time will not work for you for any reason and I will work with you to reschedule. Zoom Link: \u003clink\u003e If you haven’t used Zoom before, I recommend going to https://ksu.zoom.us/ first to download the Zoom client software. Then, when the time comes, click the link above to join the Zoom session. Your Setup: Before you join the Zoom session for grading, please have the following steps completed on your end. This helps things go as smoothly as possible: Use the fastest internet connection available to you. If you are using a laptop with a wireless connection, you may want to plug in to a hard-wired connection if available. If you can work from campus, that may be best, depending on your home internet connection. Make sure your audio input device (microphone) is working in Zoom. You are not required to have a webcam, but you are welcome to use one if you like. I will be using a webcam and microphone on my end. Confirm that all the VMs for this lab are booted and running on your system. We’ll be using the desktop sharing features of Zoom so I can see your VMs. So, make sure you close any windows or programs that are running in the background that shouldn’t be shared with me. Operations: Here are a few operations you may be asked to demonstrate: Static IP address on Windows Server Active Directory users Windows 10 client on Active Directory OpenLDAP OUs, groups \u0026 users Ubuntu client authenticate via LDAP Interoperability - Ubuntu client on Active Directory, or Windows client on Samba domain LDAPSearch screenshots (2) Finality Once you begin the grading process, no changes may be made to your system to fix any errors encountered. The grade you receive will reflect the state of your VMs at the start of the grading process. This is to ensure that grades are fairly determined and that no special consideration is given. That should cover everything. Please let me know if you have any questions prior to your scheduled grading time.\nGood luck!\nRuss\n",
    "description": "",
    "tags": null,
    "title": "Lab 4 Grading Email",
    "uri": "/z-instructor-resources/08-lab-4-grading-email/index.html"
  },
  {
    "content": " YouTube Video Resources Language: Variables from Puppet Facter: Core Facts from Puppet Values and Data Types from Puppet Templates from Puppet Video Script In this video, we’ll go just a bit deeper into the Puppet language and some additional features of Puppet. You may not need all of this information to complete Lab 2, but it is definitely helpful.\nFirst, as with any programming language, Puppet allows you to assign variables. The basic syntax for assigning a variable is:\n$my_variable = \"Some text\" As with many scripting languages, a variable name always begins with a dollar sign $, and is assigned with a single equals sign =. Unlike most languages, however, variables can only be assigned once. If you think about it, this makes sense, since Puppet may apply resources in any order. In this way, it is much more of a declarative language than an imperative language, though it shares some features of both.\nYou can then use variables in your manifest files. For example, a variable is interpolated in any double-quoted string, such as here:\n$username = \"russfeld\" notify { \"Your home directory is /home/${username}\": } Puppet also has a very powerful templating language that can be used with configuration files. It is outside the scope of what I’ll cover in this class, but I encourage you to look into it on your own if you are interested.\nAnother tool bundled with Puppet is Facter. Facter is able to provide information about the system it is running on. Puppet can then use that information in manifest variables and templates to customize the system’s configuration. To see the facts available on your system, type:\nfacter -p Of course, you’ll find that there are many facts available. You can find a full list of facts in the Puppet documentation linked below this video. You can also write custom facts, if needed.\n",
    "description": "",
    "tags": null,
    "title": "Puppet Variables \u0026 Facts",
    "uri": "/2-configuration-management/08-puppet-variables-facts/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Docker Tutorial for Beginners by TechWorld with Nana on YouTube Nginx Reverse Proxy from Phoenix NAP Using Docker to Set up Nginx Reverse Proxy With Auto SSL Generation from Linux Handbook Docker Security from Docker Protect the Docker daemon socket from Docker Docker Security Cheat Sheet from OWASP Traefik Quickstart from Traefik Video Transcript As we’ve seen, deploying services using Docker can make things very easy. Unfortunately, one major limitation of Docker is that the only way connect a running container to the outside world is by mapping a port on the host system to a port on the container itself. What if we want to run multiple containers that all host similar services, such as a website? Only one container can be mapped to port 80 - any other containers would either have to use a different port or be running on another host. Thankfully, one way we can get around this is by using a special piece of software known as a reverse proxy. In a nutshell, a reverse proxy will take incoming requests from the internet, analyze them to determine which container the request is directed to, and then forward that request to the correct container. It will also make sure the response from the container is directed to the correct external host. We call this a reverse proxy since it is acting as a connection into our Docker network, instead of a traditional proxy that helps containers reach the external internet.\nAs it turns out, the Nginx web server was originally built to be a high-performance reverse proxy, and that is still one of the more common uses of Nginx even today. In addition, there are many other Docker images that can help us set up and manage these reverse proxy connections. In this lesson, we’ll review three different ways to create a reverse proxy in our Docker infrastructures.\nThe first option is to use Nginx directly. For this, we’re going to set up a Docker Compose file that contains 3 different containers. First, we have a standard Nginx container that we are naming proxy, and we are connecting port 8080 on our host system to port 80 on this container for testing. In practice, we’d generally attach this container to port 80 on our host system as well. Notice that this is the only container connected to the default Docker network, so it is the only one that is meant to access the internet itself. Finally, we’re using a bind mount on a volume that will contain a template configuration file for Nginx - we’ll look at that a bit later.\nThe other two containers are simple whoami containers. These containers will respond to simple web requests and respond with information about the container, helping us make sure we are reaching the correct container through our reverse proxy. These two containers are both connected to the internal network, which is also accessible from the proxy container as well.\nTo configure the Nginx server as a reverse proxy, we need to provide a template for configuration. So, inside of the folder mounted as a volume in the container, we need to create a file named default.conf.template that will be used by Nginx to generate it’s configuration file. The contents of that file are shown here on the slide.\nIn effect, we are creating two different servers, one that listens for connections sent to hostname one.local, and the other will look for connections to two.local. Depending on which hostname is specified in the connection, it will forward those connections to either container whoami1 or whoami2. We also include a bunch of various headers and configuration settings for a reverse proxy - these can be found in the Nginx documentation or by reading some of the tutorials linked at the top of this page.\nSo, now that we’ve set up our environment, let’s get it running using docker compose up -d. After all of the containers are started, we can use docker ps to see that they are all running correctly.\nNow, let’s test a connection to each container. We’ll use the simple curl utility for this, and include the -H parameter to specify a hostname for the connection. So, by sending a connection to localhost on port 8080 with the hostname one.local specified, we should get a response from container whoami1. We can confirm that this is correct by checking the response against the output of docker ps. We can do the same to test host two.local as well. It looks like it is working!\nSo, with this setup in place, we can replace the two whoami containers with any container that has a web interface. It could be a static website, or an application such as PHPMyAdmin that we’d like to include as part of our infrastructure. By setting up a reverse proxy, we can access many resources on the same host and port, simply by setting different hostnames. So, all we’d need to do is register the various hostnames in our DNS configuration, just like we did when working with virtual hosts in Apache as part of Lab 5, and point them all at the same IP address where our Docker infrastructure is hosted. That’s all it takes!\nAnother option for setting up a reverse proxy in Nginx is to use the nginx-proxy Docker image. This image includes some special software that can analyze our Docker configuration and automatically set up the various configurations needed for a reverse proxy in Nginx. This Docker Compose file shows the basic way this setup works.\nFirst, we are creating a new Docker container named proxy based on the nginx-proxy image. We’re still mapping port 80 inside of the container to port 8080 on our host system, and connecting it to the same networks as before. However, this time instead of mounting a volume using a bind mount that stores our Nginx configuration templates, we are creating a bind mount directly to the docker.sock socket. This is how the Docker client is able to communicate with the Docker engine, and we are effectively giving this Docker container access to the underlying Docker engine that it is running on. This introduces a security concern, which we will address later in this lesson.\nThe only other change is the environments for the two whoami containers. Notice that each container now contains a VIRTUAL_HOST and VIRTUAL_PORT entry, denoting the hostname that the container should use, and the port within the container where the connections should be sent to.\nNow, when we bring up this infrastructure using docker compose up -d, we should be able to do the same test as before to show that the connections for one.local and two.local are being directed to the correct containers. As we can see here, it looks like it worked! But, how does nginx-proxy know how to configure itself?\nBasically, the nginx-proxy software will inspect the configuration of the various containers available in the Docker engine, using the bind-mounted connection directly to the docker.sock socket. By doing so, it will see containers that have a VIRTUAL_HOST environment variable, and optionally a VIRTUAL_PORT variable, and it will configure a reverse proxy to that container using the hostname and port specified. So, if we add a third container to our Docker infrastructure, as long as it has a VIRTUAL_HOST environment variable set, nginx-proxy will automatically see it as soon as it starts, and it will set up a new reverse proxy to that container. This is very handy, as it allows us to start and stop containers as needed, and a reverse proxy will always be available without any additional configuration needed!\nUnfortunately, as I mentioned earlier, there is a major security concern introduced when we allow a Docker container to have access to the docker.sock socket on the host system. As we already saw, this allows the software running in the Docker container to access the underlying Docker engine, which is great since it can basically perform all of the configuration we need automatically, and it can even detect when new containers are started.\nHowever, there is nothing at all preventing the software in that container, or a malicious actor who has gained access inside of the container, from performing ANY other actions on the Docker engine. So, anyone with access inside of that container could start new containers, stop running containers, change the configuration of an existing container, and so much more.\nIn addition, in most installations the Docker engine runs as the root user of the host system! This means that anyone who has access to the Docker engine and armed with an unpatched flaw in Docker could easily gain root access to the entire host system! In short, if you have root access in a container that has access to the Docker socket, you can effectively end up with root access on the host system.\nObviously this is a very major security concern, and one that should definitely be taken into account when using Docker containers that need access to the underlying Docker engine. Thankfully, there are a few mitigation methods we can follow. For instance, we can protect access to the Docker socket with a TLS certificate, which prevents unauthorized access by any Docker clients who don’t have the certificate. We can also protect access to the Docker socket by running it through a proxy that acts like a firewall, analyzing requests from Docker clients and determining if they are allowed. Finally, in many cases, we may not want to directly expose any container with access to the Docker engine directly to the internet itself, though in this case it would basically nullify the usefulness of Nginx Proxy.\nSo, it ends up being a bit of a tradeoff. If we want to use tools such as nginx-proxy in our environments, we’ll have to analyze the risk that is presented by having the Docker engine available to that container, and either take additional precautions or add additional monitoring to the system.\nA third option for setting up a reverse proxy would be to use a Docker image designed specifically for that purpose. There are many of them available, but the most popular at this point is the Traefik Proxy server. Traefik Proxy is very similar to Nginx Proxy, since it also will examine the underlying Docker engine to configure reverse proxies. However, it includes many additional features, such as the ability to proxy any TCP request, introduce middleware, and automatically handle requesting TLS certificates from a provider like Let’s Encrypt.\nThe Docker Compose file shown here is a minimal setup for the Traefik Proxy server. In the proxy container, we see that we are customizing the startup command by providing two options, --api.insecure=true and --providers.docker. This will allow us to access the Traefik Proxy dashboard, and tells Traefik to look at Docker for information about proxies to be created. We’re also mapping two ports to the Traefik proxy - first we are connecting port 8080 on the host to port 80 on the proxy, which will handle HTTP traffic. In addition, we are connecting port 8081 on the host to port 8080 inside of the container, which is the port where we can access the Traefik Proxy dashboard. We’ll take a look at that a bit later.\nThen, we set up two whoami images just like before. However, this time, instead of setting environment variables, we add a couple of labels to the containers. These labels are used to tell Traefik Proxy what hostname and port should be used for the reverse proxy, just like we saw earlier in our configuration for Nginx Proxy.\nSo, just like before, we can use docker compose up -d to start these containers. Once they are up and running, we can once again use docker ps to see the running containers, and we can use curl to verify that the proxy is working and connecting us to the correct containers.\nTo explore the configuration of our Traefik Proxy, we can also load the dashboard. To do this, we’ll simply open a web browser and visit http://localhost:8001. This slide shows a screenshot of what it may look like for our current configuration.\nJust like with Nginx Proxy, Traefik will automatically add proxy routes for any new containers that have the correct labels, and we can see them update here on this dashboard in real time. Of course, right now this dashboard is completely unsecured and totally open to anyone who has access to our system, so in practice we’ll probably want to secure this using some form of authentication. The Traefik Proxy documentation includes information for how to accomplish that.\nSo, in summary, setting up a reverse proxy is a great way to allow users from the outside internet to have access to multiple servers running on the same Docker host. In effect, we are able to easily duplicate the features of Virtual Hosts in traditional webservers, but instead of loading files from a different directory, we are directing users to a different Docker container inside of our infrastructure.\nWe looked at three reverse proxies - Nginx itself, Nginx Proxy, and Traefik Proxy. Of course, there are many others out there that we can choose from - these are just three that are pretty well known and used often today.\nFinally, it is worth noting that a reverse proxy such as this is generally only needed for Docker containers hosted on a single host. If we choose to use orchestration platforms such as Kubernetes, much of this is handled by the platform itself. So, in the next lesson, we’ll explore Kubernetes a bit and see how it differs from what we’ve seen so far in Docker.\n",
    "description": "",
    "tags": null,
    "title": "Reverse Proxy",
    "uri": "/5a-containers/08-reverse-proxy/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Video Script Hello, and welcome to the weekend Announcements video for CIS 527 In summer 2022. This week we should be wrapping everything up in the class. So you’ve got week seven discussion, which is due today by 1159. And then lab seven and the final project are due on Friday both by 1159. As well, you also should be receiving an email inviting you to complete a TEVAL for this course later today. So make sure you take care of your TEVALs. That information is really really helpful to me as I continue to improve and work on this course.\nSo for lab seven grading, you don’t need to schedule a meeting with me for lab seven, I’ve reconfigured the lab so that you can do it completely online. For task one, you’ll submit a set of screenshots showing me that you were able to successfully backup and restore a Windows Active Directory. For task two, you’re going to submit a zip file and a readme that shows your backup process. For task three, I just need the URL where I can find your installation of Munin or Ganglia so I can check to see that that’s working. And then for a task four, you’ll send me the GitLab that you’ve created, you’ll add add me to it as a collaborator. And then you’ll send me the URL where I can actually find that GitLab on your server so that I can check and see as I make changes, do they get deployed to the server properly using a web hook. And then if you do do the extra credit, you’ll send me some screenshots as well.\nSo for your final project, we talked a little bit about this last week. But the big things I need from your final project is a written report, I highly recommend that you use the template that I provide, just to get the headings on it. Remember, the template uses bullets, but your project should not use bullets, it should use paragraphs in that template. So make sure you write that in a professional format. You’re going to spend your time researching, proposing and doing SWOT analysis, I really want you to focus on the SWOT analysis, and especially on the opportunities and threats and weaknesses. Those tend to be the ones that students really struggle with, they do a pretty good job getting the strengths figured out. But sometimes the weaknesses and the threats really need some work. You can include graphics, data, charts, whatever as needed to make your points, then you’re going to do a live presentation from 15 to 30 minutes long. You can either do that live to me via zoom, or you can record it and we’ll talk about that in just a second. Basically, pretend I’m your CIO, and you’re trying to convince me that you’ve analyzed your proposal well, and that it supports your conclusion. Don’t forget to include a conclusion at the end that either shows that you should go forward with this project or your SWOT analysis might show that you should not go forward with this project, make sure you make clear in your conclusion which one you’re doing and how you’re supporting that. And then finally, you should have a small prototype that you’ll demo, it should be pretty easy to do spend maybe two or four hours on the prototype, not a whole lot of time.\nSo the presentation you can do live or pre recorded. If you want to do it live schedule a time to present with me, I recommend scheduling that for at least a half hour, probably a full hour to give enough time for q\u0026a and setup. If you want to do a recorded presentation, you’ll record the presentation and send it to me ahead of time. And then you’ll need to schedule a time to do q\u0026a After I’ve watched your video. So I’ll watch your video first. And then I will do q\u0026a. If you don’t schedule a time for q\u0026a After the recorded video, I will take some points off. So to account for that I really do want to have some time to do interaction and questions and answers, even if you record the video. So schedule those now if you need to. You can also email me for any alternate arrangements if you’re not able to present during the day. So just let me know.\nSo for this week, I’m going to be sending out TEVALs so make sure that you respond to those quickly. And honestly, all of your comments and feedback are welcome. It really really helps me as I continue to improve this course. So any feedback you have for good or for bad is really, really helpful to me. So please take some time to fill out your TEVALs. For Final Grades, check your grade in Canvas, I will send out an email when everybody’s grades are finalized, you can email me if you have any concerns, please let me know if there’s anything missing or incorrect ASAP. So I can go through and get that corrected. It’s much easier to do that now than it is next Monday when I’ve got grades due immediately.\nOther than that, feel free to keep in touch. We’ve got discussions on Discord. I’ve got Tea Time office hours, Tuesdays at 330 Fridays at 1030. You can join me there. I’ve also got one on one office hours. So lots of different ways you can get in touch and get some help on finishing your final project. But hopefully you get to the point where you’ve solved it, you’ve cracked the code, everything is working. And hopefully at that point, you continue to have purpose in your life. Hopefully you’ve got other things you want to continue working on outside of this course. But as always, if you have any questions, let me know and I will talk to you later this week.\n",
    "description": "",
    "tags": null,
    "title": "Summer '22 Week 8",
    "uri": "/y-announcements/old/summer2022/week08/index.html"
  },
  {
    "content": " YouTube Video Resources Network Hardware Troubleshooting Flowchart from Foner Books Basic Network Troubleshooting from Computer Hope 7 Simple Steps to Diagnose a Network Problem from MakeUseOf Windows Network Troublehshooting Guide from MakeUseOf The Art of Network Troubleshooting by Greg Schaffer on Computerworld Video Transcript When you are working with network connections, you’ll inevitably run into issues. Understanding how to perform basic network troubleshooting is a very important skill for any system administrator to have, and it is one that you’ll find yourself using time and time again. In this video, I’m going to briefly review some of the steps that you can take to diagnose and fix network issues.\nAs a quick side-note, many of the resources linked below this video still refer to older Linux commands such as ifconfig and traceroute instead of their newer counterparts. So, you may have to translate the commands a bit to get them to work on newer systems, but the process and theory itself should still apply.\nIn addition, many of these troubleshooting tools can be blocked by restrictive firewall rules. So, you’ll need to be aware of your current firewall configurations and adjust as necessary. If possible, you could disable firewalls for testing, but that also could create a security concern. So, make sure you keep in mind the fact that firewalls can also be the root cause of many of these symptoms.\nFirst, when faced with any unknown network issue, the very first step is to reboot any and all devices involved if possible. This includes your computer, routers, switches, modems, and any other networking devices along the path between your system and the intended destination. While this may seem drastic, in many cases it could be a simple fault in the hardware or networking software that a quick reboot will fix, while diagnosing and fixing the error without a reboot may be nearly impossible. Of course, in an enterprise setting, you probably don’t want to reboot your entire network infrastructure each time you have an issue, so you’ll have to examine the tradeoffs before doing so. On a home network, however, rebooting the computer and router is a pretty negligible cost.\nNext, I recommend determining how far across the network you can reach. In effect, this helps you pinpoint the exact root cause of the network issue, and it will tell you where to focus your efforts. This process can be performed on almost any operating system, as long as it has the ping command available, as well as a command to display the current IP address.\nFirst, you’ll need to determine if your computer is able to connect to itself. You can do so by using the ping command to send requests to the loopback interface on your system. To do so, use these two commands:\nping 127.0.0.1 ping localhost The first command pings the IP address of the loopback adapter, ensuring that the networking drivers in the operating system are working properly. If that command returns an error, or is not able to connect to your system, it most likely means your networking hardware or drivers on the local system are failing and need to be fixed. The second command pings the same address, but using the commonly available DNS name localhost. If that command fails, but the first one succeeds, it could mean that the DNS resolver or hosts file on your system is corrupt. If so, that’s where you’ll want to focus your efforts.\nIf you are able to successfully ping yourself, the next step is to determine if you have a valid IP address. You can use either the ipconfig command on Windows or the ip commands on Linux to find your current IP address. Then, use the ping command to make sure you can send and receive messages via that IP address. If that step fails, you may want to release and renew your IP address if you are using DHCP, or verifying that you have the correct IP settings if you are using a static IP address. To release and renew your IP address, use the following commands on Windows:\nipconfig /release ipconfig /renew On Linux, it is a little less straightforward, but the best way to accomplish the same task is by using these commands:\nsudo dhclient -r sudo dhclient If you are able to get a valid IP address using these steps, you can continue on to see if you are able to access the rest of the network. If not, you may need to check your network router settings or static IP address settings to make sure they are correct.\nNext, you’ll want to ping the default gateway address as configured in your system. Generally, that should be the address of your router on your network. You can find that address using the ipconfig and ip commands as described above. Use the ping command to try and reach that address. If it works, that means you are able to successfully contact your router. If not, it could be an issue with the network cables between you and your router, or a misconfiguration of either your router or static IP address. In either case, this is sometimes the most frustrating case to deal with, as you’ve ensured that your computer is working, but it cannot reach the core of your local network. I recommend looking at the Network Hardware Troubleshooting Flowchart from Foner Books to try and work your way through this problem.\nIf you are able to connect with your default gateway, the next step is to see if you can reach your intended destination. If it is another computer on your network or another network, try to ping it’s IP address. If that connection fails, most likely the problem is somewhere between your router and that computer. In that case, you’ll want to try this troubleshooting procedure again from that computer and see if you can pinpoint the problem.\nIf you are trying to reach a website on the internet, there are a couple more steps to perform here. First, if you know the public IP address and default gateway address your internet service provider, or ISP, is providing to you, you can try to ping those addresses. If they do not work, most likely the problem is with your network modem or your local ISP. In that case, you’ll want to get in touch with them to try and resolve the issue.\nNext, you should try to ping a server on the internet by IP address. I usually recommend either the OpenDNS servers, which are 208.67.222.222 and 208.67.220.220, or the Google DNS servers, which are 8.8.8.8 and 8.8.4.4. If you can reach those servers, your connection to the internet is definitely working. If not, the issue is also most likely with your ISP, and you’ll need to contact them to resolve the problem.\nFinally, you should try to ping a few web addresses by the DNS names. I usually recommend using an address that is guaranteed to be available, but one that won’t be cached on the system. I hate to say it, but using search engines other than Google, such as www.bing.com or www.yahoo.com, are all great choices for this step. When you do so, you could receive a message that it is unable to resolve that DNS name into an IP address. If that is the case, you should check your DNS settings. If nothing else, you can always replace them with settings for OpenDNS or Google DNS and test with those addresses.\nIf you are able to resolve the IP address but are unable to reach them, then you could have a firewall issue of some kind. It is very rare that you are able to ping servers by IP address but not via the DNS names, so a firewall is the most likely culprit here.\nOf course, if you are able to reach those sites correctly using ping, then the last step is to open a web browser and try to load one of those webpages. If that fails, then you’ll need to examine either the browser software itself or the firewall. The firewall could be blocking HTTP connections, or the browser could be corrupted and need reinstalled. In either case, it is most likely a software issue at that point.\nThis is just a brief overview of some of the steps you could take to diagnose a network issue. To be honest, I could probably teach an entire course just on this one subject, since there are that many different things that could go wrong in a modern computer network. However, this should give you a set of universal tools you can use to help at least pinpoint the location of the error and narrow your search to a specific device or configuration for the source of the issue. Of course, as a last resort you can always search the internet for additional troubleshooting steps or advice, but remember that sometimes you aren’t even able to do that when your internet isn’t working. So, it helps to have a basic understanding of network troubleshooting and familiarity with a few quick tools to help you out.\n",
    "description": "",
    "tags": null,
    "title": "Troubleshooting",
    "uri": "/3-core-networking-services/08-troubleshooting/index.html"
  },
  {
    "content": " YouTube Video Resources SSSD on Ubuntu Server Guide (look for the LDAP section) Video Transcript Once we’ve set up and configured our OpenLDAP server on Linux, we can configure another VM to act as an LDAP client and use that server for authentication. Thankfully, now that Ubuntu 20.04 uses SSSD, this process is pretty simple.\nFor the most part, we’ll be following the instructions in the Ubuntu Server Guide page linked below this video. It is a pretty in-depth overview of how to configure SSSD to use LDAP for authentication. In fact, if you’ve done the Windows portions of this assignment already, you’ll be familiar with several of these steps.\nFirst, we’ll need to install a couple of packages on this system to allow us to interact with LDAP systems via SSSD:\nsudo apt update sudo apt install sssd-ldap ldap-utils Next, we’ll need to get a copy of our certificate authority, or CA, certificate from our Ubuntu 20.04 VM labelled SERVER. If we followed the instructions in the previous video correctly, we should have created a copy of that file in the home directory of the cis527 user on our server.\nIf not, you can do so by logging on to the SERVER VM as cis527 and using the following command:\ncp /usr/local/share/ca-certificates/mycacert.crt ~/ Then, from the CLIENT VM, we can use scp to copy that file using SSH. Hopefully you were able to get the SSH portion of the lab set up and working correctly. If not, you may have to copy this from the SERVER VM to your host system, and then from your host system back to the CLIENT VM. To copy the file using scp, use a command similar to the following\nscp -P 22222 cis527@192.168.40.41:~/mycacert.crt ~/ Let’s break that command down. First, the -P 22222 tells us the we are using port 22222 to connect to the system, which is how it was configured in Lab 3. Then, we have cis527@, which is the username we’d like to use on the remote system, followed by 192.168.40.41, which is the IP address of our SERVER VM. Next, there is a colon, and a file path, which is ~/mycacert.crt, which is the path to the certificate file on the SERVER VM. Finally, we have a space, and another path ~/ showing where we’d like to place the file on our CLIENT VM.\nOf course, if you want to learn more about SSH and SCP, check out the video in the Extras section for more information.\nOnce we have a copy of the certificate on our CLIENT VM, we can install it using the following commands:\nsudo cp ~/mycacert.crt /usr/local/share/ca-certificates/ sudo update-ca-certificates If that command works correctly, it should tell us that it has added 1 certificate to our list. That means that our CLIENT VM now includes our own self-signed CA in its list of trusted CA certificates. That’s what we want!\nNext, we’ll need to create an SSSD configuration file. Unfortunately, unlike the example we’ll see later with Windows and Active Directory, we’ll have to create this one manually. Here’s an example of what that would look like. First, we’ll open the file:\nsudo nano /etc/sssd/sssd.conf and then put the following contents inside of it:\n[sssd] config_file_version = 2 domains = ldap.cis527russfeld.cs.ksu.edu [domain/ldap.cis527russfeld.cs.ksu.edu] id_provider = ldap auth_provider = ldap ldap_uri = ldap://ldap.cis527russfeld.cs.ksu.edu cache_credentials = True ldap_search_base = dc=ldap,dc=cis527russfeld,dc=cs,dc=ksu,dc=edu Notice that we’ve customized this file to match our LDAP configuration information from the previous video.\nAlso, we must make sure that that file is owned by root:root and only the user has permission to access that file:\nsudo chown root:root /etc/sssd/sssd.conf sudo chmod 600 /etc/sssd/sssd.conf Finally, we can restart the SSSD service to load the new settings:\nsudo systemctl restart sssd If this command returns an error, it usually either means that there is a typo in the sssd.conf file created earlier, or that the file’s permissions are incorrect. So, make sure you double-check that if you run into an error at this point.\nNow, let’s tackle the creation of home directories. This is handled by the Pluggable Authentication Modules, or PAM, framework. There are many ways to do this, including editing a config file manually. However, there is a quick and easy way to do this as well, using the pam-auth-update command:\nsudo pam-auth-update In the menu, press the SPACE key to enable the “Create home directory on login” option, then press ENTER to save the changes.\nFinally, we can do some testing to make sure that things are working correctly. First, we’ll need to be able to access our LDAP server using its DNS name, so we can test that using dig:\ndig ldap.cis527russfeld.cs.ksu.edu Hopefully you were able to get your LDAP server working in the previous lab. If not, you may load some of the model solution files from Lab 3 provided on Canvas to get this part working.\nNext, we’ll make sure we can access the LDAP server\nldapwhoami -x -h ldap.cis527russfeld.cs.ksu.edu This command should return anonymous. If not, your CLIENT VM is having trouble contacting your SERVER VM or the LDAP server itself. In that case, you may want to check your firewall configuration - did you remember to allow the correct ports for LDAP?\nOnce that command works, we can try it again using TLS by adding -ZZ to the command:\nldapwhoami -x -ZZ -h ldap.cis527russfeld.cs.ksu.edu Hopefully this command should return anonymous just like it did previously. If not, there is probably a configuration issue with your TLS certificates that you’ll need to resolve before moving on.\nFinally, if everything looks like it is working, we can try to load information about one of our LDAP users using the getent command:\ngetent passwd russfeld This command should return information about the user we created on our LDAP server. If not, then we may need to check the settings in our sssd.conf file to make sure the LDAP information is correct.\nThankfully, if that worked, we can try to log on to the system as that user using the su command:\nsu russfeld It works! We can even go to our home directory and see that it was created for us:\ncd ~/ pwd Now, at this point, I should point out one very important impact of how our system is configured. I am logged in as an LDAP user, but notice that if I try to log in as root, I’m able to use sudo without any additional configuration! Yikes!\nHow did that happen? It actually is due to a setting in our sudoers file that is present by default on all Ubuntu systems. Let’s take a look at that file:\nsudo visudo If we look through that file, we’ll find a line that says members of the admin group may gain root privileges. In fact, that’s exactly what happened! Just because we chose to name our group admin on the LDAP server, our users would automatically gain sudo access on any system they log into. The same would happen for a group named sudo as well. So, it is very important to make sure your groups on your LDAP server use unique names, otherwise you may inadvertently create a major security hole in your systems.\nTo test the graphical login, you’ll need to fully reboot your computer. Once that is done, you can click the “Not listed?” option at the bottom of the list of users, then enter the credentials for your LDAP user to log in. If everything is configured correctly, it should let you log in directly to the system.\nYou have now successfully configured your system to allow Ubuntu to use LDAP servers for user accounts. This should be enough information to help you complete that section of Lab 4. As always, if you have any questions, feel free to post in the course discussion forums on Canvas. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Client Configuration",
    "uri": "/4-directory-services/08-ubuntu-client-configuration/index.html"
  },
  {
    "content": " Warning See the warning in the video script below for a correction that is not present in the video –Russ\nYouTube Video Resources File and Folder Permissions from Microsoft How to Understand Those Confusing Windows 7 File/Share Permissions from How-To Geek How to Take Ownership and Change Permissions of Files and Folders from Digital Citizen Advanced Security Audit Policy Settings from Microsoft Video Script Now, let’s take a look at file permissions in Windows 10. File permissions are very important in any computer system, as they control which folders and files each user is able to access. In this video, I’ll go through some of the basics of configuring file permissions in Windows.\nAs demonstration, I’ve created a folder at the root of the C:\\ drive, called test. Then, inside of that folder, I’ve created another folder called subfolder. This gives me a simple file hierarchy.\nFirst, let’s look at the permissions for the test folder. To find those, right-click on the folder and select Properties. Then, look at the Security tab. Here, you’ll see a summary of the current file permissions. For this file, there are four groups listed:\nAuthenticated Users - all but full control SYSTEM - full control Administrators - full control Users - read \u0026 execute, list contents, and read files We’ll examine each of the those permissions in detail as we make changes.\nTo make changes to permissions, there are two options. The first is to click the Edit button, which brings up a simple dialog for making changes, but does not include all options available. Therefore, I don’t recommend using this interface unless you want to make very quick changes.\nInstead, I recommend clicking the Advanced button below, to get a better idea of the options available.\nAt the top of this window, you can see the folder name, as well as the current owner of the folder. If you need to change the owner to a different user or group, you can click the Change link to do so.\nOn this window, there are three tabs. Let’s look at the Permissions tab first. Here, you can see all the entries in the access control list, or ACL, for this folder.\nIn addition to the user or group and permissions, we can also see how the permissions are inherited. Windows permissions can be inherited from parent folders, and then those permissions can be applied to child folders and files as well.\nIn many cases, you won’t need to change the inheritance of a folder, as you’ll generally want to inherit the permissions that the system sets. However, you can disable inheritance by clicking the Disable Inheritance button at the bottom. When you do, it will ask you if you want to convert inherited permissions to explicit permissions, or remove them entirely. In most cases, I recommend always converting them to explicit permissions. You can always delete the unneeded ones later, but removing some of the inherited entries may make the folder inaccessible. Let’s disable inheritance on this folder.\nBefore we make changes, let’s look at the current permissions. I’m going to click on the entry for Administrators, then click Edit. Notice at the top of the window there is a Type option for either allow or deny. I always recommend using allow permissions, as that will give your permissions a consistency to how they are structured. If you feel that you must deny a permission, that is usually a good indication that you should rethink your users and groups to avoid such a scenario.\nBelow that, you can set where this permission applies. There are many options there, and most of them are pretty self-explanatory.\nFinally, there are the basic permissions in Windows. Again, they are pretty self-explanatory. If you are unsure what a particular option does, I highly recommend consulting the Windows documentation. Also, note that you can click the Advanced Permissions link to the right to see more advanced permissions. In most cases, you won’t use them, but they are available if needed.\nGoing back to the main dialog, there are two sets of permissions that I recommend not changing on ANY folder. First, each folder should have an entry for Administrators, giving that group full control of the folder. If, for any reason, you feel that your Administrators group should not have full control of a folder, you should rethink your permission structure. System administrators will need to have control of a folder in order to change the permissions, and most users should never be Administrators.\nLikewise, do not modify the permissions of the SYSTEM group on any folders. That permission is vital for Windows services and processes to be able to perform operations on the folder.\nOne entry you may want to change is the entry for Authenticated Users. Currently, this entry allows any user with access to this system to access the folder or make changes. In essence, you can think of the Authenticated Users group as Everyone. In many cases, you’ll want to remove that entry entirely, unless you want a folder to be publicly accessible on your system.\nThe last entry is for the Users group. This gives the permissions for any user not in the Administrators group. Again, you may or may not want to remove this entry, depending on your needs.\nWarning Correction: By default, the Users group on Windows 10 contains the Authenticated Users group, so it actually includes all users on the system, not just those outside of the Administrators group. I don’t recommend removing Authenticated Users from Users as it may have unintended consequences. Instead, you may want to make your own group for this purpose, and explicitly add all users who are not in Administrators to that group.\nFor this example, I’m going to remove the entry for Authenticated Users, but modify the entry for the Users group to allow users in that group to modify the contents of this folder.\nBefore clicking Apply, notice that there is a checkbox at the bottom to replace all child permissions with inheritable permissions from this object. If you would like to reset the permissions on folders within this one, you can use that option to do so. I’ll do it, just to show you how it works. Of course, if there are several files or folders within this one, that operation could take quite a long time.\nThere are two other tabs to look at. The first one is the Auditing tab. Here, you can direct Windows to log events related to this folder, such as successful reads, writes, and more. Depending on your organization’s needs, you may need to implement a strict auditing policy through this interface. We won’t worry about it in this class, but I recommend reviewing the relevant documentation if you are interested.\nThe last tab, Effective Access, allows you to determine what the effective permissions would be for a particular group or user. This is a very useful tool, since users may be part of multiple groups, groups can be nested, and permissions can quickly become very convoluted. When in doubt, use this tool to make sure you have set the permissions correctly.\nOnce I am done here, I can click Apply, then click OK to apply my changes and exit the dialog.\nLet’s briefly look at the child folder’s permissions, just to see what impact those changes had.\nYou can see that it now inherits permissions directly from its parent folder, as expected. From here, I can add additional permissions that are applicable to just this folder and all of its subfolders.\nYou should now be able to start working on Lab 1, Task 3 - Windows Files \u0026 Permissions. A large portion of that task involves creating a file structure with permissions as defined in the assignment. Make sure you read the instructions carefully, and post questions in the course discussion forum if you are unsure how to interpret a particular direction.\n",
    "description": "",
    "tags": null,
    "title": "Windows 10 File Permissions",
    "uri": "/1-secure-workstations/08-windows-file-permissions/index.html"
  },
  {
    "content": "Putting it all together.\n",
    "description": "",
    "tags": null,
    "title": "Final Project",
    "uri": "/8-final-project/index.html"
  },
  {
    "content": " YouTube Video Resources Kubernetes Tutorial for Beginners by TechWorld with Nana on YouTube Video Notes Note Instead of creating and recording my own video overview of Kubernetes, I decided it would be best to just include an existing video that does an excellent job of covering that topic. My video would have been very similar, but probably with fewer graphics and a bit more condensed.\nI encourage you to watch the video above through at least the “Kubernetes Architecture” portion (around 35 minutes) to get a good feel for how Kubernetes builds upon what we’ve done in Docker and Docker Compose. If you want to get some hands-on experience with Kubernetes, you can continue in the video to the tutorial using Minikube.\nIf you know of a better introduction to Kubernetes video that you think would fit here, feel free to let me know. I may consider including it in a future semester.\nYou aren’t required to use Kubernetes for any of the labs in this course, but if you are comfortable with it you are welcome to give it a try. That said, if you need help debugging it beyond my skill, I may ask you to switch back to Docker and Docker Compose instead of pushing ahead with Kubernetes. –Russ\n",
    "description": "",
    "tags": null,
    "title": "Kubernetes",
    "uri": "/5a-containers/09-kubernetes/index.html"
  },
  {
    "content": " Create Two Droplets (5) Configure Droplets (5) SSH Port 22123 Timezone date Firewall (22123, 80, 443, backend filter) SSH Configuration (10) Grading Key on Frontend Frontend to Backend via config file SSH config - disable password \u0026 root Install Apache (5) Domain Names \u0026 DNS (5) 2 A records for frontend and backend Configure Apache Virtual Hosts (10) Apache virtual hosts config on both systems Public Key Certificates (10) Certificate works Redirect Works ",
    "description": "",
    "tags": null,
    "title": "Lab 5 Grading Checklist",
    "uri": "/z-instructor-resources/09-lab-5-grading-checklist/index.html"
  },
  {
    "content": " YouTube Video Resources Join in Active Directory Domain from Server-World How to Join Ubuntu 18.04 / Debian 10 To Active Directory (AD) Domain from Computingforgeeks Video Transcript In the following two videos, we’ll look at how to enable interoperability between our Windows- and Linux-based systems. In many organizations today, you’ll have a mix of many different operating systems, so it is very helpful to understand how to integrate them together in a single directory service.\nIn this video, I’ll show you how to add an Ubuntu VM to a Windows Active Directory Domain. This is by far the most common use-case for interoperability, and has been used by many organizations including K-State. For most users, Windows Active Directory provides a great set of features for Windows-based systems, and it is simple to integrate Linux and Mac clients into that environment.\nFor this example, I’m using the Windows 2019 server set up as defined in Lab 4’s assignment. I’ve reconfigured the domain to have the fully qualified domain name ad.cis527russfeld.cs.ksu.edu for this example, as you would in a real environment. This is due to the fact that many Linux packages do not accept the .local top-level domain I used in the previous video as an example.\nFor the client, I am using a copy of my Ubuntu 20.04 VM labelled CLIENT from Lab 3. For this assignment, you’ll use that same VM, but restored to a snapshot taken before it was configured as a client for OpenLDAP. Remember that you’ll need create this snapshot before you start the Lab 4 assignment, or else you may need to create a new VM for this activity.\nFinally, I’ve found there really isn’t a great guide for doing this online, but I’m generally following the instructions in the Server-World guide linked below this video. I’ll do my best to clarify what parts are important and what parts I leave out and why.\nFirst, as with any client I wish to add to a domain, I’ll need to set a static DNS entry to point to the domain controller itself. For the second entry, I’ll just use the VMware router as always. Once I’ve done that, I’ll disable and enable the network connection so that my changes take effect.\nThen, I’ll need to install the realmd package. This package allows us to interface with Active Directory Domain servers.\nsudo apt update sudo apt install realmd The instructions found online typically have you install a large number of packages at this step, but don’t do a good job of describing what they are used for. Thankfully, Ubuntu 20.04 includes a special program called packagekit by default, which allows us to automatically install the packages we need as we need them. So, we’ll start by just installing realmd and going from there.\nOnce we have realmd installed, we can use it to query our Active Directory domain using the realm discover command:\nrealm discover ad.cis527russfeld.cs.ksu.edu If that command works correctly, you should see output giving information about the Active Directory Domain we just queried. However, if you get an error at this step, that usually means that something isn’t working correctly. In that case, double-check that you have properly set your DNS entries, and that you can ping your Windows 2019 server using both the IP address and the Active Directory Domain name. You’ll need to be able to use DNS to resolve that domain name in order for this system to join the domain.\nOnce we are ready to join the domain, we can simply using the sudo realm join command.\nsudo realm join ad.cis527russfeld.cs.ksu.edu Once we enter that command, we’ll be prompted for the password for the Administrator account. This is the Administrator account of our Active Directory domain, so we’ll have to provide the correct password here.\nHere’s where the cool part happens. The realmd program is smart enough to know what packages we need in order to interface with an Active Directory domain, and because we have packagekit installed on Ubuntu 20.04 by default, the realmd command can automatically install those packages when we use the realm join command. So, we don’t have to worry about remembering to install all of these packages manually - it is done for us automatically! Very handy!\nOk, now that we have joined the domain, we can see if we can get information about an Active Directory user account using the id command:\nid russfeld@ad.cis527russfeld.cs.ksu.edu If everything is working correctly, you should see some information about that user printed to the screen. If not, you’ll need to do some troubleshooting.\nNow, we can even log in as that user using the su, or “switch user” command:\nsu russfeld@ad.cis527russfeld.cs.ksu.edu There we go! We are now logged in to this system as one of our Active Directory users. However, there are couple of things we can do to improve this experience. First, notice that we have to enter our fully qualified domain name for our Active Directory domain to log in. That can get really annoying, espeically with a long domain like ours. Also, what if we try to check our home directory:\ncd ~/ We find out that this user doesn’t even have a home directory to store files in. That’s no good!\nSo, let’s log out of this user and try to fix these issues.\nexit First, let’s change a setting so that we don’t have to enter the full Active Directory Domain name each time we want to log in as one of those users. To do that, we’ll edit the sssd configuration file. The sssd package helps provide access to authentication resources across many different sources.\nsudo nano /etc/sssd/sssd.conf In that file, we’ll see a line like the following:\nuse_fully_qualified_names = True We’ll simply set that setting to False and then save the file.\nuse_fully_qualified_names = False Finally, we can restart the sssd daemon to load the new settings.\nsudo systemctl restart sssd Now, we can use the id command with just the username:\nid russfeld and we should get the correct info. We can even use that username to log in to the system using the su command.\nsu russfeld exit Yay! Now, let’s tackle the creation of home directories. This is handled by the Pluggable Authentication Modules, or PAM, framework. There are many ways to do this, including editing a config file manually. However, there is a quick and easy way to do this as well, using the pam-auth-update command:\nsudo pam-auth-update In the menu, press the SPACE key to enable the “Create home directory on login” option, then press ENTER to save the changes.\nThere we go! Now, when we log in as our domain user, it will automatically create a home directory for us:\nsu russfeld cd ~/ pwd Awesome! Now, one interesting thing to discuss at this point is the fact that many online tutorials require us to install the oddjob and oddjob-mkhomedir packages. As far as I can tell, these are only required if we are using selinux instead of apparmor to protect our system. Since we aren’t doing that, we’ll just leave this out for now. However, if you do decide to use selinux on your system, you may have to do some additional configuration in order for your home directories to be properly created.\nOk, the last thing we may want to tackle on this system is the ability for our domain users to log on graphically. In previous versions of Ubuntu, this required a bit more configuration. However, in Ubuntu 20.04, all we have to do is restart the system to reload a few of the changes we just made. Then, we can click the “Not listed?” option at the bottom of the login window, and enter our domain username and password.\nVoila! That’s all it takes to log on graphically using an Active Directory domain user account on Ubuntu 20.04. The process has gotten much simpler over the years, to the point where it isn’t that much more difficult than adding a Windows 10 client to a domain.\n",
    "description": "",
    "tags": null,
    "title": "Linux Client on Windows Domain",
    "uri": "/4-directory-services/09-linux-client-on-windows-domain/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Wireshark Documentation How to User Wireshark to Capture, Filter, and Inspect Packets from How-To Geek Video Transcript Many times when you are working with networks as a system administrator, it is helpful to be able to see the actual traffic being sent across the network. Thankfully, there are many tools available to help you do just that. In this video, I’ll introduce one of those tools, named Wireshark.\nWireshark originally began as Ethereal, a network monitoring program developed in 1998 by Gerald Combs. In 2006, the name was changed to Wireshark to avoid copyright issues, and has been under constant development ever since. It is completely open source under the GNU General Public License, or GPL. Wireshark can be used to capture and inspect individual packets of network traffic, and it natively understands and decodes several hundred different protocols, giving you a very handy way to inspect not only the type of traffic on your network, but the contents of those packets.\nBefore we continue, there is one important warning I must share with you. Using Wireshark on a network allows you to potentially intercept and decode any unencrypted packets on the network, regardless of whether they are sent or received by your computer. This is a violation of K-State’s IT policies, and therefore you should never use Wireshark while directly connected to K-State’s network. As long as you are only using Wireshark within your VM network, and you’ve confirmed that your VM network is not set to “bridged” mode, you should be fine. So, make sure you are very careful when using this tool.\nFirst, we’ll need to install Wireshark. It is available for a variety of platforms. For this example, I’ll be installing it on Ubuntu Linux. To do that, we can simply use the apt tool:\nsudo apt update sudo apt install wireshark When you install Wireshark, you may be shown a message about installing Dumpcap in a way that allows members of the wireshark group to capture packets. Press ENTER to go to the next screen, then use the arrow keys (← and →) to select \u003cYes\u003e on the menu asking if non-superusers should be able to capture packets, then press ENTER to confirm that option.\nAfter you install Wireshark, you’ll need to add your current user account to the wireshark group. If you are using the cis527 account, you can do the following:\nsudo usermod -a -G wireshark cis527 Next, you’ll need to log out and log in for the new group membership to take effect. Otherwise, you won’t be able to directly capture packets unless you run Wireshark as root, which is not recommended.\nOnce you do so, you can search for “Wireshark” on the Activities menu to open the program. If you configured it correctly, it should show you all of your network interfaces on the first page. If you do not see them, check to make sure that your user account is properly added to the wireshark group and that you’ve logged-out and logged-in again.\nTo capture packets, we must first select which interface we’d like to listen to. Since I would like to capture packets on the actual network, I’m doing to select ens33 from this list. Your network interface may be named slightly differently, but it should be obvious which one is the correct one. As soon as you do so, you’ll start seeing all of the network packets sent and received on that network interface. By default, we are not listening in “promiscuous” mode, which would allow us to see all the packets on the network, regardless of the sender or recipient.\nAs you can see, even if you aren’t doing anything on the network yourself, there is always a bit of background traffic. Many of these packets are from your system and others on the network performing simple network requests from several of the background services or daemons. Most of them can be safely ignored for now, but if you are concerned about malicious network traffic on your network, any of these packets could be suspect.\nNow, let’s see if we can capture some interesting network traffic. First, I’m going to open a web browser, and go to a simple web page. I’m visiting my old personal page on the K-State CS systems, since it doesn’t automatically redirect me to a secure connection. That way we can see the contents of the packets themselves.\nNow that we’ve done so, let’s use the filtering features in Wireshark to see those packets. First, I’m going to press the “stop” button at the top to stop capturing packets. Next, I’m going to enter dns in the filter and press ENTER to only show the DNS packets in the output. There are still quite a few of them, but after scrolling through them I should see the ones I’m looking for.\nHere I’ve selected the first packet I sent, which is a standard DNS query for people.cs.ksu.edu. Below, I can see all of the layers of the packet. The top layer shows the frame from the Physical and Data Link layers. Below that, we see the Ethernet protocol information from the Data Link layer. By expanding that, we can see the source and destination MAC addresses of the this packet. Going further, we can see the Internet Protocol Version 4 header from the Network layer, which gives the source and destination IP addresses for this packet. Note that the original destination was the default gateway, which is also the DNS server I’ve configured this system to use.\nWe can also see that it used the User Datagram Protocol in the Transport layer. Here, we can see the source and destination ports. Notice that the source port is a very high number, meaning that it is most likely an ephemeral port on this system, whereas the destination port is 53, the “well-known” port for the DNS application layer protocol.\nFinally, we can see the contents of the DNS packet itself. If we look inside, we can see the query for people.cs.ksu.edu inside the packet. This view helps you clearly visualize the layers of encapsulation that each packet goes through as it makes its way across the network.\nA couple of packets later, we can see the response to the earlier query. Going back through each layer, you can see the source and destination MAC address, IP address, and port numbers are all reversed, just as you’d expect. Finally, looking at the contents of the DNS packet, we can see the response includes an answer for the query. Here, it shows that people.cs.ksu.edu is a CNAME or “canonical name” record, which points to invicta.cs.ksu.edu, the actual server it is stored on. Thankfully, DNS will also give us the IP address of that server, which is the second record, an A or “address record,” in the response. We’ll discuss these DNS record types in a later video.\nDepending on your browser’s configuration, you may also see additional DNS queries for the each URL included on the page that was loaded. For example, here I see queries for projecteuler.net, russfeld.me, and beattieunioncemetery.org, which are all linked at the bottom of my page. Most browsers do this as a way to speed up subsequent requests, as they assume you are likely to click on at least one of those links while visiting that page. Since it has already done the DNS query, it is one step closer to loading that page for you. In fact, many browsers may already send requests to that server in the background and have the page cached and ready to go before you click the link.\nThis is a very brief introduction to the power of Wireshark and how to use it to capture packets. Over the next few videos, we’ll explore some Application layer protocols and use Wireshark to help us explore the packets for each one. In the meantime, I encourage you to play around a bit with Wireshark and see what sorts of packets you can see on your own.\n",
    "description": "",
    "tags": null,
    "title": "Network Monitoring with Wireshark",
    "uri": "/3-core-networking-services/09-network-monitoring-with-wireshark/index.html"
  },
  {
    "content": " YouTube Video Resources Conditional Statements and Expressions from Puppet Expressions and Operators from Puppet Classes from Puppet Puppet Labs Standard Library from Puppet Puppet Forge from Puppet Video Script Puppet also has very powerful programming constructs which can be used in your manifest files. One of the most useful is the conditional statement, or “if” statement. The syntax is very similar to most other languages. For example, here is the sample “if” statement from the Puppet documentation:\nif $facts['is_virtual'] { warning('Tried to include class ntp on virtual machine; this node might be misclassified.') } elsif $facts['os']['family'] == 'Darwin' { warning('This NTP module does not yet work on our Mac laptops.') } else { include ntp } In this statement, the manifest is determining if the system is virtualized, or if the OS family is Darwin, the base kernel family for Apple Macintosh laptops. If either of those is the case, it will print a warning message. However, if neither of those is true, then it will include the ntp module. We’ll talk about modules later in the video. An “if” statement such as this, combined with the information that can be gleaned from a system using Facter, allows you to create manifest files that could be applied on a variety of different systems.\nOne caveat to be aware of: make sure you are careful about your data types. In Puppet, as in many other languages, the boolean value false and the string value \"false\" are different. In fact, Puppet treats non-empty strings as the boolean value true by default. In addition, all of the facts from Facter are strings, so you must be careful how you use them.\nConsider this example:\n$boolean = \"false\" if $boolean { notify{\"This is true\":} } else { notify{\"This is false\":} } When you run a manifest file containing this code, you’ll be notified that the value is true, since is a string. To resolve this problem, you can use the str2bool function included in the Puppet Labs Standard Library. First, install the Puppet Labs Standard Library module using the following command:\npuppet module install puppetlabs-stdlib Then, modify the manifest file as follows:\ninclude stdlib $boolean = \"false\" if str2bool(\"$boolean\") { notify{\"This is true\":} } else { notify{\"This is false\":} } Now, you’ll get the expected result. The Puppet Labs Standard Library includes many other useful functions, so I encourage you to check them out. The documentation is linked in the resources below this video.\nIn addition to the “if” statement, Puppet also includes a “case” statement. The basic syntax is as follows:\ncase $operatingsystem { \"centos\": { $apache = \"httpd\" } \"redhat\": { $apache = \"httpd\" } \"debian\": { $apache = \"apache2\" } \"ubuntu\": { $apache = \"apache2\" } default: { fail(\"Unrecognized OS\") } } The matches here are case-insensitive, which is very helpful. You can also combine some of the labels and use regular expressions, as in this example:\ncase $operatingsystem { \"centos\", \"redhat\": { $apache = \"httpd\" } /^(Debian|Ubuntu)$/: { $apache = \"apache2\" } default: { fail(\"Unrecognized OS\") } } When using regular expressions, note that the matching is case-sensitive.\nFinally, Puppet code can be further organized into classes. As with most object-oriented languages, a class definition is very simple. Here is one example:\nclass myclass (String $message = \"Hello\") { notify { \"${message} user\": } } Once the class is defined, you can use the include keyword to declare it in your manifest file, using all default values for parameters:\ninclude myclass Or you may declare it using a resource syntax, allowing you to override default parameter values:\nclass { 'myclass': message =\u003e \"Test\" } Either way, it is important to remember that you must always declare a class to use it. A class definition itself is not sufficient for the resources inside the class to be configured.\nFinally, on most enterprise Puppet systems, the manifest files have been further organized into a set of modules. Each module is a self-contained set of manifest files, templates, and configuration files, all for a particular use. For example, above the sample code references the ntp module, which is a module available from Puppet for managing NTP servers.\nUnfortunately, writing your own modules is a very complex task, and I have decided that it is outside of the scope of what I’d like to cover in this class. Feel free to continue following the documentation in the Learning Puppet VM to see more about how to write your own modules.\nIn many cases, however, there are already modules freely available to perform a variety of common management tasks. So, I highly recommend checking out the available modules on Puppet Forge to see what’s out there. As with many system administration tasks, don’t try to reinvent the wheel if one already exists!\n",
    "description": "",
    "tags": null,
    "title": "Puppet Programming Constructs \u0026 Classes",
    "uri": "/2-configuration-management/09-puppet-programming-constructs-classes/index.html"
  },
  {
    "content": " YouTube Video Resources Slides How to manage services in Windows from DigitalCitizen.life Process Explorer Download from Windows Sysinternals Process Library from Uniblue Video Script Let’s take a look at how Windows interacts with programs. Each program running on an operating system is called a process. Within each process, there can be multiple threads of execution running concurrently. As a system administrator, the most important thing to keep in mind is that each process running on a computer will consume the system’s resources, either the available memory or CPU time. So, the more processes you run, the slower your computer may seem as you consume more of the available resources.\nThe operating system stores a few important pieces of information about each process. The most notable is the PID or process identifier. Much like the user accounts and groups, each running process is given an identifier. In this way, if you have multiple copies of the same program running, each one will have a different PID.\nTo examine processes on Windows, there are a couple of tools available. First, built-in to Windows is the Task Manager. It has been present in Windows since the early days, and gives lots of information about the processes running on a system. You can access it quickly by right-clicking on an open area of the taskbar, or by using the classic CTRL+ALT+DEL key combination.\nAnother tool I recommend is Process Explorer, part of the Microsoft Sysinternals suite of tools for Windows. You can download it using the link in the resources below this video. Using Sysinternals, you can see additional details about each process running on a computer. You can also replace the built-in Task Manager with Process Explorer if you so choose.\nOn Windows, you’ll notice that even though we aren’t running any programs, there are still dozens of processes running in the background. Most of them are what we call Services. A service is a process that runs in the background on Windows, and is usually started and managed by the operating system. They perform many important tasks, such as maintaining our network connection, providing printing functions, and logging important system events.\nTo access the services on Windows, simply search for the Services app on the Start Menu. Here you can see all the services installed on your system, as well as their description, status, and more. I’m going to choose one to review in detail.\nOn the first screen, you’ll see some of the general information about the service. It includes the startup type, which could be either automatic, delayed, manual, or disabled. You can also start or stop the service here.\nOn the Log On tab, you’ll see the user account used to run the service. Just like with any other process on Windows, each service must be associated with a user account in order to determine what permissions that process will have. On Windows, there are actually three pseudo accounts that are typically used with services. Those accounts are LocalSystem, LocalService, and NetworkService. Of course, you can always override these defaults and provide the information for another user account, but then this service will have the same permissions as that account.\nThe Recovery tab describes what actions the system should take if the service fails for any reason. Again, you probably won’t need to modify these options unless you are working with services of your own, but it is important to know that they can be configured here. For example, if an important process fails, you could have the computer automatically restart itself or run a program to notify you.\nFinally, the Dependencies tab lets you see any other services that this one depends on, or services that depend on this one. Many Windows services require other services to be active before they will work properly.\nOne important thing to note about Windows services in particular is the use of the Service Host Process, or svchost.exe. To help conserve resources, Windows can actually embed several services as threads in a single process. In that way, the operating system only has to manage a single process instead of several. If you look at the processes running in Task Manager, you’ll see several entries for Service Host. You can even click the arrow next to that process to see which services are embedded in it. Unfortunately, because this creates a single point of failure, the Service Host Process is a frequent target of Windows malware and viruses. In fact, I once saw a virus try to hide its own executable by naming it svcnost.exe, hoping that a system administrator wouldn’t notice the slight spelling difference very quickly.\nWith this background information, you are ready for the next step, which is to install some software.\n",
    "description": "",
    "tags": null,
    "title": "Windows 10 Processes \u0026 Services",
    "uri": "/1-secure-workstations/09-windows-processes-services/index.html"
  },
  {
    "content": " YouTube Video Resources How to Back Up Windows 10 from Tech Advisor How to Use All of Windows 10’s Backup and Recovery Tools from HowTo Geek How to Make a Full Backup of your Windows 10 PC from Windows Central These resources mostly refer to Windows Server 2012 or 2016, but should work for 2019 as well.\nAD Forest Recovery - Backing up a full server from Microsoft Windows IT Pro Center How to Backup Active Directory Fully in Windows Server 2016 from Tactig How to perform Authoritative Restore of Active Directory Objects - 2012 R2 from ITIngredients (should work for 2016) Windows Server 2012 - Active Directory - Backup and Restore, Part 1: System State from David M Tech Blog (should work for 2016) Video Transcript Windows includes several tools for performing backups directly within the operating system. In this video, I’ll briefly introduce a few of those tools.\nFirst is the Windows File History tool. You can find it in the Settings app under Update \u0026 Security. Once there, choose the Backup option from the menu on the left. To use File History, you’ll need to have a second hard drive available. It can be either another internal drive, another partition on the same drive, or an external hard drive or flash drive. Once it is configured, File History will make a backup of any files that have changed on your system as often as you specify, and it will keep them for as long as you’d like, provided there is enough storage space available. By default, it will only back up files stored in your user’s home directory, but you can easily add or exclude additional folders as well.\nRight below the File History tool is the Backup and Restore tool from Windows 7. This tool allows you to create a full system image backup and store it on an external drive. This is a great way to create a single full backup of your system if you’d like to store one in a safe location. However, since Windows 10 now includes options to refresh your PC if Windows has problems, a full system image is less important to have now than it used to be.\nHowever, one tool you may want to be familiar with is the System Restore tool. Many versions of Windows have included this tool, and it is a tried and true way to fix some issues that are caused by installing software or updates on Windows. You can easily search for the System Restore tool on the Start Menu to find it. In essence, System Restore automatically creates restore points on your system when you install any new software or updates, and gives you the ability to undo those changes if something goes wrong. System Restore isn’t a full backup itself, but instead is a snapshot of the Windows Registry and a few other settings at that point in time. If it isn’t enabled on your system, I highly recommend enabling it, just for that extra peace of mind in case something goes wrong. As a side note, if you’ve ever noticed a hidden folder named “System Volume Information” in Windows, that folder is where the System Restore backups are stored. So, I highly recommend not touching that folder unless you really know what you are doing.\nIn the rare instance that your operating system becomes completely inoperable, you can use Windows to create a system recovery drive. You can then use that drive to boot your system, perform troubleshooting steps, and even reinstall Windows if needed. However, in most instances, I just recommend keeping a copy of the standard Windows installation media, as it can perform most of those tasks as well.\nWindows also has many tools for backing up files to the cloud. For example, Windows has Microsoft’s OneDrive software built-in, which will allow you to automatically store files on your system in the cloud as well. While it isn’t a true backup option, it at least gives you a second copy of some of your files. There are many other 3rd-party tools that perform this same function that you can use as well.\nFinally, Windows Server includes the Windows Server Backup tool, which is specifically designed for the types of data that might be stored on a server. You can use this tool to create a backup of your entire server, including the Active Directory Domain Services data. Losing that data could be catastrophic to many organizations, so it is always recommended to have proper backups configured on your domain controllers. As part of the lab assignment, you’ll use this tool to backup your Active Directory Domain, and then use that backup to restore an accidentally deleted entry, just to make sure that it is working properly.\nOf course, there are many other backup tools available for Windows, both free and paid, that offer a variety of different features. If you are creating a backup strategy for your organization, you may also want to review those tools to see how well they meet your needs.\n",
    "description": "",
    "tags": null,
    "title": "Windows Backups",
    "uri": "/7-backups-monitoring-devops/09-windows-backups/index.html"
  },
  {
    "content": " Note The first part of this video references ASP.NET 4.6, which has been replaced by ASP.NET 4.7 in Windows Server 2019.\nYouTube Video Resources URL Rewrite from Microsoft Add A Website to Windows Server 2016 using Host Headers from Ionos by 1\u00261 How to add DNS Forward Lookup Zone in Windows Server 2019 from Computingforgeeks How to add DNS A/PTR Record in Windows Server 2019 from Computingforgeeks How to Create a Self Signed Certificate in IIS from AboutSSL Microsoft Server 2016 - IIS 10 \u0026 10.5 - SSL Installation from SSLSupportDesk How to Install a SSL Certificate on IIS 10 from SSLs.com Setting up an HTTP/HTTPS Redirect in IIS from Namecheap Video Transcript In this video, I’ll discuss some of the steps for installing and configuring the Internet Information Services, or IIS, web server on Windows Server 2016. This will set the stage for the next video, which will discuss the process for installing a .NET-based web application.\nAs before, I’ll continue to use my Windows Server 2016 VM from previous labs. To install IIS, click on the Manage button in the Server Manager application, and select Add Roles and Features. From there, you’ll follow the same steps you did when you installed the Active Directory Domain Services role, but this time choose the “Web Server (IIS)” role instead. Then, click Next a couple of times until you reach the Select Role Services page. There, you’ll need to checkmark the option for “HTTP Redirection,” which can be found under the “Web Server \u003e Common HTTP Features” list items. Also, enable the option for “ASP.NET 4.6” found under the “Web Server \u003e Application Development” list items. When you do so, it may enable a few additional options. Finally, click Next once again, then click Install to install the new server role.\nOnce it has been installed, you should now see the new IIS option on the left side of the Server Manager application. Click that option to see information about your IIS server. To access the configuration options for that server, let’s open the Internet Information Services (IIS) Manager, which can be found on the Tools menu in Server Manager, or by right-clicking the entry here.\nIIS Manager provides a convenient way to manage and configure your IIS server. Starting on the home page, you’ll see icons for a variety of features that you may want to configure for your server. For this video, I’m going to go through the process of adding a new website to this server, as well as the steps to properly configure and secure it.\nFirst, let’s create the directory for our new site. I’m going to create a new folder at C:\\inetpub\\example to store the website. The C:\\inetpub folder on Windows is the default location for IIS to place files, so it is a good logical place for this to be stored. Then, inside of that folder, I can place a simple file named index.html to act as the homepage for this website. I’ll add a bit of text to the file as well, just so it is clear that we are accessing the correct file when we navigate to it later.\nIn the list on the left side of IIS Manager, expand the entry for your server, and then right-click the Sites folder and select Add Website. In the window that appears, give your website a name and a path. I’m going to point it at the folder I created at C:\\inetpub\\example for this website. Lastly, I’m going to configure the binding for the site by entering the host name I’d like to use for this website. I’m going to use example.local for this website. Finally, I’ll click OK to create the site.\nOnce the site is created, I can open up a web browser and navigate to http://localhost to see what the web server shows. Unfortunately, right now it just shows the default IIS page that we’ve seen before. This is because we still have the default site enabled on our system. If we try to navigate to http://example.local to access our site, that doesn’t work either. This is because our web browser will try to use DNS to look up that website, but currently .local is not a valid TLD.\nSo, to test this website, we’ll need to add a few entries to our DNS server.\nSince our Windows server is a domain controller, it also includes a built-in DNS server. So, we can add a few A records to our DNS server to point to our websites. To access the DNS information in Windows Server, we’ll go to the Server Manager, and then look for the DNS entry on the left side. This will show information about the DNS servers in our domain. Right now there is just one, our domain controller. To modify the DNS information, we can right-click on that server and choose the DNS Manager option.\nIn this window, we can expand the entry for our server on the left, and then we should see some information that looks familiar to us based on what we learned in Lab 3. The Windows DNS server uses the same concept of forward and reverse zones, just like we saw in Bind. So, let’s go to the Forward Lookup Zones option.\nHere, we can see a couple of zones for our Active Directory, which are automatically maintained by the Active Directory Server. So, we will just leave these alone. Instead, let’s create a new forward zone. In the wizard, we want to create a new primary zone, but we don’t want to store that information in the Active Directory. If we don’t uncheck this option, we are limited to only creating zones within our AD domain, which we don’t want. In the next page, we’ll use the zone name local to allow us to create DNS entries with the .local suffix. Of course, for the lab assignment, you may have to modify this to fit your environment. Finally, we’ll choose to create a new zone file, and we’ll disable dynamic updates to this DNS zone. Then, we can click Finish to create the zone.\nOnce the zone is created, we can open it and choose to add a New Host (A or AAAA) record. This is pretty simple - we’ll just give it a host name and an IP address, just like we would expect. So, I’ll use example as the host name, and 192.168.40.42 as the IP address to match my example.\nThere we go! No, we can open Firefox and navigate to http://example.local and it should open our webpage! This works because our Windows server was set to use itself as its primary DNS server way back in Lab 4, so it will look up these DNS names using the DNS server we just configured.\nNext, let’s configure this website to use a secure connection and a public key certificate. Back in IIS Manager, select the server in the list of items on the left side, then open the Server Certificates option to the right. Here, I can click the Create Self-Signed Certificate option on the right-hand side to create a new certificate for our server. In the window that appears, I can give the certificate a name. In this case, I’ll just use the name of my website, example.local. That’s all it takes! We now have certificate we can use.\nNext, we’ll need to assign it to our site. To do this, right-click on your site in the left-hand list in IIS Manager, and select Edit Bindings. There, we’ll add a new binding for type “https” and the same host name as before. Finally, at the bottom, we can select the certificate we just created as the security certificate for this website. Finally, click OK to save those changes.\nWe can test those settings by opening a web browser and navigating to https://example.local. However, at this point, most web browsers will complain about the connection being insecure. This is a bit misleading, as the connection will be properly encrypted just fine. However, since your browser cannot establish a chain of trust for the certificate, it is warning you that the website could be compromised and a malicious third-party could be on the other end of your connection. In this case, we know that the connection is safe, so I’ll just click the option to add a security exception for this site. That should allow you to see the website once again, this time via HTTPS.\nFinally, let’s configure this website to automatically redirect users from HTTP to HTTPS, just like we’ve done previously with Apache using Certbot. There are many ways to do this, but one of the simplest is to install the “URL Rewrite” module for IIS. I’ll be closely following the guide from Namecheap that is linked in the resources section below the video, so feel free to refer to it as well for screenshots of this process.\nFirst, I’ll need to download and install the module. You can also find it linked in the resources section below this video. Once it is installed, you’ll need to close and reopen IIS Manager. Now, if you select your site in the list on the left, you should see a new URL Rewrite option in the list of icons. To add a rule, double-click that icon to open its settings, then click the Add Rules option to the right.\nFrom here, you should be able to follow the guide from Namecheap to set up the redirect rule. I won’t demonstrate that full process here, as you’ll need to do that as part of your lab assignment.\nNow, let’s test this setup. First, clear the cache of the web browser you are using for testing, just to be sure that we aren’t still reading any cached information from that site. Then, navigate to http://example.local, and hopefully you should be redirected to https://example.local automatically.\nAs you can see, working with IIS is very similar to working with Apache. Some things are a bit easier to configure in IIS, while others are simpler in Apache in my opinion. Thankfully, many of the concepts are the same across all web browsers, so it is easy to adapt your knowledge to fit the current software you are using.\nIn the next video, I’ll discuss the steps for installing a .NET web application on this server.\n",
    "description": "",
    "tags": null,
    "title": "Windows Web Server",
    "uri": "/6-application-servers/09-windows-web-server/index.html"
  },
  {
    "content": "Cleaning up the mess.\n",
    "description": "",
    "tags": null,
    "title": "Wrap Up",
    "uri": "/9-wrap-up/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Dynamic Host Configuration Protocol (DHCP) on Wikipedia Bootstrap Protocol (BOOTP) on Wikipedia What is DHCP? (Dynamic Host Configuration Protocol) from Lifewire ISC DHCP Open Source Software for DHCP Servers Dynamic Host Configuration Protocol (RFC 2131) Standard from IETF Link-Local Address on Wikipedia Video Transcript The Dynamic Host Configuration Protocol, or DHCP, is a core part of operating any network today. This video will introduce DHCP and demonstrate how it works. In your lab assignment, you’ll be setting up and configuring your own DHCP server, so this information will be very helpful in completing that task.\nAs a quick note, much of the information in this lecture is adapted from information provided by Seth Galitzer, our system administrator here in K-State CS. He created the original slides as part of a guest lecture in this course when it was offered on campus, and was gracious enough to share this information for future versions of the course.\nFirst, let’s review a bit of internet history. Prior to the 1980s, there were many networks that existed across the world, but they were not interconnected. In 1982, the TCP and IP protocols were developed, with the aim of unifying all of those networks into a grand interconnected network, or “internet.” At the time, there were only a few hundred computers worldwide which would be part of this network, so manual configuration wasn’t too bad. As there were more and more computers on the internet, they realized that it would be helpful to have a way to automatically configure new systems. So, in 1985, the Bootstrap Protocol, or BOOTP, was developed in order to provide some automation. However, BOOTP was very limited, and only could perform some functions. As the internet was growing by leaps and bounds at this point, they decided a new solution was needed. So, in 1989, they formed the DHC working group to build a better way. In 1993, their initial specification for DHCP was released, and in 1996 the first working server was available. A year later, in 1997, they finalized the protocol into the standard it is today.\nThere were some major reasons why DHCP was needed. As discussed, the internet, as well as many corporate networks, were getting larger and larger, and manual configuration of those networks was simply too difficult to manage. In addition, with the introduction of laptops and mobile devices, computers needed to be able to seamlessly move from network to network without needing additional configuration. So, they really needed a method to automatically configure network settings on a computer when it initially connected to a network. Hence, the creation of DHCP.\nBeyond that, there are many features built in to DHCP to make it a very powerful system. First, it includes the concept of “leasing” an IP address. When a system first connects to the network, it is given an IP address and a length of time it may keep that IP address. Once that time is up, it must renew its lease on that IP, or the system may assign that IP to a new system. In this way, as systems come and go, the DHCP server can reuse IP addresses once the leases have expired. In addition to IP addresses, modern DHCP servers can also send additional information about the network, such as DNS servers, SMTP servers, and more. Using DHCP, modern computers can be much more portable, allowing them to seamlessly connect to any network providing a DHCP server. Finally, even though DHCP is very powerful, there are many instances where a static IP address is preferred, such as for servers and printers. Thankfully, DHCP is fully compatible with static IP addresses on the same network. All that is required is a bit of configuration to mark which sections of the network should be automatically configured and which shouldn’t. In addition, many DHCP servers can be configured to always give the same IP address to a particular host, providing even more flexibility.\nLet’s dig a bit deeper in to the DHCP protocol itself to see how it works. In essence, DHCP is a 4-step handshake that happens when any new computer connects to a network. In the first step, the computer sends out a special “discover” message to every system on the network, asking for help to connect. That message is sent to the destination IP address 255.255.255.255, which is a special broadcast address telling the network to send that packet to every computer on the network. This allows the computer to communicate, even if it doesn’t have the proper network settings yet. In the second step, a DHCP server will receive that “discover” message, and send back an “offer” message containing an IP address and additional settings the computer could use. Once the computer receives that message, it will then broadcast a “request” message, which requests a specific IP address. It could be one it was using previously if it is renewing a lease, otherwise it will be one from the “offer” message it received. When the DHCP server receives that “request” message, if that IP address is still available, it will respond with an “acknowledge” message confirming the address, or it will respond with an error to the computer, which starts the process over again. Once the computer receives an “acknowledge” message, it can then configure its network settings using the information it has received, and it is good to go.\nSo, in short, it goes something like this:\nClient: “Hi, I’m new!” Server: “Welcome! Here’s some settings you could use.” Client: “Cool! Can I use these settings?” Server\" “Sure can! You are all set.” DHCP also works with multiple servers. Since each server will receive the “discover” message, it will respond with its own “offer” message. The computer can then choose which “offer” to respond to, and the servers will only respond with an “acknowledge” message if it was the original sender of the offer. In that way, you can have multiple DHCP servers available on the network, providing additional redundancy and capacity.\nIf a computer fails to get an IP address from DHCP, it can automatically configure an IP address using the Automatic Private IP Address (APIPA) configuration protocol. In essence, it will assign an IP address in the link-local subnet, that allows it to work on the network without conflicting with anything else. Both IPv4 and IPv6 have set aside a block addresses for this use.\nSeth was also gracious enough to provide some sample DHCP configuration files. These were used on CS systems when the department was located in Nichols Hall years ago. Here at the top, you can see the configuration settings for the DHCP lease time. By default, each lease is good for 10 minutes, but clients can request a lease as long as 2 hours (120 minutes, or 7200 seconds). Below that, we can see some default settings for the network, giving the domain name, DNS servers, subnet mask, broadcast addresses, and more for this network.\nHere we can see an example for a fixed-address configuration. Whenever a computer contacts this server using that MAC address, it will be assigned the IP address listed below. This is very handy when you have laptops that may come and go on the network. In this way, they are always configured to use DHCP so that the user can use them elsewhere without any problems, but when they are connected to this network they are effectively given a static IP address. Finally, below that we can see a configuration for a simple pool of IP addresses available for automatic configuration.\nOf course, this is not a complete configuration file, nor will any of these settings work for your lab assignment. So, you’ll need to read the appropriate documentation as well as discover your own network’s settings in order to configure your DHCP server.\nLet’s look at a quick example of how this would look in practice. Here I have configured an Ubuntu VM as directed in Lab 3 to act as a DHCP and DNS server. I also have a second Ubuntu VM acting as our client. Finally, I have disabled the DHCP server in VMware on this network segment.\nFirst, on my server, I’m going to start Wireshark so we can capture these packets. I’ll also add a filter for bootp to make sure we only see the DHCP server packets. Since BOOTP and DHCP are compatible protocols, this is the way that Wireshark sees the packets.\nNext, I’m going to boot up the client, which will cause it to request an IP address as it boots. Once it has booted and I’ve logged in, I’m also going to release my IP address:\nsudo dhclient -r -v This should show that it sent a DHCPRELEASE packet to my DHCP server. Then, after waiting a few seconds, we can request a new IP address:\nsudo dhclient -v Examining the output, you’ll see that we send a DHCPDISCOVER message, then we receive a DHCPOFFER from the server. We can then request an IP using a DHCPREQUEST message, and finally we’ll receive a DHCPACK message back from the server acknowledging that request. Here, you’ll notice that we send the DHCPREQUEST before we even receive the DHCPOFFER. Since we had an IP previously, we can just go ahead and request it again and see if it works. If so, we’ll be on the network a little bit faster, but if not, we can just respond to the DHCPOFFER we receive and continue the original handshake.\nGoing back to the server, we can clearly see those four packets, as well as the earlier DHCPRELEASE packet. By examining any of those packets, we can see the different bits of information sent from the server to the client and vice-versa.\nIn addition, by default the DHCP server will log information to the system log, so we can find information about these packets by searching through that log file:\ncat /var/log/syslog | grep dhcp When troubleshooting a DHCP server, it is very helpful to review any error messages present in the system log.\nWith that information, you should be ready to configure your own DHCP server. As always, if you have any questions or run into issues, please post in the course discussion forums on Canvas. This process can definitely be frustrating the first time you do it, since there is so much new information to read and understand. Don’t be afraid to ask for help if you get stuck!\n",
    "description": "",
    "tags": null,
    "title": "DHCP",
    "uri": "/3-core-networking-services/10-dhcp/index.html"
  },
  {
    "content": "Greetings! Here is the information you’ll need to know to get your lab graded.\nGrading Date/Time: \u003ctime\u003e Contact me ASAP if that time will not work for you for any reason and I will work with you to reschedule. Zoom Link: \u003clink\u003e If you haven’t used Zoom before, I recommend going to https://ksu.zoom.us/ first to download the Zoom client software. Then, when the time comes, click the link above to join the Zoom session. Your Setup: Before you join the Zoom session for grading, please have the following steps completed on your end. This helps things go as smoothly as possible: Use the fastest internet connection available to you. If you are using a laptop with a wireless connection, you may want to plug in to a hard-wired connection if available. If you can work from campus, that may be best, depending on your home internet connection. Make sure your audio input device (microphone) is working in Zoom. You are not required to have a webcam, but you are welcome to use one if you like. I will be using a webcam and microphone on my end. Confirm that all the VMs for this lab are booted and running on your system. We’ll be using the desktop sharing features of Zoom so I can see your VMs. So, make sure you close any windows or programs that are running in the background that shouldn’t be shared with me. Operations: Here are a few operations you may be asked to demonstrate: SSH to Frontend \u0026 Backend Access Frontend \u0026 Backend via HTTP/HTTPS DNS Configuration Finality Once you begin the grading process, no changes may be made to your system to fix any errors encountered. The grade you receive will reflect the state of your VMs at the start of the grading process. This is to ensure that grades are fairly determined and that no special consideration is given. That should cover everything. Please let me know if you have any questions prior to your scheduled grading time.\nGood luck!\nRuss\n",
    "description": "",
    "tags": null,
    "title": "Lab 5 Grading Email",
    "uri": "/z-instructor-resources/10-lab-5-grading-email/index.html"
  },
  {
    "content": " Puppet Learning VM Deprecated As of 2023, the Puppet Learning VM is no longer being maintained. The videos below demonstrate some of the features of Puppet, which can also be done on your Ubuntu VM after installing Puppet Agent. Unfortunately, it is not easily possible to simulate an enterprise Puppet setup without this VM, so I’ll keep these videos up for demonstration purposes. –Russ\nYouTube Video Resources Overview of Puppet’s Architecture from Puppet Video Script In this video, I’ll give a quick demonstration of what a true enterprise Puppet setup might look like. Once again, I’m going to be using the Learning Puppet VM, and this time I’ll be following the agent_run quest to demonstrate these features.\nThe agent_run quest gives us a client system already set up and ready to go. We can access it using SSH:\nssh learning@agent.puppet.vm Once we are on that system, we can try to force a Puppet Agent run using the following command:\nsudo puppet agent -t However, when the Puppet Agent tries to contact the Puppet Master server, it presents an error about client certificates. As a security measure, we must sign the certificate for each agent that tries to contact the server before it will be allowed access. Without this step, any malicious user could gain valuable information about our system configuration by talking with the Puppet Master.\nTo sign that certificate, we can go back to the Puppet Master by exiting the current SSH session:\nexit Then we can use this command to show all the unsigned certificates on the system:\nsudo puppet cert list To sign a certificate, we can use this command:\nsudo puppet cert sign agent.puppet.vm Now, we can reconnect to our agent node:\nssh learning@agent.puppet.vm And try to run the agent again:\nsudo puppet agent -t This time, we should be successful. However, we haven’t really told Puppet what we want configured on this system. So, let’s go back to the Puppet Master and do so:\nexit On the Puppet Master, we would like to edit the manifest file used to configure each system. As before, I’ll quickly install Nano to make editing files much simpler, but feel free to use Vim if you would like:\nsudo yum install nano Next, we’ll edit the default site manifest file, which is located at the end of a very long directory path:\nnano /etc/puppetlabs/code/environments/production/manifests/site.pp This is the default site manifest file for this system. On an actual system, you may define different environments and roles for each system, which may alter the path to this file. For this example, we’ll just make a quick edit to show how it can be done.\nAt the bottom of the file, add the following:\nnode 'agent.puppet.vm' { notify { \"Hello Puppet!\": } } Then, use CTRL+X, then Y, then ENTER to save and close the file.\nFinally, return to the agent VM:\nssh learning@agent.puppet.vm and run Puppet Agent once again:\nsudo puppet agent -t If done correctly, you should see a notification of \"Hello Puppet!\" in the output.\nThis is a very short demo of the power of Puppet’s Master and Agent architecture. The Puppet Learning VM quests go much more in-depth in ways to use Puppet in an organization. I highly encourage you to review that information if you are interested, but it is not required to complete Lab 2.\n",
    "description": "",
    "tags": null,
    "title": "Puppet Agent \u0026 Master Demo",
    "uri": "/2-configuration-management/10-puppet-agent-master-demo/index.html"
  },
  {
    "content": " YouTube Video Resources Backing Up Files from Ubuntu Documentation Backups from Ubuntu Server Guide How to Backup your Ubuntu Desktop with Déjà Dup from HowToForge Ubuntu 20.04 System Backup and Restore from Linuxconfig.org How to Compress and Extract Files using the tar Command on Linux from How-To Geek How to Import and Export Databases and Reset a Root Password in MySQL from DigitalOcean How To Backup MySQL Databases on an Ubuntu VPS from DigitalOcean (steps should still be valid for 20.04) Video Transcript There are many different ways to back up files and settings on Ubuntu. In this video, I’ll discuss a few of the most common approaches to creating backups on Ubuntu.\nFirst, Ubuntu 18.04 includes a built-in backup tool, called “Déjà Dup,” that will automatically create copies of specified folders on your system in the location of your choice. It can even store them directly on the cloud, using either Google or Nextcloud as the default storage location, as well as local or network folders. This feature is very similar to the File History feature in Windows 10.\nIn addition, many system administrators choose to write their own scripts for backing up files on Ubuntu. There are many different ways to go about this, but the resources from the Ubuntu Documentation site, linked below the video, give some great example scripts you can start with. You can even schedule those scripts using Cron, which is covered in the Extras module, to automatically perform backups of your system.\nWhen backing up files on your system, it is very important to consider which files should be included. On Ubuntu, most user-specific data and settings are stored in that user’s home folder, though many of them are included in hidden files, or “dotfiles.” So, you’ll need to make sure those files are included in the backup. In addition, most system-wide settings are stored in the /etc folder, so it is always a good idea to include that folder in any backup schemes. Finally, you may want to include data from other folders, such as /var/www/ for Apache websites.\nIf you are running specific software on your system, such as MySQL for databases or OpenLDAP for directory services, you’ll have to consult the documentation for each of those programs to determine the best way to back up that data. As part of this module’s lab assignment, you’ll be creating a backup of a MySQL database to get some experience with that process.\nThere are also some tools available for Ubuntu to help create backups similar to the System Restore feature of Windows. One such tool is TimeShift. I’ve linked to a description of the tool in the resources section below the video if you’d like to know more.\nFinally, as with Windows, there are a large number of tools, both paid and free, available to help with creating, managing, and restoring backups on Ubuntu and many other Linux distributions. As you work on building a backup strategy for an organization, you’ll definitely want to review some of those tools to see if they adequately meet your needs.\nThat concludes Module 7! As you continue to work on the lab assignment, feel free to post any questions you have in the course discussion forum on Canvas. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Backups",
    "uri": "/7-backups-monitoring-devops/10-ubuntu-backups/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Installation (Computer Programs) on Wikipedia Here’s What Happens when you Install the Top 10 Download.com Apps from How-To Geek Process Monitor from Microsoft Sysinternals InstallWatch Pro from Lo4d.com Video Script One major part of configuring a computer is installing the desired software. However, it is sometimes very difficult to tell exactly what is happening during a software installation, and each time you install software on a system you run the risk of introducing more vulnerabilities and stability issues on the system. In this video, I’ll give a brief overview of how to observe what is happening when you install a piece of software.\nSoftware on Windows typically consists of several components. First, you have the actual executable itself, as a .EXE file. It may also have several dynamic link libraries, or DLLs, that it uses. A DLL is simply a shared library of code that can be used by other programs or updated without changing the executable file itself. There could also be settings files such as initialization files, or settings stored as registry keys in the Windows registry. Finally, depending on the software it may also install drivers or services as well. Let’s look at one piece of software that is pretty common and see what it does when we install it on our system.\nOn this system, I have installed InstallWatch Pro, which is available in the Resources section below the video. I’ve also downloaded a copy of Process Monitor from the Microsoft Sysinternals suite of tools. Finally, I downloaded a full installer for Mozilla Firefox.\nFirst off, I need to start InstallWatch Pro and let it make a snapshot of the current system. This process may take several minutes.\nOnce that is done, I can also start Process Monitor. It will start recording data automatically.\nThen, I can instruct InstallWatch Pro to install Mozilla Firefox by providing the location of the installation file here.\nWhen the installation process starts, I’ll just click the default options in the installer and let it continue as it would normally.\nWhen it is finished, InstallWatch Pro will pop back up, and you’ll need to click Finish there so it can work on making a new snapshot after the installation. You should also quickly go to Process Monitor, then click File and uncheck the Capture Events option.\nOnce InstallWatch Pro is done making a new snapshot, it will display all of the items installed by Mozilla Firefox. We can see several files were added, mostly in the Program Files folder. There were also many keys added to the registry. I recommend doing this process on your own virtual machine at least once and reviewing what you find.\nIn Process Monitor, we can add a filter to just see all items performed by the Mozilla Firefox setup process, which uses “setup.exe” as its process name. Just click the filter button at the top, and add the appropriate filter. Now we can see each and every option performed by the setup process. In this case, there are nearly 100,000 of them! It could be very tedious to dig through, but if you know you are looking for a particular item, you can use the search features in Process Monitor to find it very quickly.\nAs you continue working on Lab 1, I encourage you to take a little time and see what each program installs. You might be surprised!\n",
    "description": "",
    "tags": null,
    "title": "Windows 10 Software Installation",
    "uri": "/1-secure-workstations/10-windows-software-installation/index.html"
  },
  {
    "content": " Note This portion is no longer part of the Lab 4 assignment, but I left it here because it is interesting to know that it can be done. –Russ\nYouTube Video Resources How to Configure Ubuntu Linux Server as a Domain Controller with Samba-tool by Jack Wallen on TechRepublic Samba Not Starting on Ubuntu Server 16.10 from StackExchange How to Disable Systemd-resolved in Ubuntu from AskUbuntu Video Transcript In this video, I’ll show you how to set up an Ubuntu VM to act as a Windows Active Directory Domain Controller using Samba. Unfortunately, at this time it is not straightforward at all to use OpenLDAP as the server, hence the decision to use Samba instead. This is a much more complicated process than adding Ubuntu as a client to an existing Active Directory Domain, but I feel that it is important to see that it is indeed possible. This is helpful if your organization is primarily Linux-based, but you wish to include a few Windows clients on the network as well.\nFor this example, I’m using the Ubuntu VM labelled Client from earlier in this lab. I’ve created a snapshot to store the work done previously to add it to an OpenLDAP domain, but now I’ve restored the snapshot labelled “Before Lab 4” and will use that snapshot to create a Samba server. The reason I am doing this on the VM labelled Client and not the one labelled Server is because the Samba installation will directly conflict with Bind9, OpenLDAP, and many other tools we’ve already installed on the server. So, this allows us to avoid any conflicts with those tools.\nFor the client, I have restored my Windows 10 VM to a snapshot taken before it was added to my Active Directory Domain. Remember that you’ll need to create this snapshot before you start the Lab 4 assignment, or else you may need to create a new VM for this activity.\nFinally, I’ll generally be following the great guide by Jack Wallen that is linked in the resources section below this video, but I’ll be making a few minor changes to match our environment and clarifying a few things that he leaves out.\nBefore we begin, we’ll need to set a static IP on this system, since it will be acting as a server. I’m going to use the IP ending in 45 in my local subnet. For the DNS entries, we’ll also need to set the first entry to be this system itself, just like we did with the Windows Domain Controller. The second DNS entry can be the VMware router or any other valid DNS server.\nFirst, we’ll need to install Samba as well as a few supporting tools:\nsudo apt update sudo apt install samba libpam-winbind smbclient This will install many supporting packages as well.\nNext, we need to make a few edits to our hosts file as well as update our hostname. This will help Samba find the correct settings as we configure it in a later step. First, let’s edit the hosts file:\nsudo nano /etc/hosts I’ll need to add or edit a few entries at the top of this file. Here’s what mine currently looks like:\n127.0.0.1 localhost.localdomain 127.0.1.1 dc 192.168.40.41 localhost 192.168.40.41 smbrussfeld.cis527.cs.ksu.edu smbrussfeld 192.168.40.41 dc.smbrussfeld.cis527.cs.ksu.edu dc The first line is updated to be localhost.localdomain, just to make it clear that that IP address is for the loopback adapter. The second line gives the new hostname for this system. In this case, I’m calling it dc for domain controller. The third line redirects the localhost domain name to the external IP address. Finally, the fourth and fifth lines give the name of the domain as well as the fully qualified domain name for this system, respectively.\nTo go along with this change, we’ll edit the hostname file as well:\nsudo nano /etc/hostname It should now be the same as the hostname used in the hosts file:\ndc Finally, we’ll need to restart the computer for these changes to take effect.\nOnce it has restarted, we’ll need to remove any existing Samba configuration files and database files.\nThankfully, there are some great commands for finding the location of those files. First, you can find the location of the config file using this command:\nsmbd -b | grep \"CONFIGFILE\" It is usually at /etc/samba/smb.conf. So, to delete it, you’ll use:\nsudo rm -f /etc/samba/smb.conf Next, you can find the locations of all database files using this command:\nsmbd -b | egrep \"LOCKDIR|STATEDIR|CACHEDIR|PRIVATE_DIR\" It should give you four different directory paths. So, for each of those directories, you’ll need to enter that directory, then delete all files with the .tdb and .ldb file extensions. So, for the first one, you could use these commands:\ncd /var/run/samba sudo rm -f *.tdb *.ldb Repeat that process for the other three directories to make sure they are all clear.\nOnce that is done, it is time to create our new domain. We’ll use the samba-tool command to do this in interactive mode:\nsudo samba-tool domain provision --use-rfc2307 --interactive First, it will ask you for the realm. For this example, I’ll use the name smbrussfeld.cis527.cs.ksu.edu. Following that, it should ask you for the NetBIOS name of your domain. The default should be smbrussfeld, which is fine. You can also accept the defaults of dc for the server role, and SAMBA_INTERNAL for the DNS backend. For the DNS forwarder address, you can use the VMware router’s IP address, or any other valid DNS server address from Lab 3. Finally, you’ll need to enter a password for the Administrator account. I’ll use the same password as always.\nOnce you’ve entered that information, your domain will be configured. Now, we’ll need to add a user to Samba and enable it using the following commands:\nsudo smbpasswd -a \u003cusername\u003e sudo smbpasswd -e \u003cusername\u003e Next, we’ll need to disable the systemd-resolved service. This service is installed by default on Ubuntu 18.04, and is responsible for caching local DNS queries. However, since it binds to port 53, it will prevent our Samba server from binding to that port. To disable it, we’ll use the following commands:\nsudo systemctl disable systemd-resolved sudo systemctl stop systemd-resolved We’ll also need to disable the existing Samba services, and enable the Samba Domain Controller service:\nsudo systemctl disable nmbd sudo systemctl stop nmbd sudo systemctl disable smbd sudo systemctl stop smbd sudo systemctl unmask samba-ad-dc sudo systemctl enable samba-ad-dc sudo systemctl start samba-ad-dc Lastly, we’ll need to replace any existing Kerberos configuration file with the one created by Samba. So, you’ll perform these commands to delete the existing file (if any) and create a symbolic link to the one from Samba.\nsudo mv /etc/krb5.conf /etc/krb5.conf.orig ​sudo ln -sf /var/lib/samba/private/krb5.conf /etc/krb5.conf If you get an error from the first command, you can safely ignore it.\nFinally, if everything is configured correctly, we can query our domain using the smbclient command:\nsmbclient -L localhost -U% Hopefully you should get output from that command giving information about your domain. If not, you’ll probably need to review the Samba log file stored in /var/log/samba/log.samba to see what the error is.\nIf everything is working correctly, you can then add your Windows 10 VM to the domain. First, as with any client I wish to add to a domain, I’ll need to set a static DNS entry to point to the domain controller itself. For the second entry, I’ll just use the VMware router as always. Then, I can add it to the domain following the instructions in the earlier video.\nAfter a reboot, it should allow you to log in as the domain user created earlier.\nIt is quite a process, but hopefully this demonstrates what it takes to create an Active Directory Domain using Samba. I highly recommend that you try this process at least once, even if you don’t plan on completing it as part of Lab 4, just to see how it all works together.\n",
    "description": "",
    "tags": null,
    "title": "Windows Client on Linux Domain",
    "uri": "/4-directory-services/10-windows-client-on-linux-domain/index.html"
  },
  {
    "content": " YouTube Video Resources .NET Forum from JitBit SQL Server 2017 Express Edition from Microsoft Video Transcript In this video, I’ll go through some of the basic steps to install a .NET web application on your server. For this example, I’ll be using a trial version of the JitBit .NET Forum software, as it is a good representative example of what it takes to deploy an existing .NET application to your server.\nFirst, I’ll need to install Microsoft SQL Server Express Edition to use as the database server for this application. This is a compact version of the full Microsoft SQL Server that is designed for smaller applications and development. You can find a link to download that software in the resources section below this video. Once it has finished installing, you can click Close to exit the installer.\nNext, I’ll need to download the JitBit .NET Forum software from their website. I’ve provided a link to that software in the resources section below this video as well. Feel free to download that software and follow along if you’d like. However, you won’t be able to use JitBit’s .NET Forum as your web application for the lab assignment, as I’d like you to demonstrate you can install a web application independently.\nOnce I’ve downloaded the software, I can extract the ZIP file into my downloads directory. When you open that directory, you’ll see a file that is very obviously the “README” for this application. In general, many applications will provide installation instructions either via their website or as part of the downloaded software. After all, if customers aren’t able to install your software, they are not very likely to use it either. However, that doesn’t mean that the installation instructions are always good, nor will they always exactly fit your needs either. So, you’ll need to learn how to read the instructions and adapt them to fit your situation.\nIn this case, I’m going to install this software on top of the existing website example.local that I created in the last video. So, I’ll adapt these instructions just a bit to fit that setup.\nFirst, I’ll need to copy all of these files to the folder I created for this website, which is stored at C:\\inetpub\\example. You’ll notice when you do so that it will overwrite the file named web.config, so we’ll have to deal with that a bit later. Finally, I’ll also need to delete the file named index.html that I created earlier.\nNext, I’ll need to convert the existing website to an application in IIS Manager. To do that, right-click the Example website in IIS Manager and choose Add Application. There, you’ll give the application an alias, as well as the physical path to the files for this application. In addition, you’ll need to choose the application pool. Since this application is using .NET 4.0, I can just use the “DefaultAppPool” here. When you click OK you should see your application appear in the menu to the left.\nI’ll also need to confirm that the website itself is configured to use the same application pool. You can do so by clicking the website in the list on the left, and then clicking the Basic Settings option on the far right.\nOnce you’ve created your application, you’ll need to change some file permissions. The official guide from JitBit recommends that you change the user identity used by your application pool, and then assign that account the permissions needed. So, to do that, I’ll click on the Application Pools option on the left side of IIS Manager, and then right-click the DefaultAppPool and select Advanced Settings. In that window, find the Identity option, and change it to use the NetworkService built-in account. Click OK to save that setting.\nOnce that is done, I’ll fully restart my IIS server to make sure the changes take effect. I can do so by right-clicking the server name in the list on the left, and then choosing Stop. I can then right-click it again and choose Start to start it again.\nNow, navigate to where you stored the files for .NET Forum, and give the NetworkService account full control of the files in the App_Data folder. This allows your web application to store files in that folder.\nLastly, you’ll need to configure the database connection. That information can be found in the web.config file in your application’s directory. The instructions give a variety of options for configuring the database, but in general we can just use the default option to use SQL Server Express.\nTo test your application, clear the cache in your web browser once again, then navigate to http://example.local. You should see the application load with the title “Acme Forum” on the page. To test the application, you can log in with the default credentials listed in the instructions, and then create a new forum. If everything is working correctly, it should allow you to do this without any problems.\nHowever, you should hopefully have noticed that it no longer redirected you to HTTPS when you loaded the website. Did you? When we created that URL redirect, it stored the settings in the web.config file that was previously located in this folder. So, we’ll have to set that up again.\nOnce you do so, you should test it once again. Hopefully this time it should redirect properly. If you’d like, you can also review the contents of the web.config file in the web application’s directory to see the additional information added to that file by the URL Rewrite module.\nThat should do it! You’ve now configured and deployed your first .NET web application in IIS. This example shows some of the details you may have to deal with when deploying a web application in IIS, but each application is different. In short, you’ll always have to read the documentation carefully, but use your own knowledge and experience to adapt the instructions to match your own server’s configuration. In addition, don’t be afraid to search for additional information on the internet. All of those resources will help you to complete your task.\n",
    "description": "",
    "tags": null,
    "title": "Windows Web Application",
    "uri": "/6-application-servers/10-windows-web-application/index.html"
  },
  {
    "content": "It appears that I have managed to confuse lots of people with the Windows AD portion of this lab. That’s not my intent, and any confusion is totally my fault here. This is mostly due to the fact that I reused an older video showing how to set up an Active Directory server and not clearly describing how things should be changed. I’ll do my best to clarify what you need to know and how you can adapt any existing setup you may have so that it works.\nWhen configuring an Active Directory (AD) domain, there are two important settings: the root domain name (numbered 1 in the screenshots below) and the NETBIOS domain name (numbered 2 in the screenshots below).\nIn an ideal setup, those names are closely related. Typically, if the root domain name is ad.cis527.cs.ksu.edu then the NETBIOS domain name would be ad - the first part of the root domain name. In most cases, you should not change the NETBIOS domain name from the default that is proposed by the setup process. (If you did, that’s fine - keep reading!)\nIn the video for setting up an AD domain, I used a simplified root domain name cis527.local, which meant that my NETBIOS domain name would be cis527 as shown in the screenshots below:\nFor the lab assignment itself, I give you a more concrete desired root domain name of ad.cis527\u003cyour eID\u003e.cs.ksu.edu. However, I do not specify the NETBIOS domain name, which should default to ad. However, you can freely change it, and I believe that the video is leading many students to change that value to either cis527 or cis527\u003cyour eID\u003e. To be clear, you should not change the NETBIOS domain name at this step - whatever is proposed by the configuration wizard is correct.\nIf you’ve changed this, that’s fine! You’ll just have to adapt a couple of things further down the line.\nTo see how your domain is set up, open the Active Directory Users and Computers tool in your Windows Server VM, and find the user you created in the Users folder and open it. There should be an Account tab that looks like the following screenshot (this is from a different VM setup, so the domain names are purposefully different than what the lab specifies):\nIn that screenshot, you can clearly see the root domain name labelled with the number 1, which is adrussfeld.cis527.cs.ksu.edu, and the NETBIOS domain name labelled with number 2 is ADRUSSFELD. This is the VM that I used for creating the Linux Client on Windows Domain video that was new for Summer 2020. So, make note of both of these settings in your AD configuration, as you’ll need them when trying to log on to the domain or run commands.\nWhen referencing usernames on an AD domain, there are two basic methods:\n\u003cusername\u003e@\u003croot domain name\u003e \u003cNETBIOS domain name\u003e\\\u003cusername\u003e So, when I want to log in as that user on Ubuntu, I would use method 1 with the root domain name (this is before we change the sssd configuration file to not require the full domain name):\nHowever, if I want to do the ldapsearch command to search the Windows AD, I’ll need to use method 2 with the NETBIOS domain name:\nSo, you should be able to adapt to whichever method is required, making changes where necessary to make the commands work.\nQ: What if my root domain name and NETBIOS domain name are different? For example, my root domain name is ad.cis527\u003ceID\u003e.cs.ksu.edu but I set my NETBIOS domain name to cis527\u003ceID\u003e.\nAs far as I can tell, you should still be able to complete Lab 4 with an AD domain configured in this way. The only issues you may run into are when using the realm join command and the ldapsearch commands on Ubuntu. You’ll have to pay close attention to which method is being used, and adjust the commands as necessary to fit the situation. So far, I’ve worked with at least two students who ran into this issue but were able to get it to work with minimal problems.\nIt may also be very important to make sure that both your Windows client and Ubuntu client are configured to use the Windows AD server as the first DNS entry. I usually just set a static DNS entry on each of those systems before joining the domain as directed in the video - in the real world you’d have your DHCP server do this for you.\n",
    "description": "",
    "tags": null,
    "title": "AD and NETBIOS Names",
    "uri": "/4-directory-services/11-adds-netbios/index.html"
  },
  {
    "content": " Note The examples in this video are for an old version of Lab 3, but should be instructive enough to be useful. Read the new Lab 3 assignment carefully and follow its instructions when configuring your system for grading. Some commands are different - see the transcript below for updated commands –Russ\nYouTube Video Resources Slides Domain Name System (DNS) on Wikipedia List of DNS Record Types on Wikipedia Domain Name System (DNS) History from Living Internet BIND on Wikipedia HOSTS.TXT from March 22, 1985 Root Name Server on Wikipedia Root Files from IANA How To Configure BIND as a Private Network DNS Server on Ubuntu 18.04 from DigitalOcean (works for 20.04 as well) Bind9 Server How-To from Ubuntu Community Help Wiki DNS Configuration from Ubuntu Server Guide BIND Manuals from bind9.net Video Transcript The Domain Name System, or DNS, is another integral part of working with the internet and larger networks today. In this video, we’ll cover the history of DNS, how it works, and how to use it in an enterprise organization using the BIND software.\nAs a quick note, much of the information in this lecture is adapted from information provided by Seth Galitzer, our system administrator here in K-State CS. He created the original slides as part of a guest lecture in this course when it was offered on campus, and was gracious enough to share this information for future versions of the course.\nFirst, let’s review some quick history. As you may know, the precursor to today’s internet, the ARPANET, was first conceived in 1965. By 1969, the first four nodes of that network were connected. Over the next two decades, ARPANET slowly grew in size, and by 1982, with the introduction of TCP and IP, it exploded in size as various networks joined together to create the internet we know today.\nIn the early days, they found it would be helpful to have a list of human-readable names for the nodes on the network. In that way, instead of remembering the IP address of a particular server, you could just remember its name and look up the IP address, much like you would look up the phone number of a person or business using a phone book. So, they created a file called hosts.txt, which was hosted by the Stanford Research Institute. In essence, it contained a list of all of the servers on ARPANET, paired with its IP address. Anyone could query that file with a computer name, and get back the IP address of the system.\nWhile it was a useful system, it had some major drawbacks: first, it must be updated manually. At one time, the only way to add a new system to the file was to contact the person in charge of it via telephone. As the file grew larger, it was more and more difficult to maintain consistency and avoid name collisions. Finally, it became very taxing on SRIs system, as each system on the growing internet would download a copy of the hosts.txt file to store locally, sometimes requesting it many times per day to get the very latest version of the file. So, a new system needed to be built to provide this feature.\nIn 1983, the first version of the Domain Name Service, or DNS, was published as an RFC. It was later finalized in 1987. They proposed creating a distributed system of name servers, as well as a hierarchical, consistent name space for all the systems on the internet. Beyond just working with domain names and IP addresses, they also added the ability for the system to work with many different protocols and data types, hopefully designing it in such a way that it would work well into the future. Thankfully, their design was very successful, and we still use it today.\nThe domain name space we use today has a hierarchical format, much like this diagram. At the very top of the diagram is the root nameserver. It is responsible for keeping track of the locations of the DNS servers for all top-level domains, or TLDs, in the current domain name space. The original DNS specification calls for 13 root name servers, which is still true today. However, due to advances in technology, there are nearly 1000 redundant root name servers on the internet today, each one a clone of one of the 13 root servers. The file containing information for all of the top-level domains is very small, only about 2 MB, and can be viewed by following the link in the resources section below this video.\nBelow the root name server is the name server for each chosen top-level domain. For example, these are nameservers for the .com, .org, .edu and other top-level domains. Under each top-level domain are the name servers for each individual domain name, such as yahoo.com, slashdot.org, or k-state.edu. Within each domain, there can be additional levels of delegation, such as the cs.k-state.edu subzone, maintained within the K-State CS department for our internal systems.\nWith the hierarchical design of the domain name space, it may take a few steps to determine the appropriate IP address for a given domain name. For example, if you wanted to find the IP address of www.wikipedia.org, you might first start by querying the root name server for the location of the .org name server. Then, the .org nameserver could tell you where the wikipedia.org name server is. Finally, when you ask the wikipedia.org name server where www.wikipedia.org is located, it will be able to tell you since it is the authoritative name server for that domain. In practice, often there is a caching DNS server hosted by your ISP that stores previously requested domain names, so you won’t always have to talk directly with the root name servers. This helps reduce the overall load across the root servers and makes many queries much faster.\nThe most commonly used DNS software today is BIND. BIND was originally developed in the 1980s as the first software to fully implement the new DNS standard, and it has been constantly under development ever since. The latest version of BIND is BIND 9, which was first released in 2000, but still consistently gets updates even today.\nThe DNS specification includes many different types of records. The most commonly used ones are listed here. For example, an A record is used to list a specific IPv4 address for a host name, whereas a CNAME record is used to provide an alias for another domain name. For this lab assignment, you’ll be configuring a DNS server using BIND within your network and using several of these record types, so let’s take a look at how that works.\nHere I have configured an Ubuntu VM as directed in Lab 3 to act as a DHCP and DNS server. I also have a second Ubuntu VM acting as our client. Finally, I have disabled the DHCP server in VMware on this network segment.\nFirst, on my server, I’m going to start Wireshark so we can capture these packets. I’ll also add a filter for dns to make sure we only see the DNS server packets.\nOn the client, I have already verified that it is configured to use the other Ubuntu VM as a DNS server. You can see the currently configured DNS servers using this command:\nresolvectl status At the bottom of that output, it should show the current DNS server for your ethernet connection. You’ll have to press Q to close the output. To query a DNS record, we can use a couple of different commands. First, we can use the dig command to lookup a DNS name:\ndig ns.cis527.cs.ksu.edu Looking at the output received, we can see that we did indeed get the correct IP address. We can also run that command for win.cis527.cs.ksu.edu and ubu.cis527.cs.ksu.edu. Note that the output for ubu.cis527.cs.ksu.edu includes both the CNAME record and the A record.\nTo perform a reverse lookup, we can use the dig -x command. Since my sample network is using the 192.168.40.0/24 subnet, I could look up the following IP address:\ndig -x 192.168.40.41 It should return the PTR record associated with that IP. I can do the same for the IP address ending in 42 as well.\nOn a Windows computer, you can use the nslookup command without any additional options to perform both forward and reverse DNS lookups.\nBack on the server VM, we should clearly be able to see the DNS packets in Wireshark. Each one gives the type of record requested, and just below it is the response packet with the answer from our DNS server.\nWith that information, you should be ready to configure your own DNS server. As always, if you have any questions or run into issues, please post in the course discussion forums on Canvas. This process can definitely be frustrating the first time you do it, since there is so much new information to read and understand. Don’t be afraid to ask for help if you get stuck!\n",
    "description": "",
    "tags": null,
    "title": "DNS",
    "uri": "/3-core-networking-services/11-dns/index.html"
  },
  {
    "content": " YouTube Video Resources Resource Type: User from Puppet Resource Tips and Examples: Package on Windows from Puppet Managing Windows Configurations from Puppet Puppet on Windows Pack from Puppet Forge Installing and Using Windows Modules from Puppet download_file Module from Puppet Forge Running Puppet’s Commands on Windows from Puppet Using User and Group on Windows from Puppet puppetlabs-acl Module on Puppet Forge Video Script Here are a few hints for completing Lab 2, based on the struggles some students have had during previous semesters.\nFirst, let’s talk about passwords. On the Puppet documentation for the user resource type, it notes that on Linux, the password given in the configuration file must already be encrypted. Thankfully, it gives you some hints here on using built-in functions, or functions from the Puppet Labs Standard Library, to calculate the correct encrypted password. You can also use the Sensitive data type to redact the plain-text password from log files. Of course, on Windows, you can only use cleartext passwords.\nNext, installing packages on Windows can be quite difficult without using a package management program such as Chocolatey. In the resources section below this video, I’ve included a few links describing the process for installing and managing packages in Windows in detail. In short, you’ll need to be very careful about the title matching the actual DisplayName of the package, as well as the install_options to make them install silently. This may take a bit of trial and error to get it working correctly.\nIn addition, when you download the installation files for Windows, make sure you get the full installers and not the “stub” installer that just downloads the real installer in the background. Sometimes you have to dig a bit deeper on the vendor’s website to find these. As stated in the assignment sheet, you may also choose to download the files using the download_file Puppet module. Either approach will work.\nAnother important note: on both Windows and Linux, changes to group membership do not take effect immediately. On Linux, the current user must log-out and log-in to see the change, whereas on Windows a reboot is required in most cases. Because of this, when testing your manifest files, you may find it necessary to apply the manifest, then logout/reboot and apply it again for all resources to be configured correctly. That is fine, provided that it works as intended on the second application.\nFinally, you may find that defining permissions in Windows is difficult using the default file resource. You may choose to install the puppetlabs-acl module to configure Windows permissions directly. The resources section below the video includes a link to that module and its documentation as well.\nI hope these hints help you successfully complete Lab 2 with minimal frustration. As always, if you have any questions or run into any issues, please post on the course discussion forums before contacting the instructor. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Hints",
    "uri": "/2-configuration-management/11-hints/index.html"
  },
  {
    "content": " Windows File Server (5) Everyone share Restricted share (permissions) Windows Group Policy (5) GPO for Everyone GPO for Restricted (attached to what?) Ubuntu File Server (5) Everyone share Homes share cis527 user enabled Ubuntu Drive Mapping (5) fstab entry libpam-mount entry Windows Web Application Server (15) App Installed \u0026 Working SSL Certificate SSL Redirect Host Header Ubuntu Web Application Server (15) App Installed \u0026 Working MySQL on Backend Virtual Host SSL Certificate SSL Redirect Ubuntu firewall (-5 points if not enabled or configured) ",
    "description": "",
    "tags": null,
    "title": "Lab 6 Grading Checklist",
    "uri": "/z-instructor-resources/11-lab-6-grading-checklist/index.html"
  },
  {
    "content": " Note It appears that MySQL will now be installed with TLS enabled on Ubuntu 20.04, so you may not have to do that step. –Russ\nYouTube Video Resources Install MySQL on Ubuntu 20.04 LTS Linux from LinuxConfig.org How To Install and Secure phpMyAdmin on Ubuntu 18.04 from DigitalOcean (works for 20.04) How To Install the Apache Web Server on Ubuntu 20.04 from DigitalOcean How To Secure Apache with Let’s Encrypt on Ubuntu 20.04 from DigitalOcean How To Set Up A Remote Database to Optimize Site Performance with MySQL on Ubuntu 18.04 from DigitalOcean (works for 20.04) How To Configure SSL/TLS for MySQL on Ubuntu 18.04 from DigitalOcean (not required for 20.04) Video Transcript In this video, I’m going to discuss some of the steps needed to prepare your Ubuntu servers running in the cloud to act as web application servers. For this example, I’ll be using a single server as both my web and database server, but for the lab assignment you’ll need to adapt this configuration to have the web application installed on your FRONTEND server access the database server on your BACKEND server.\nAt this point, you should already have Apache installed, as well as a couple of sites and virtual hosts created from Lab 5. I’m going to use one of those existing virtual hosts as the basis for this web application. Remember that, for the lab assignment, you’ll be creating another new virtual host for this application.\nFirst, you’ll need to install PHP and it’s associated Apache module on your FRONTEND server:\nsudo apt update sudo apt install php libapache2-mod-php That will install and configure PHP on your system. To test it, you can create a new file in the web directory of one of your virtual hosts named test.php and add the following content:\n\u003c?php phpinfo(); ?\u003e Then, navigate to that virtual host and load the test.php file from that website. For my example, I would go to http://foo.russfeld.me/test.php. Hopefully you should see a website showing all of the PHP configuration information for your system. Once you confirm it is working, it is always a best practice to delete this test.php test file, as the information it provides could be used by a malicious person to attack your server.\nNow that we’ve confirmed that PHP is working, it is time to install the MySQL database server. This time, on your BACKEND server, do the following:\nsudo apt update sudo apt install mysql-server That will install the MySQL server on your system. However, by default it is configured very insecurely, so you’ll need to reconfigure it to be more secure. Thankfully, there is a script to do just that:\nsudo mysql_secure_installation That script will ask you a series of questions to help you secure your MySQL server. In general, you should answer “Yes” to all of the questions, and set the password policy to at least “MEDIUM” security. In addition, you’ll be asked to set a password for the root account on the server. As before, make sure it is very memorable password, and do not reuse any of the passwords we’ve previously used in this course.\nOnce that is finished, you can test the connection to MySQL using the following command:\nsudo mysql -u root It should take you to a new prompt that begins with mysql\u003e. Once there, we should create a new user for us to use for development. So, enter these commands at that prompt:\nCREATE USER 'admin'@'localhost' IDENTIFIED BY '\u003cpassword\u003e'; GRANT ALL PRIVILEGES ON *.* TO 'admin'@'localhost' WITH GRANT OPTION; where \u003cpassword\u003e is the password you’d like to use for that account. Once that is done, you can enter:\nexit to close that connection. Now, you can test the new connection without using sudo as follows:\nmysql -u admin -p -h 127.0.0.1 It should ask you for a password, and once you enter the password you created for the admin account above, it should take you back to the mysql\u003e prompt. While there, enter the following command:\n\\s to see the connection information. You’ll notice that TLS (SSL) is not enabled. To enable that, follow the steps in the guide from DigitalOcean linked in the resources section below this video. I won’t walk through those steps here since you’ll doing that as part of your lab assignment. In addition, you’ll want to follow the steps in that guide for configuring access for remote clients, as we’ll be doing that to configure a web application in the next video.\nThat should cover everything you need to prepare your system for a web application that uses PHP and MySQL. In the next video, we’ll go through the process of installing a simple application in this environment.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Web Server",
    "uri": "/6-application-servers/11-ubuntu-web-server/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Adjust Windows 10 Firewall Rules \u0026 Settings from Online Tech Tips Configuring Virtual Network Adapter Settings from VMware Using the Virtual Network Editor from VMware Video Script In this last video on Windows 10, we’ll discuss some important security and networking details needed to finish getting your VMs set up and configured properly.\nFirst, let’s talk about security. Whenever you install a new operating system, there are 4 major steps you should always perform before doing just about anything else on the system. Those steps are:\nConfigure User Accounts \u0026 Passwords - at least get your administrator and standard account set up. You can add more users later, or you may add it to an Active Directory Domain for user accounts. Configure the Firewall - Windows 10 has the firewall enabled by default, but you should always confirm that your firewall is active before accessing the internet. On older, unpatched versions of Windows, researchers were able to demonstrate that the system can be compromised in as little as 5 minutes just by connecting it to the internet without any additional security or steps taken. Install Antivirus Software - All computers should still run some form of antivirus software. Windows comes with Windows Defender installed by default, and it will suffice as a basic form of antivirus protection. On a production system, I recommend installing a professional antivirus solution if possible. Install All System Updates - You should also install all available system updates for your operating system. This makes sure you have patches against the latest known flaws and attacks. Windows will generally do so for you automatically, but you can check for updates manually through the Windows Update. Let’s take a quick look at the Windows Firewall, since you’ll need to allow an application through the firewall. You can find it by searching for “firewall” on the Start Menu. Lab 1 directs you to install the Internet Information Services (IIS) web server. You’ll need to allow it through the firewall somehow. I won’t show you how in this video, but I encourage you to review the links in the resources section below the video for documentation showing how to accomplish this task. There are several ways to do it.\nTo test your firewall configuration, you can use your Ubuntu virtual machine created as part of Lab 1. First, make sure they are both on the same network segment in VMware by looking at the hardware configuration for each virtual machine. Then, you’ll need to get the IP address of the Windows computer. There are several ways to do this, but one of the simplest is to go to the Network Settings by clicking the networking icon in the system tray, near the clock, then choosing the Ethernet adapter. Here you’ll find the IPv4 address, usually in form of four numbers separated by decimal points. We’ll spend most of Module 3 discussing networking, so I won’t go into too much detail here.\nOnce you have that IP address, switch to your Ubuntu virtual machine, and open up the Firefox web browser. At the top in the address bar, simply input the IP address and press enter. If everything works correctly, you should be presented with the default IIS screen as seen here. If not, you’ll need to do some debugging to figure out what is missing.\nWith that, you should now have all the information you need to finish the Windows portion of Lab 1. If you have issues, please feel free to post in the course discussion forums or chat with me. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Windows 10 Security \u0026 Networking",
    "uri": "/1-secure-workstations/11-windows-security-networking/index.html"
  },
  {
    "content": "Here are some helpful networking diagrams to better explain the desired state before and after the end of Lab 4. In all of these diagrams, I am using a sample network with the IP address 192.168.40.0. You will have to adapt the IPs to match your network configuration. In addition, each network address will need to be updated with your unique eID as directed in the lab assignment.\nAfter Lab 3 Once you’ve completed all of Lab 3, your network diagram should resemble the one shown above. Recall that all systems are currently configured to get DHCP and DNS information from your Ubuntu VM labelled Server, so in general it must be running at all times for this network to function. In addition, the Ubuntu VM labelled Server does not use itself for DNS, but still uses the VMare built-in DNS server, though you may have configured an alternate entry so it can refer to itself for DNS as well. This is the most likely starting point for this lab if you completed Lab 3.\nStarting Lab 4 with a Working Lab 3 Solution This diagram shows what an ideal network setup would look like, provided you have a working Lab 3 solution. In Lab 4, most of the changes required to get from the final Lab 3 network to this configuration are spread across multiple tasks. However, here are the major highlights:\nVMware NAT Network - the built-in DHCP server should still be disabled as it was in Lab 3. Ubuntu Server VM - now uses itself as a primary DNS entry, with an alternate entry set to the VMware DNS server (Lab 4, Task 4). Must be able to resolve ldap.cis527\u003cyour eID\u003e.cs.ksu.edu Ubuntu Client VM - no changes, still gets both DNS and DHCP from the Ubuntu Server VM (Lab 4, Task 5). Must be able to resolve ldap.cis527\u003cyour eID\u003e.cs.ksu.edu Windows Server VM - new VM (Lab 4, Task 1). Should have a static IP address of 192.168.40.42 and a primary static DNS entry pointing to itself, with an alternate entry set to the Ubuntu Server VM or the VMware DNS server (Lab 4, Task 2). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. Windows 10 VM - now has a static DNS entry to point to the Windows Server VM, with an alternate entry set to the Ubuntu Server VM or the VMware DNS server (Lab 4, Task 3). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. Ubuntu Client VM - Windows Snapshot - when configuring the Ubuntu Client VM to connect to Windows Active Directory, it should have a static DNS entry pointing to the Windows Server VM, with an alternate entry set to the Ubuntu Server VM or the VMware DNS server (Lab 4, Task 6). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. This is the ideal setup for a this lab, but it depends on a fully working Lab 3 solution. In addition, you’ll need to keep the Ubuntu Server VM running at all times, which may present a problem on systems that struggle to run multiple VMs concurrently. So, below I discuss two alternative options.\nLab 4 Alternative Network Setup - Option 1 (Static DNS Entries) In this diagram, we are bypassing the DHCP server configured in Lab 3, and reverting our network back to using the VMware DHCP server. In addition, we are avoiding using our own DNS server running on the Ubuntu Server VM unless absolutely necessary. So, the following changes are made to the network from Lab 3:\nVMware NAT Network - the built-in DHCP server must be re-enabled if it was disabled in Lab 3. Ubuntu Server VM - now uses itself as a primary DNS entry, with an alternate entry set to the VMware DNS server (Lab 4, Task 4). The DHCP server that was configured in Lab 3 is disabled using sudo systemctl disable isc-dhcp-server. Must still be able to resolve ldap.cis527\u003cyour eID\u003e.cs.ksu.edu Ubuntu Client VM - now gets a DHCP address from VMware. We must set a primary static DNS entry to point to the Ubuntu Server VM, with an alternate entry set to the VMware DNS server (Lab 4, Task 5). Must be able to resolve ldap.cis527\u003cyour eID\u003e.cs.ksu.edu Windows Server VM - new VM (Lab 4, Task 1). Should have a static IP address of 192.168.40.42 and a primary static DNS entry pointing to itself, with an alternate entry set to the the VMware DNS server (Lab 4, Task 2). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. Windows 10 VM - now has a static DNS entry to point to the Windows Server VM, with an alternate entry set to the VMware DNS server (Lab 4, Task 3). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. Ubuntu Client VM - Windows Snapshot - when configuring the Ubuntu Client VM to connect to Windows Active Directory, it should have a static DNS entry pointing to the Windows Server VM, with an alternate entry set to the VMware DNS server (Lab 4, Task 6). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. In effect, each system that is getting automatic IP addresses is now using the VMware DHCP server. However, since that DHCP server does not give the correct DNS addresses, we must set static DNS entries on everything, with the Ubuntu Server VM and Ubuntu Client VM pointing to the DNS server running on the Ubuntu Server VM, and the Windows Server, Windows 10 Client, and the Ubuntu Client VM’s Windows Snapshot pointing to the DNS server running on the Windows Server (after Active Directory is installed). In addition, each of those systems should have an alternate DNS entry that points to the VMware DNS server, allowing them to still access the internet properly.\nHowever, if you are still having issues with your DNS entries or DNS servers not working correctly, fear not! There is one more alternative option available to you.\nLab 4 Alternative Network Setup - Option 2 (Hosts File Entries) In this diagram, we are bypassing the DHCP server configured in Lab 3, and reverting our network back to using the VMware DHCP server. In addition, we are avoiding using our own DNS server running on the Ubuntu Server VM unless absolutely necessary. Finally, we are adding entries to the hosts file on each system to bypass DNS completely, ensuring that we can still complete this lab even if our DNS systems are not working at all. So, the following changes are made to the network from Lab 3:\nVMware NAT Network - the built-in DHCP server must be re-enabled if it was disabled in Lab 3. Ubuntu Server VM - now uses itself as a primary DNS entry, with an alternate entry set to the VMware DNS server (Lab 4, Task 4). The DHCP server that was configured in Lab 3 is disabled using sudo systemctl disable isc-dhcp-server. Must still be able to resolve ldap.cis527\u003cyour eID\u003e.cs.ksu.edu. If it cannot, add an entry to the /etc/hosts file that links 192.168.40.41 to ldap.cis527\u003cyour eID\u003e.cs.ksu.edu. Ubuntu Client VM - now gets a DHCP address from VMware. We must set a primary static DNS entry to point to the Ubuntu Server VM, with an alternate entry set to the VMware DNS server (Lab 4, Task 5). Must be able to resolve ldap.cis527\u003cyour eID\u003e.cs.ksu.edu. If it cannot, add an entry to the /etc/hosts file that links 192.168.40.41 to ldap.cis527\u003cyour eID\u003e.cs.ksu.edu. Windows Server VM - new VM (Lab 4, Task 1). Should have a static IP address of 192.168.40.42 and a primary static DNS entry pointing to itself, with an alternate entry set to the the VMware DNS server (Lab 4, Task 2). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. If it cannot, you are better off just rebuilding your AD server - this should always work on a working AD server! Windows 10 VM - now has a static DNS entry to point to the Windows Server VM, with an alternate entry set to the VMware DNS server (Lab 4, Task 3). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. If it cannot, add an entry to the C:\\Windows\\System32\\drivers\\etc\\hosts file that links 192.168.40.42 to ad.cis527\u003cyour eID\u003e.cs.ksu.edu. Ubuntu Client VM - Windows Snapshot - when configuring the Ubuntu Client VM to connect to Windows Active Directory, it should have a static DNS entry pointing to the Windows Server VM, with an alternate entry set to the VMware DNS server (Lab 4, Task 6). Once the Active Directory server is installed, it should be able to resolve ad.cis527\u003cyour eID\u003e.cs.ksu.edu. If it cannot, add an entry to the /etc/hosts file that links 192.168.40.42 to ad.cis527\u003cyour eID\u003e.cs.ksu.edu. In essence, we still configure each system with the correct static DNS entries. However, as a fallback in case those DNS entries are not working properly, we can use entries in the hosts file to bypass DNS completely and guarantee that we’re always going to get to the correct system.\nThis is a very useful workaround, and while an ideal network in the real world shouldn’t require it, many times system administrators include entries in the hosts file to ensure that systems can still be used in the event of DNS failure. Otherwise, an innocuous DNS failure could prevent users from even logging on to the system, either via LDAP or Active Directory.\nAs shown in the diagram above, the only DNS entry that must work is the Windows Server VM must be able to use itself for DNS. If that does not work, then most likely your Active Directory configuration failed and you will need to try again. Windows Active Directory relies heavily on DNS to function properly. I’ve not ever had this happen in my experience, so I highly doubt this failure case will be common.\nYou can find more information about how to edit your hosts file at this resource .\n",
    "description": "",
    "tags": null,
    "title": "Lab 4 Networking Diagrams",
    "uri": "/4-directory-services/12-lab4-networking/index.html"
  },
  {
    "content": "Greetings! Here is the information you’ll need to know to get your lab graded.\nGrading Date/Time: \u003ctime\u003e Contact me ASAP if that time will not work for you for any reason and I will work with you to reschedule. Zoom Link: \u003clink\u003e If you haven’t used Zoom before, I recommend going to https://ksu.zoom.us/ first to download the Zoom client software. Then, when the time comes, click the link above to join the Zoom session. Your Setup: Before you join the Zoom session for grading, please have the following steps completed on your end. This helps things go as smoothly as possible: Use the fastest internet connection available to you. If you are using a laptop with a wireless connection, you may want to plug in to a hard-wired connection if available. If you can work from campus, that may be best, depending on your home internet connection. Make sure your audio input device (microphone) is working in Zoom. You are not required to have a webcam, but you are welcome to use one if you like. I will be using a webcam and microphone on my end. Confirm that all the VMs for this lab are booted and running on your system. We’ll be using the desktop sharing features of Zoom so I can see your VMs. So, make sure you close any windows or programs that are running in the background that shouldn’t be shared with me. Operations: Here are a few operations you may be asked to demonstrate: Access File Shares from Windows 10 Client GPO in Windows Server Access File Shares from Ubuntu Client fstab and libpam-mount entries for automatically mounting shares Working Windows Web Application (host header, SSL redirect) Working Ubuntu Web Application (virtual host, SSL redirect, MySQL on Backend) Finality Once you begin the grading process, no changes may be made to your system to fix any errors encountered. The grade you receive will reflect the state of your VMs at the start of the grading process. This is to ensure that grades are fairly determined and that no special consideration is given. That should cover everything. Please let me know if you have any questions prior to your scheduled grading time.\nGood luck!\nRuss\n",
    "description": "",
    "tags": null,
    "title": "Lab 6 Grading Email",
    "uri": "/z-instructor-resources/12-lab-6-grading-email/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Simple Network Management Protocol (SNMP) on Wikipedia MIB Reference from SimpleWeb SNMPv2 MIB Reference from Harald T. Alvestrand How to Install and Configure an SNMP Daemon and Client on Ubuntu 18.04 from DigitalOcean (works for 20.04 as well) How to Use The Net-SNMP Tool Suite to Manage and Monitor Servers from DigitalOcean (works for 20.04) SNMP Agent from Ubuntu Community Help Wiki Video Transcript In the next few videos, we’ll take a look at few other important networking protocols that you may come across as a system administrator. The first one we’ll review is the Simple Network Management Protocol, or SNMP.\nIt was first developed in 1988 as a way for system administrators to query information from a variety of devices on a network, and possibly even update that information as needed. Remember that, in 1988, this was well before the development of the web browsers we know today, so it wasn’t as simple as using the web-based configuration interface present on most routers today. However, while most systems today support SNMP, it is primarily used for remote monitoring of networking equipment, and not as much for configuration as it was originally intended.\nOver the years, there have been many different versions of SNMP developed. Each one includes a few different features. The first version, SNMPv1, is very basic. While it works well for some things, it only includes a plain-text “community string” for authentication, resulting in minimal security. SNMPv2 is a significant revision to SNMPv1, and includes a much more robust security model. However, many users found that new model to be overly complex, and a second version called SNMPv2c, for “Community” was developed without the new security model. Finally, SNMPv3 was developed to include better security, authentication, and a more user-friendly design. It is the only currently accepted standard, though many devices still use the older versions as well.\nIn SNMP, the data is presented in the form of variables. The variables themselves have a very hierarchical structure, so that similar types of data are grouped together. However, the variables themselves can be difficult to read directly, since each level of the hierarchy is denoted by a number instead of a name.\nTo help make the variables more readable, SNMP includes a Management Information Base, or MIB, to define what each variable means. Each individual device can define its own MIB, though there are some standards available for common types of data. You can find a couple of those standards linked below the video.\nThe SNMP protocol itself lists many different types of protocol data units, or PDUs, as part of the standard. For example, the GetRequest PDU is used to query a particular variable on a device, and the Response PDU would be sent back from the device. You’ll be able to see several of these PDUs a bit later in the video when we use Wireshark to caputre some SNMP packets.\nAs mentioned earlier, one feature of SNMP is the use of a “community string” for authentication. In SNMPv1, the community string is a simple text identifier that you can provide along with your request. The server then determines if that community string has access to the variable it requested, and if so, it will return the appropriate response. However, since community strings are sent as plain-text, anyone who was able intercept a packet could find the community string, so it wasn’t very secure. In later versions of SNMP, additional security features were added to resolve this issue. In this video, we will see an example of using SNMPv3 with proper security and encryption.\nNow that you know a bit about SNMP, let’s see a quick example of how it works. Once again, I have configured an Ubuntu VM as directed in Lab 3 to act as an SNMP server, and I’ve also configured a second Ubuntu VM to act as an SNMP manager or client.\nFirst, on my server, I’m going to start Wireshark so we can capture these packets. Notice that I’m capturing packets on the ethernet adapter, since I’ll be accessing it from another system. I’ll also add a filter for snmp to make sure we only see the SNMP server packets.\nNext, on the client system, we can query the data available via SNMP using a couple of different commands. First, I’m going to use the simple snmpget command to query a single variable. I’ve already configured this system to use a set of authentication credentials stored in a configuration file, which you can do as part of Lab 3’s assignment. In this case, I’ll query the system’s uptime:\nsnmpget 192.168.40.41 sysUpTime.0 In the response, we can clearly see the system’s uptime. If we switch back to Wireshark, we can see that it captured some SNMP packets. If we were using SNMP v1, they would be plaintext and we could read the information here clearly. However, since we are now using SNMPv3\nTo see all the available SNMP variables on your system, you can try the following:\nsnmpwalk 192.168.40.41 This command will result in thousands of lines of output, giving all of the variables available on the system. Looking at Wireshark, there are lots of SNMP packets being transmitted. In fact, each data item in SNMP is sent via its own packet.\nSince it can be very difficult to find exactly what you are looking for using the snmpwalk command, you can use grep to search the output for a particular item. For example, to see all of the variables related to TCP, I could do the following:\nsnmpwalk 192.168.40.41 | grep TCP-MIB If I know the set of variables I’d like to query, I can also include them in the snmpwalk command, such as this example:\nsnmpwalk 192.168.40.41 TCP-MIB::tcp Either way, you should see the variables related to TCP. In the lab assignment, you’ll need to query some information about a different set of variables, so you’ll have to do some digging on your own to find the right ones.\nThat’s a quick overview of how to use SNMP to query information about your system across the network. If you have any questions about getting it configured on your system, use the course discussion forums on Canvas to ask a question anytime!\n",
    "description": "",
    "tags": null,
    "title": "SNMP",
    "uri": "/3-core-networking-services/12-snmp/index.html"
  },
  {
    "content": "\rDecrypt\rIntcIml2XCI6XCJ5MDZFVnhicUdFTmJ2Qk0rWEtSSlZnPT1cIixcInZcIjoxLFwiaXRlclwiOjEwMDAwLFwia3NcIjoxMjgsXCJ0c1wiOjY0LFwibW9kZVwiOlwiY2NtXCIsXCJhZGF0YVwiOlwiXCIsXCJjaXBoZXJcIjpcImFlc1wiLFwic2FsdFwiOlwiODNxZjZhUVBRU009XCIsXCJjdFwiOlwiN2VFbVZLaWpXd256VTc1VmRHbXhzOWVMR2xFV2Y3V25YdmVLMDVIL1E4cVJ1WFp4U3B2TloydkZzWVRkbllYcmFDdUJDZC9jLzA5ZWh4bTF5YzFjQm9XUGtGL3IrQUVEMEF1UUZtQmpnY3VqRlBabEFLYXdXTlJzdzZwM28xeGpjdEhiS1JQTkFKbW4yMWJGUENIMlFPbEExQmxOU3JNUCsyYmJkdnE4aWE5TjE2L24xalJNUDk4VVNYWjQzVDFIWE5LK1JHaVdnY2lLZlV0WHpJUnIvRWRUSjZwcDRmSmt4MndRbFYrZUtRcmJpNkV4L0F6bjdYZE4ram9JMWF5MjNvUUZ1TDFDNTQveDVGNWhVYWw2U2hwUndBNTgzQ3RndjkxV3R3a25sQU8zK2d4RDJ6ZUtDSW1lbmluQ2pabjVQVGc1eXlBNVBWTnozZWx6eVd3NE1vMis1bUFlUXZMa05qSkFlMlo4aEFTS05aemdQQ3ppZTVtVFVUcEhKb1JEYitidHdRREFZU0VhbnJham5GajdmeXN3Y1FLd1ZQRGIwZGlKQXZ5ajFPeGlCMy9OSmNFWXNCQlhlRG9neUhGaWc0blhRckJHOEtYSnIrQnhZeFJ0TTR0TzBnbFlvWkxxenlFVC84VDRRUmpBSmZ6cllBTEdvMngzMW5BZHR2d21paHNjcXhnNXc0Ni83Zm9KVzdqWHRnNmZUQVkxYitsanB5YkVjb2M3Z3E0U3ZjdlZnV0ZGWEh1Q2UxTXdXaVFFT04rcTlIanM0NXRwZ051eFo0Y0xUMjlrRGxTMXNmbm9WTzBUZkZVaWhUT054NkhsYk90MVdnZ0JsTWYwbUtPb1pRQnhmU1M4MXBnaGVrYm54Q3Evc1hzSW03WFZtWVVORDNsblBsU3dPY2IyeE1QUFd2UnpEWUs0NkhMc0pIT095M2src3Y1WkoybFN3VWMzdXpsajRtY2wxRXU4U3I5YXhoYjFBK2U1VkZRSTZyRDR5d01tanZkYjZCQkQ0YmpEdUxYeHlJVFVpd2huVSs4VVpKa1paTjZiRkZkYVRLREFvd1dZVWo3cXAzUkJwTDJob3NIOFRvWGFmRS80VDVaczhidHliTEx5N2FVdUxxVElnNHFybUZTVWdjNzVmL015dG1uaTFuQWRZUzBjZmRvdHN0c3hhalVKQUlMem5uSE45YjJhTmJ3a29BRXJVMWZVa3ZXQUcvdFI4VHpudFljbDZ1cjBoakRnR3dMbjFsbk5PNWV2YzQ2ZXJ1R3FaYUg3RXpwekZrdXFHQ0dVZkNEdW9IOU9EeDhtOWU2dkxlOTFnRm4rT3prNjltRGlZRks4TE5teDRuOVRFRER3RGNqbXFKZ3JOR29mMS9qQUZ5ZHR5V2thNUs5ZWJ1eEhQZnAxSGxHaHg1cnUwdHhGckx1bUpJSUZsNElMelNPSVVCU09xWlhTOFZrb1NKdnkvbU5ITjA5THFtQXhvNXRXQzg1V08zZzEvUy9Vei9PU3hNTVgrWHA0bzMxZzdVYjVmNjBBRWp4dW4wbkFEbUxpT2pCN0plekNsRzVqR3ljZ0RkWUxWNjIxdW1rSEk0bnFScVRUM2ZNdC9IRWpYNm51WUw5Mk5GWkZDRXd3Q2ZaUUcrZzFPeE9kMVBqdEFyUjBwSUxSYmQvem1yNG5vRElJMnAwZ1BSVXVXUTBsRjlPU1dzcTRxdm1aa0E0dzhMd2E2SkNIN1psUnpPVmhWbFN5SGNJd3pvR2dSQVZtbXNpSUNYT241UWUrdmFXYnVXeEV2OWdiQ1FjWlN2dDU2Z0hrTFYrQ1ZJb3pmbnpUQmFjdjBiUEg4bGFtSWpmYWFnckdDSURhRndHdWFobWd3WUk5M2NPOWxMSkRVQzMyTnltSHZ1eHlMbFJ2WEFnU2JIOEpuQk03TnVGSXNURzBUTmdGT0pQYjA0RXVPeDQ2bFd3dnNZaCtxcXZsakNqMkV3ajdZWm9nWnRjNFpqbjJ4Sy9CNHRZWktFTk9UaGtMclJGMDhuZHN0Q3hYRXNQV2R5N05NYmlrVmsxR0lvYlVqQUtLUENPQmtMR3gzbmp2dUIvSjg2dmFLejRXNlk3UzA0YkxNL1NSbE5zNzYvbmJucHpqd1Z0U1hwNWVSdFg2ZjZPSkhVZ3VBTEFISmlyQ3I1eHB5U0I1dUJlOUNCTlZzaEIvcmRseXZha0EyRjlGSXN6OUdsNU1FdkVLUjhvd0I2U1UreEE4dFlJTlY0YnhGUVdqV2syNElxV09DbklKWEtCMjZaTWMxVDB1WkxlWVZydkQyWGJER08wSzN5dEp5QVlzc0twbGVCVHlNYWhGU1lVTGwya3ExekoxZTRYVk5YUXMwS3U5S01NTy9NU3VsVmU3anRKUGI4VXdiRmxLOENWaVMxRWNUNTRXUVJYVEdOMmtpSGNQSUN2dklBT1ZuMzlpVE4yQU9yS3EvRktoZ0xndi9ZdXh3c0JYNEh6aVRESzAycVlCUDBtY1BUNVpCQlkwbk1sUzZWVk5KczE5R3daOWJnaHV2cjJHR0FScjhpZC9IZGpoUWo5eUkxK1kvRzV4Z3RXN1hQaEpVZjRyMndMMjIwaVRUeDJoekxmZzZOd2g0elFzbGtseUFLd0hQVXJPeFIzQnhSYWQvYkNQZnp5eGlQOTFEdXUxaEE4UldYdTN4UEdZOFRnZDJLQm5FVGN4NFZGMGRUY2dEdjA2N3E0UHRaM3FBTXVvZUZnR0g0dG1DaHhvL1pOMVd5NTJWVGxPYVpjcUM2RjNldVhIUEFnVFdWN1pqRDhzb00vZUdGVkNpeitRcExBcTFNN050aTczZlpLSmtzeVVLTUNRTjZkOUxRWmlrcnptM1hOaUlLRU13d3o0NzUvd0paSjdhbk9UTnM1blJOekdvMHNXQVMwdW5zNE1EblhNWHN4bUJ1bTV1Nm9hek83dm5NWXdvbUFyUWExeExSdWpId0Y1YWwxOEhjMmhLNEpQZlBKRy9NUVhDaTF0bHFJR3RHMFNuTDZGbnB1OTdxK0tocm1BUVhLaWY0ZlFFbzFpaFhVZVNEQ1dUTlY5d2M1RmtWM1BUQUI4VzVaMis3S0Faa08vVXU3K3gyakluUW82c25hT1diYjRvaERQNTRZUzBBVjl1MjhJc2RBS0R0QXBRa3RQbXc0S3ZCSGdDU1d6enhvdHhyam9mQ3JsOThzblpMOGwxU3ZCZTg4UzZvVVZuc2tLdk9PQXRtaFozUnlDWTB0R3ljanMvRGZQZUFjQ05OT2ZUUWgrWFQyeHRMRkhrTmlEaEU5eS9JWHBWK0ZMRUh4VzFJWTM2TGJaU2xYU245cmFrb25zd3pIR3ZiRFJHRmtTK2RlUkZyenJHRTd2T1ZkamhmaisybXEvbXZNaWUvRHc2MlVtMXFYZ3pyZzREOXE3TEVIUVNYa1luUHRmQ2hDZ1JkYVlaZHNkZXZkbnc2dG53SGUvN0xJdVN5cWhHYWtxalk3aVdZemFNNllFNjFDSEd0MExGaW9pckM3WEtURmovb3BkTFNUYmUyT2E5WjFWTGMxRUlIbzlZc3NtazVNRnlMbGdEcUhDeHhNc1hjcnBvN205VUVYelRNNkN6WWF6M24rZFAvTjMzMFMxMVozV2dqWEFGOWhnaDYxWFBnSjBxdDNQampHanBOaWFwc2hsYkRCVXV6cWlkc3B5TUNDZTZkeTlIQ0hWK0pUaTUwZmNMU2JTa0dMcEwyQmdqSlFVbWlyNlBaR1VEQ3R6MHF6MHUzZkFJU2RkdzNscERpZGJJMWxvdDkyWjRPYVdadG5HSTRXb0w5UGorUHFXd2RrL3BUY0pwcWxaYzMrUUd2a3VXSVE3dFpTY3c0QWJzbFl6bVNUUjU0enRjWnZJUHJXaEFEMmNxTFJtTVJsb3REZTBialhCTjFKczQwWXNzUURhdmtIVXZjTkVOY21hNXFjeWFrdHlZaWpST096TU1PMmNGcVViUUR5QTgyWms4V29qL0lHTU1wTEptcFRhVkZQQWVhaG10cHMzMkdvNUhHQWFVSWNocnFvY290OXpNNE1RVUxkb1J4U1ErL1dEQ0hrK1Y1VnJmUGpFOEwxNVFObGdzNFZHeEQ1cWNwQ294c2lCN0IzOHJRNWovcFBkNENUMGZIZ1dRbHI0elB4NktYdDA4QWJuVlNzM05OZWkwYjkwcWhuamRxaEFaZzZJNXVHNUZROTNRbUVFSnVyNWhCaS91QzVRSmJITEFUNUU3SmxwMHZiTnh4MDVVMVlLbE5HVFJqbXNZZXZlcGk0c0RGSmFKR0RNUUxLNDhLbmY2dE1DTlBsTFM4VzB3Z29SY1pGSHlzU212NWlBRWd2NWhFREtFMExRZExJTUhXSVpvdHYvYjBBdE83K3VKdTVNVzJpdHlkc2xTNGtvYlpEcThZVE5KSDg3eGpXSk1nQzVOczNvaGhqS08yM2N3NFJ4Y1RvTHRYS00wQjF6ZHVNSmVWRWFXQjFPVEJIL2hVQzNaeWV0TWJ0a1YrK2thNzQwNDJPWmVvYW1xblp6Z25JY1AycEI3UmluWTdCL0QzRXRlVEN4aXZkR3Q1bXppSmhCZnhCMlNTVVA0SGpaay9USUJwNlBaYTlkbm8wQTFyUmJnQk1RRlNpWnhMSFo3Nk1NY0lldEpoUmJCd2hXUE9hOTRKbW4rS1FSZlIrcHM0Z2p5NDh6ZlFVRzNsOU9vUkF1QUFUUTBITGFXTFpLOUNjMVRiWThzeHFlRnFpSk94RHBtWEZwaEdGakRLeGpKUDNDQjR3ZHcyczNsRmJvWUpKMm4zbE5PWEp1MUNEQkdYRVIvUXhOekY4Mmd1ZkcxVlNvTjl2WDM3SWpSUndVRVZDZC9UVmVEVWpQbWhvdTRnZGtpZWNETE93UTYyVVZ2d3hSbVh3ekFIR0hnS05SQVpHZ2UzQ2xXRS8vajM2RzcyRlVTYmpuKzIzbmZCMGIxY1RZY0VieEIxelhUeHVkZmJvS29kTjMwbytjUWFLdVRwWWlWMU1zSDdRYTdWVVo4WS9VTGJvMmM0TkI4VzNWN2VFeThaS0xEbVEwOEtpcEFMSWltZEFpY2dnSG9UWkYrbFAzUUllMTUzZGlxMFVUWEh5MGJGeE8xQ05mMTZObUtwK3M3bDZWWUI2akhMNVN6cHYydlVnRS9LUWxQYmo5ZERLZEIvMGJqdDRxTjJmUmZGaEkwUVkrQ0p0b0x1blgyWDdiQmNXbVRjdUNoL2F1ZENJaVkzNHFYMnlNVWRKTXQzOXVUc21ZS3NxYUlyS0ZlcTgycU56cFRXVlE0V2FQam9YbFB0SkxqVlBJU2FPTHBEckxjdURFdEJ1MmJRRmUzVnNjMTV1aWw0Q1hjUkFVV0tWbEViTnZ0ck5kcUJsdXoyOFlpcy8wMVRuWmx0NEV3MGdNbGZBbmVHMVdEazQ3Z1ZHQ3pSR1ZBOUJVTVJvWVBOVXhJQXlZNVBPdmRSZkdJSXRKZ3dXa2tFYm9Xdkc3bm1TVFhpRnFEcFIwYURQek1CL2I0cUtELzZQcmx3NDhRRGtJd3JxbUhHWEpjU1FMWVRhKzJXNlZyVDBIYjFUTmR6cWZEVTFzbkNuR29uTXc5U25jaUR2UWlsWjh1SzVJQVhMVytuTFRyRGk2dUtxV3ZSZ08wR2RPdkhtT2RKUDdFVU1vd0FRMWtpQTZHRkk4K3JYY2h0M0UvYmFqYXRWZ25Cak8yV3dhR3k2NjlCQmtMTkdYVGpwVGFocTVGQkNnc1ZqR3JNdXV0ejl6ZlBhMnFuWXhYWWZLYThQTXdmc2Fxa2kyTkRUUXBSTEZ0RmZLamdTVmNwZlJqOHJiM3Q4ZjZINmlWWHNZQ0pwQy9hcmp0T0ZxZ01vbU1veUwzdzR6ZFRLUTN2Y2hrY2lDMkM5ZTBpT2hQaElTUG0ycWpmblhyWlByOFk3ZGFldHlvSlhJdDRickhOYkF5ZFNmSWQ5UmRLcnBsYWJvM294QlNGbVR1b3JYQTdlV0NMUExJUEowTnRyMUZqam9aWHJYYm8yRDVkSWw4VHhrTlpvaURLa3cyTjc3WUV0QStnVUYrNVNkRHVjbFFEUEVBUVhYc0N5N2w4UnptZzF4R3R0N0w4R1ltdDA0eFg4SEpCSHBPZUloRlNRSDl3Tk5USnRHY3JOTXM1clNlNlcycjQ4Tm1UNHFsS2twZ1hDUkFoVDRKUzhrNVhldGI5Rkc5NG9zOUUxRVlFZytDZTd1TUM0S2tKVjJPT0dGSFdoMHl6dkVNYitGLysrdEJySExrdUxRcmw0V0o1aG9QZTZPYWJ5ZDQyVHQvaHNaOFc3UzhtajRaNlVXT25ZTGZKRnRaN2ZuY2g0U3VXWHFxaDBsSEpJUG5qaXhGKzRiVWY3cm9ZaVRqT0dFL3hUdWR3cGhKbW44ZktEY1dDSkdxNnFKdVNSOFpCV2ZSd2w1ckllYzVsNXAvbXNZZmNraGRodWdLeWdzS3FxZG53OUdJTGo0NEtGNG1DMzZGdHVrY1lQaTZRN21pR25penE3VXNYVFpiQmV4NXB5dUFVdGkwN0wzU2luTU9EQ0lBQ2NSb2xzT3NPdkNvd2pld1IyaDEzMWl3NVJLeG1Ra3VvMms0djcvS1kyVHJDVTgyNEZ4bE5xSGVONTk1NnBjNzE4dGo2TmFCU0VFbERFSG5Pb0ZtcXFDWGc3Q2pLUm5WV1lWbGI3N2NYTWpWN3JMNENiTTNzbytqS0YwVWRhUEVlMjV1ZGx0M0ZpSzBzR3dFc09uTlR6bXo4SGpFc0xENUtUdmkwdytwZ1JUWituKzBPWlpSeTFCVUo1b1QvV3R6d0RUekRXQVBBKzVOUjJOTkJvVW8rVnJtMWRYT0o2Z09Hak5YZzdMdnRRWGg1dEVGQ1hlR3NXT3pBT1BVUnYwRW9tOU5hTDBVRkhGajdmekNGYkQvZEJjeTlCV2NkaklBbFRnSk1HaHVXbVYrdGRGcFhNdjZNMmpnOGQ2UkZOdTBuL09nK1hOcHEvNFFhQ3FzblViWW9JZWpGSGRvby9jckJpR29oU1JZa0tLL1ZVdEZ5cVVFTElPam9RVUhkRWZvM2RwMXVDQUFneFdkS1F0VG5iSEsyUzBzYVpmZWtObkVnSGlUMDQzeUpoRDhGU2I0dzJ2dCs2Unp1WjJyUGNMK3g0V05oWnNTNG5LRWxCbDA5SXFLc2ZkTW1aS0pCU1lDSGIzZ2loZThueDZqYkFsQkpkTU9pZVdLV2FwUWRXRDBKWU5wclJERW9tOVRhZ0lyV1F6N3lUdkExMFNRWUwxbmVrVjFuOHV1cEVBY2tkT0k5cFF0SjNrWmRSNDlLV1E4S0RNQlRGRERtZUd2Q3p0QzA4OUI0b09BOU1mSEtSeVNsMXN4Ti9ja2Z2TEorNXBEcUdOUUhaWjZuTkdqdVAwTjZnSzhTNDFkcWlTSk1haG5IY2xLVWZDeStlMzNmOWZjWXRYM2t3eTVBM2c2TmVSTmpyZ3pacTFEZmsxVHJrM2FSVWJjbHp4aTJkWkxkaVltRDE5anBqSVJ3K2dZNUVIdk95eGM1S2ZtSkpqM21LR0MwTDExNGVMVXRQT1pTSit4RU1iU1VUNUY1eUZzdXVESTFOeWxwTCtuTHg2bk55Uy92MGNOZlU0dkZlRndrVStRNmN5cWtjZ3o1UG9DT2xwS2Q1TmFkOXkxcXJnTzFNclBXRFJ3TEVtRmwxUndOWHAwM0lHVzU0Vzk3YmFmYitObGp0ZTEwb1kydGhTZ1MxVDRHZGl6RncwMHdMaDd1VUxqQ1FNRmtMTFZUU0pWazFDWko5UlhwcDZ1Qms2UWU2bUFSdGNUREgzWE83b00yQXVOanMwOWQzUDN3MUZWb05LNU9MZDZpYXQ5RnhmVW45TUpLNXNSMDlVTEZGSlNQOW0yK3hmbC8rMzM3T0J6NVhmbEZPclowd0h3NXFMZ2dtR1Y4QzZFYXc5VWx2ZzJIZHV3d3hYRlUxaG1GUUs1Z255MGdSRjhLQ1ppWG83U25Jd0RIN2RZRFU2bTdVbllnTit1YmNSRzZiNUk4YlZBa2lUbEh3MXFsc3p3dkVyUHlqSUtMK3FtdU9jbUl0NVBzZzNzWHdteXhWL3pHNGsrb0x1dTZsWmt6bXpzQWp1QTNIQ0orVmlBUkNzeUdkb21NSGMya3BLSmRxT0lzaU1ONk9SQlcxcnNKdkxtU09qU28xbFVyZ283SWxPL2lxUGpZVzBHWkxOMHIzNTdQVlo4OGdjNVBXcjdoUWpORGE0Nk1TN2lqVldTRTlFUm5DdlQ4M2ZoTEEzdGluK09iRHZuY2w5c0NhbVRtVExRVzlMQjN5TlpZaUEwRDkzbTRKVTdFajRxTXhpV3NDZjNpRmpnL0hubkhtaUxVSXBTazJYdWVmSUNTOVZCYXJoeE9pNDRrbCt0YnNPemtBM1Y4MkVobWlqWWl0bkYvRytNdGdFQXVNQkh5TzROQzJXZHZ5Tk5wcXZuYU5wRmRwaXM1cEk4R2JHbWpHeFVlbC9Gb3lxK053NlBxU3BaYXkrM0lJZU5rZk9POW1xd0U4U3pZb1kwMlU5L1FTclBGUjRianMrb3RCcSsvdjV1VkF1SFRHeXNGYVREeEFqQzZ0aHdLckxWeVRjbzcza3VzWkxBUjFKaEtTTExBV2xZKzJPTG14OWhMbXltckQ4MEhxMXhFcHQ4cVhSMVY5dXBvSmRONUNpL1RNM1Z0cnJaYmVnWjlDUXczY1JHTytXVmdQK0xTMzlpU3ZKNU9BSFAvUEpQNVhnS0RENGxmMjZIblA2L1hYN0JVVStwVVZnQ29QRDR3Ylc2RHpKdUM4S1c1TXNXVmgzVzc5dTJnSUVmRHgzNHFnYkErN0UwcWlqaHNnaDh5MDh2aG9DblM1VjhkSmMreTh2azkyV1pkbnZ3anl3MFVaWXlYQi9Tb3RYbll2QkF2MXpNZkVPYitXUnBTWTBuTUdGcURSK1BiWUkvQTIwaDFUZTVDWDdmMDBVWjI3U0VrTVhOV1FDWit4RzIyNnpubjg1LzU5VUlUTnRGcVdGZXFIMTN1Vk4yUFZLUVZ1RjVIM05KTXEwc1JmMkZTMisrbTl4Zmg3alRkTFBuSEhjdnBUZkZDb1NFaGEySGg1NWxtRWlFUzhIVUhodnEyeTRXY0Z3eTR4SkRhdEdJUkI5SzhzOUZTRFpXbmZ6MTMwajNXM3dRNHNYRjUxRUFKeTNLWC80Um5veGVCMXFPMGo3RlB5TmFFeS8xMVN3V0YzSHIvZFF6bmh1NlJsd29IVHVPbGFkWndSY1ErelZMbk9rS3ZLemFuWFNkNmpoQVBVR3NUNmJ2dnp6N3p1M3Fqb2NNZU9Eb3VOTFJSWnpNM290L21ISmNUcWRFWXhMNzFkYmhpT1BGWGpMY3AxaS94YzVDeTlQdGw2dmFQTVhsblQ1VWRxaXNRc05PZy9UYW8wOEI4dDFsSDBYM285VE42cTYvcWd4d1loelc1RVhTaC9WZW9lcmVmS29PT0FmVUZ1Y0cySlltK0dyc1dRZFJLZVd5dDdRNksyaWM1M3NOejRvWTNJY1RyOURpVmQzQzhxZ21MWHl4Ym5LQS9ZNFVxMU5tM0hiTEtudHlDK1liQVg0aG5GRTdHeVpjQm83cSs2QldaMFRKbjV2NWg3aU9scFFhZ09NSDdxb3RDZEVpWEI1OU5FK2VlSjJ3WTNKc1hkanpJdCtlVTczNEJEMzJLVktOSnJ1ZUZjbjdRVXk5Mi9PWnc0MVN5MEk0TzlPR3oraVU3VDcrWEY3RlpMbWRyZ3BwT0FIZVRReCttNUltU3RnOHZOYm9nTE95ZVpMblFmcUphZUZsU0RxcXFJdGJvLzY1WTh6STFNVHFvY0Q4alF5UVpWRVBoa0M4TnJ2bmhNTGZhTnlYRitNa3A1T1Rzc0pIbDNZTnRKM1JxMXQ2Mk1Fc3lZRERRTFlIR29qeWlWNXA4S1VBTTFKN3F2RGxVQUp4ZE9QemlHdjJ0Mkx5ZTJZdFJHZHdxWXc5akJoRlA3VEJ4YjVER3dKZkNBanh2U1ltbkpDYmJTcjFwZDNNY2Nkd0VVZGM4ekRWV05Va2ZQM0R4cm9NT1FBYkUyQzZpTCtWWXBaUkRud1QybkhYTjNBWHJmbWhTaG5wT3VlR1NnTjI4YktWaCtHeGFidXB5dE51S0l1c0dlZ3VxTEF4UERkUDh2VXIxK0o1YlF0SlVmTHNiWC9XbTYyeGJRNkdNczEvS29HTkNrNjN1eXVUYTdvVWN5OGhYclZzNlVQam8ycUpUVFlMeWZ3c1R4K2JoZ1Q3ZkhJTmEwdEFvYUpUUm52bmgrTDRFSUtmdmpWU0UwUVpxVURBR1A3dEhIRWFwK29tUGNUYlY4SkErNEd3enlEanBhMUpCZ1BYNjB4ck5ZSGFDVEc3bHVFUVFKcmtmOXpvaUEycnN1TkRsbkE1S2lZRHhOdEljeWZTbGszMXpJWHFic2dDL3JRN0tUZStNdTFEM0ZrTCtHYzRZNEl0TXhDbWVhZ0duYUtHTzVNOFpIaFhtdTAzNVdGTVpNQzBIbmY5clltVkdHK1ErTWxDWGVhYlBLL01wcFVnVTFNUTFTYkEvUHpPMFRFblNFS201UHdMZ0JFZlhsMlROQ1BNQWxEY0ZFTFpwVnJvQjBTQ0d5SDdrNHFjVlBxSXJlT3ZvR2Jpa3NER1B1SHRUZlJtckxRWWEyTklCV1NzY0l4Sm1KcUdRUkIzRm1lb2RUT0ptb1hObnJ5OWpIQmFQN0RWZXZBNElLUEkxWnArRlBaRFEyYUhtT2RBSEk4ZkpGcThWLzlrM2REZzl1cnpWaFIxcFUvaXFtdUJhZG1melljVGc3NjR0R0R4eStHNnRKeGhiQVdLeUhyOWxvL0tRdUhRU3JVRm52RUpmcUlmZlJBeTAwaVppVTNiVWFYdVB6cFNOcGdsSEtvY20rK3ltM1JwV0tjajlpRHpTMXNIL3ZPamMzWHBYVHlHRjRaeHljbW1EUW1Kc2JUT3YyTVpPREpUNGxzR1NUR0hDNUp4ZHorTFhXekpwYlViSldZbko2N0JBSkhTR0FIdWh2MzI4NzhOMlFJL3IydzlBUUJiRitNM1RuSEUraThRK05RZFdSY2E0dWRKeTZaWHhBSEpXV2wxVkNiWUhmNEdFemdFbEJjcDdvOUlQcnBRU2p5NlllbUtNaUpkMnQ2aWtLTG1VbEo1YTZBYmtYTWJZb1ZDTEZ5MVp2cEtoc0VORzlOOWxXTjJGeTJ3UXYvcjJ3WC9UZFVSR3cwa0dKenJVclo0ZFNvZnByQysxWnplOHpJb3lwRm9vcHZYRktxdWg1R0VCMEJRdW5tTHZ4K2M0MzhHeUdjMk1hN2NLbnZFOTRJMHRsemZaQWVrREJVMDBsUjNBaVdxQnk4ay81MjhCQTNqbGFoMW9zL1lSVktTOVZqd1Q0Z2xNdXpEUE9JVmZ3S1FlUmRuRFQ3RS9BYW1kS2NZZmk3ZUZSRDRVRTYxdWl2aFh1NHZZUEhkUVl5dllyV0pMYjBXUlR4bUgwSks5cXk3eFZqY1dyalJYNUVHaEVTZTlNcVJYNVRnek9QcVNaQXlNazNCTWJwZGZsWUh3Wng4bE84d01uYUVadVRMdCtUdk56Y2RXS3NEblc4UlVnZ2t6VzVMMmFqOW9SNGNqZmpSRE5oRFhIeFVJV2xiRE9zNXRkNDRIdldmTUFNckEzaG4vQy9XbytjSGdQcUFuUzV6cWJsRVdZNk0xVFdNa09WYWsraC9sbnZ1UHBqQjQxcHo0N3NCYlQvTlRVazl0QjV1bFpNcGdTVTZrUnc5Yk84MUdaeDYwSi8rU2lnSGI1UjV2Ujg1c0N2alUyTWFIdUhldGJJMUg5eXBEZzZDUlFITjF2Um1DSkRiQ3Z5NFRReU8yN3hzRzFURUxNNE81b2NWOUh0eWlLT0ZMa05TQWMxUlNRNUdITUlxR0x0VS9EcDNFSlAxSW1mdWF0RE8xS3prbnRvSXNpYWlmRStpeFF5RmZ6MTQzc2s2TW9weWwyT0JldUJZOUtzTjNEekhyUTJoc1o5UFpvUGRGMjdvMWg3Y2VyTTJiRUZUMjlJWFpyNXNOd00rSzUwVzJGM1Vucmo0NCtaam5BcE93am80NmZaY3k3Q1Q1cC9nUVU3NkVJbm1ORlBnQ251SE9DVmp4Rit3UFJGeHlxQ2RkSnpFOGNyTjRwQjliNFFRMkVCKzJmQi9YRmsweG9mS2JBMTVEOWxjZXllZlJFZVFDTEV3RG42b1paSHIyTU92ejRIbHVMVmVNb0NYUmNpVG5FeCtoTnpwalhIZ2Exem5FRHNUWTFHRzlYYjhRNUloZnBYWDRkdEVBVzFWWFVRZ1lBWVI2QUx4aUVsZXZvZ05hOVFiSHUxS0ZWdEFJZGdmZUNjODhmek5pNGJJN1JIcmhybnQzNVNVRVJOOStqTllzY0JLZEhrTEZDemVuSDNncldXeitUYzYvZ0pMQWxwNmlDOWt4VGcvS083bjFITEQyS3ZGL2VML292cm9sUEgxR2c0aE9ucnhZc0VMV2JYa3RoNXZUempCcnFROSthMmJsSFVKdVl4RmNJdGpnN2JDQXZJTzgwYlFuR3Irbk9SU0J6NzNybTFyZm1DTzViRE9ibUF4NnR5bW9ya2RMcFNZQjRKNWFWYjdsWElDM3ZndHcwUWNMZzhzVTJ3YVhIZ0wrSDFOOWVsSllJWTVUTmpPT3lCZlBGdlRTVnRpV2RqUlZ5eE9EMWJVOXZLSXU4M2NSRlhGM096UlExVmM5bXpOWjlpSG9ybWpndHVEUnFGdFdjRWFWSG9tUGJXeGoyMnZ5OEhNR1dVWHZ6dVVQbFovNHVxR21TNkhFdU1ZZmRNdmdBekp4elZEYitPQkg2UlkxNTRZbWk4Z0MrL2lVYzdNb2hITGlxbWRWWDgwZHBKbytSOWdWaGt1cDUwOXRLYWJNZ3BhZnd2RUdpL293cnRGWXpDdHliK052UVB6em5WWDhRM0F3bStqNU1wZ0dUaEFaUnlsUlVXVVRxYnVscS9uanp0Sk1xSVRHWkJCSmJkLzF5S1QyZzNRd1R6TkdpclBDd2xDTFFWU2s3N2dkM1ZSUWxEZzVJMHZkM21meTNUNE1ZZVY4YUJPYkE4TXJNSlVNUHpXaDhCSUxyQkZBUk5rSkdSTUc1STVBY2hkcENLdzhiN2hBei9tREtEa1VwZDF1MStCaE44T3hJYW1aU1lzWGFxeW95QS8rRDdkeWNKa01lRDV2U3ZzNEpMeC9teWRFVlRMcnJ6WCtXYjRyTmgydDg2cUNMeld2cnpyVFU3Vnp5MCs3b0E0bWJLRFVDVnllMVpzQ0lTeUJaZktDTTBITVQydnh5UFQxZ1lwVXJ6c2NVdmtHbkMrRU5ZVjl2V1F5NXNKL09aYkI4R2Znd3EzMXorZmJtYkN5YXVERnhGYmcweGw1OWlpaDVhQ0g4eWh3VHpZdjlRNnZuVWxKZTNXcTV6WDRuczNqMklDdjV1aTZhY2ZWN0FIemEwUzg1ak0wRzNpdzJUcFlXdnRNdkFoVjUxNFpteU5xL24yTnlHWk9LWkkwLzlFeXRUNEt3aWt5U0FGbTlMR3FzR2d4UHFMeHNJR0J0K0s2K3BlOGZwZENZT1hteXBGNEMvcWZRQVloVUwxQmVOZzRvN2ZaWkx6WlZIZUtUYlUvVHdGUUJvb2hPVGRoYzlQTmlKNFJWYTVHc2FRazlON05NUmNXbHJzcVNOT214YzF1VWh2S0VUR3Z5TXJlQmowUXN6UXRNdGp4VTF2UmtwUDNBcHVnUkdVdGM0cHhHc09RK3FUOTJKTk16QjExNWZRc202MG1BeUxwc1NSV2FrNWlxTFBFbmhvdkNWTkpQTkN1SmZnb3NCOGloNzV1THMzR2k1VXUra2NacEVtNHpRaDBXWkx2RHlzZTN5WFlyVndvckFOL2lHeUZKbUhMMG9oSHlJTVIrU3NSWWdWYVJLQlNERElUWUUvbG5acENNWWhnVTRYQzRvVkJPYVJIWlhncFpWVDJkV3J0WjRscEJLdEdKKzhXTUs5R05BUjlPdU5uUWlKU2dsUVA1aXA0bTNPNE5wcWZSYzRrWHlPaTdyblorWU5UTGQ5WStLd1B2UERFSjd3Rklmc2JZWS9PR24xNUJaaVUvRW1LdXV0YkhkN2E5YlBMMUlKSUNXQUM4VVI5RGxpY2k5WWpqQjcvN0NvME1ETEtqTlh4WkFCSTB3QzFOZ2t6SDRhU3VsYmlFZjZqaml6eUJVbHc3eXBHcVhHdzB0aFBTWjBhZWJhY1FrVDN5ZHp4SFcvN1dTczUwV1pscFFTbjh3cU5uY3l2aDB0MzRpb0hDZUNhMnBuSGNuVXlldTU1TFo5aWVnNkxHaUY0OUxvT0R5SGZUblErSFJ3SENNYzlnWVpUc2E2U010dEw4SHlSYVQzMnhWZy90SGlvWVNtODZTK2d4MDNkcFBLeXdEY3Y4dVpuQWpWNHB0TGR6VW1OUXlZVVJrRWdJK2pua1AwY2xDL3pXZHRTVHcyZ0ZGRm1PdS9iTEdqTkFGKzYvRVAxS1FTK2pzNFMwNGZsZGFKWHNRRUQ2YnV3TThEa0dXRlAza3dPNmc5L0hOKzNtbmI2T29oaSs1NWdGQk9TbGQ4U3ExOEFwUHlWeElDbTBaTVQxcHNuK1JFWVZYUlpIcmFGb2NTTEVxdlpUeEpaM05iR0YxYUd1d0ZmTmNTTUxiZTlpM0ZXeTBERDMzMWFHZlJYaDRFMFVuRlIwMTZTSlVOSVIzbXJDcytuTWJnVmZKRFVTU1BSSnBjOWlGWUlqTW83eDBWZXZUN3FrYThBSlVHZFVZOG0rUEpNRHp0TEUzSWJaQWhad2RuZVdOb0EwOEw4Mnp4QkUwWno3K3lMeG1YUy9FajJ0RHB0RHJUbytDaUxIM3BiemRCZXNtV0EwYWRiYnpnUzlNVUdXUElkZWF4NHp1SGxOZ3Z6THI0aEFsd3VOUGpvVUFnNEpCbFA3dE9IUWhmbWZRU2RNZkc3OC9NWXIxNW1ENU1uaCszRkliNm5hWDVrMVZxNEdKd09oRnRidnFoMnBzYUVYYU81WElZck9kRVBBSUpUVWVrc3ZDYTIvSjJKVk9sc3V0OEN0SDZKTDg1djZCZ25OTEhLODRzbjMrbkgrVVFBL0RTdExRTmFKKzlsS1ptdGxRcnBxZENYM1RYbEV5QzRud1pHMXI5cFQrdTY4STlVVXdvV25wdHY0Ky9LcEtYRTEyOTVsUDZaajRHcnFoei84LzR6cVNkSmFPUnZJeFRSaFdRdHFSOE8ydUhvRUZwaytEMitLcS8zaldNcUlHNkhhQjNCdytKQ3AydFdLN2RETThXQlVrRXhDY2JkQXFjZWFUV1BoYUxta1ZlMkNwdkF1VFI5R3lOSVlGbk1SU2ZCK0hJMHA2d0hTY1ovWWdkMzgzY3U3cXpxKzJ2Y0xFbzNvbWI5SUlrNnlqMUlmNnBSUUtwOFE2NGxGVWdjbG9hU0J3YlYyMW03c0N0clBJSjg0V2VqSTN4ZHFEREVnR1B0WElOU1lSL20xcGsySDRHWDVJVUhnRUZBUE9STUIwanVoeEoyY0FKTHNxRE5qdmk1WUYxUjVkVnE2d2dpMjZaS0dOQnJTMzRSNW55SDFrbGtLUGJlZE5icjRrenI4alhmcUxKLzk2YTNyVW9WR1Y5dzIyb093UWVNUjREbVdhZTVCZjhEb1FGcFNXbWdzZVZYY1U4MktEeDRERCtWT1BhQlQrUW1NbEZMYzloU0gyaE1YelFOUXM5d3BBOVRldTRTTGpvNmN4dElwL1VRNUw5ZEhQL3poNkduUWxFRDRrWWlKbXdPdml5dFc5MUQxM2phbGFPNWw4ekViQ0lOc0JoVVdXanBjMEFaU0c0VUNyVmtSNUFzSEVNUlRob1hXZ21OZ1FPN2ZHS1puSDJLYXljQVFzNENScUk0eFExWGtPa2JWUm5Lc3plVkZyK3IwM3VuRjF0TDRTY240M0xrOExUOUsvdnNvNnFyaVRjb1JvdkhGU3A3Sm1GYmtVdnZxNHduUGsrQVdKMTFwZDNxRUEzcngvWHV4QXFwWUtVY0JnbXc0aTBSREo0QzdEWUJZaTNlVjEvWHpUMkN5dnNxOUpQdyt1eHF0RHZxUHV5ZXloQjEyNlVCZXowSFk5c3Rvd2JvVkNNeWlURDRFNHBQcmhwUEQ0VlpnbWM2TFE3dDVkeHdzTG1Rbk8wKzJxb1NrK3YvQkpBaUJZZWVpMHliSXRObnhrcjFnRWZLTUV0ZmRHQlZidXBwbmUrTjFoNGM5T2xkLytCVmUvb0ZTSkpwSFJZS09FVy9WREN4V2wrRjhmVUtEZFQyM1hTejRnMzJHNVR3c3hSck84enVuZXRVUHNSMnl6R3U3UGZsZHE5Z242U1JhUVMzK2NLck16b3B0a0I4NEIwb25lbEJuWWgva2FuSWhrWHVUSVA2VUlqVjZqdWZudlZFQU43eCszOUJGUXl1ckFtTFVBQXRmL1JmbUltbDRGdldaRFB0WmNGbENodVh3blVjT1U1QUFycjNRR1BVMW1lZ1ZkQjJZaUlNVGcyMFlOWE1yWitKdU84TGptdFI2RzNsODhPRGgvR0VBRDI5MitYRHEyYkpWM3k2UjVQYTdmWlV6TDRITTFuVW9zeW5COEluakdmTGJJTlR2YlNieTc0RjN0YUc3V2dGWExXU0lrSUpkZ3M3YjNBeEpMM1cwM3d5bGdKQkV2R2Y5N01paXB6bFVRL1k1OHhsQ0ZDampWVTUzNlhlWXpjc0QxVlNwN280bFZ0MUhVY1ZFby83bmc2eDV3UTFIUHhUU2cyNXZqbTR2T01qSHRkbmJQQ0FpYisvVGpiZjlrWVZFY2FTOHJSajQ2NWVZNXpmdzY5dGlDeXBvU1dhR3NpWXJJM2hDZXZ2N2VQcFBFNWVJL1lRSUJKcTREdG40Z0ZQQUhtYzkxUkN5bkZqWUxoZ0FyajZYQ2cyWEFLUDNQa1VRSEIzNVZuNDFjL2pwV1R2Wnp1bEhRUmpJQzZpU203RFZ0Witwa0dTb0JXR1JSK0gvYUhNWGozREY5a2NOSHFUQWZXMytIUmd2ZFNqdkZNWmMrZEZtbys4YzVITDFMU2JHZHhxYXpiWVV3cEpWVGJETXlJSFl6ME1SOCs4ZWdkcy92TTh0R0ZtNUVmU0FPSnd2TzNRSE5NZlh6NlJpTDBLY2tyQU9oS0REQlpSdm9hdFBpLytLT2tiY2xoVDBrcE5GaUR6bys1Yi9RdWlwZHNyRW0yeW9wRGdLaXhSOXlQTjBlYzJTZlpFNVRsYktGRkwyUXVaSE1RWE9FWjVUWWNTdDhvQW5NVGsrMzRXS1VpVFNsb3NsUDN4c1ZnZExRU2ZncEM4RXV0dkFGMmwrbDdqb0ZsQUtBc2xvcFBpdjhmbG00RllPRTlJN1FzK0xSOURMQzRlWngyQVBGVUk2S1RGOXlESnBqeC82ZUtFaklEeG1DZnI2M1JEV1djMzN4ZytDN09CVEE2UUgxNDczdFN2cHVVZ0FVOHdQUlU2RmdydWhVYjJidmtrbk5jTEppVlhtd3BlY3dOQ05uRlZieVJRMnNEdzMvTGxEaWdicW0yd25KRFNJMVo3SW04N3JxM01ESFcvL2gwR0YvOEw4TFhKWEZoY2VDQkJmYVBhNDFycVlwWDBURWgvVVIzbVhpQnZ3MmE4WWlpYVo2UDk3YWowck8yQ25LQ0VrekYvMFQvWDNJLzJidnlPZWF0ZEgzMlpyNGVhSkhLakpEdWk4RXdUQjZWb3dLcjAzSFdkMFh6dGl1YW5JOVFuM0EyV29TTzZIUkhQZHFyd09QU0ZyeUQyYW5lMnJqZDBlMWFjNDJqUy80UmI2OXZOYmJvM2xQcmx5dTBvTHRKMlNNTFpsb3VSdU1sMGhqYWZIS3k2TGxmTFlJQ1NTMk5KODNGMVpGRTBlZTVZYnB5ZzltR1d6a1pKQXllV21wQ2ZwRGIwK2c3THZlRzdWWWxqRDZYbXpOUExHbXpDcitmQ2FrSWZWMFRwVVR4NGZUMjNZR2lXMXVndDlWNXY1bTgzRWpmU2VudThaMTJ3aDhLN0tTZEFVL1N3T1NxdGlXMEhFQllzVXNpU092eWpZTFJLUXoxZjIza2ZRUytzaTlnM01tWmpHZHhlZG5memxDOHd4TkhNQkR2cGlFWWVwcStqczJ3Tm5mUzlPOGZaK3NMVTZwZDN0QUNZQkovNCt1ampKSG9ldHdVYks2MUJTQWVJWkdxSkU3aEpKNDBXQVhlV3NsbWlnU1B6bVh6MjlPckVGcFBSYSt5QmYxdWNHQnhad3h3UThxZnhya29teWtXL216dFF5NGdnVXNpcXNhVXhNV0Y2SHJEVC9sdFVzMlFCQ05TMU1GVWRIWFZ6Mys3aFpVUHJScUNveFNUc0V3SXREM2FuZGpUNiswc1AvVFlHNHdXZEQvd0hBejd1TXhRUnh4emw2ZEMyTlJnSkc1SWdsbEhuMVpkWWt4YjEvcG5rRkgyTDluUWJ0RUlzVkZBUGQrNEdKNkZqUnd1aVhjcWFmL1BvWXFqdlZYcjQwRkZVTFdCeWQ4bDhxeisxenNvVUNHUmJwcTQ3ODlrM2JPZ3U2Z3FLYzFZQ3dDSm02VEVFMDhpZXRYM2xlT3JFNGh6cGxWaVowblZFa014OTM5VXh4ZzlBMnd2b3dUYkFCRHBuTlFvakdRdHd4ek1Xb0Y4cXZEU0hteDd5akFzNmxQZWd4UEhZTmNlNms5aGx2VVA4WWM0QW1zbDEvNTFuNjdVZitReUZKNG1iK2xrV3VUYTd2cnJTcHRmR2ZQMWMrTUM4a3VzOGhkSVpLWkJnQjRHNnRjWXd0Tkg1a2FhZ2s0TW8zcGhQL0srWmFRREhtQk15SzR1TDgyY2RabytwYWxuRUlUMW0xb1k4T2hyclNFU1ZkMmtiVDhPT1E4QktDdnJ0TUk5eEJ1NjN6YjM2QStwUFFxVU4vMXM5dE12c3BFaDVDSGxZVE9PcC9DQVo1eWV3elBjUUoyT1BmWC9EQ1BFTTBrZjBYRVVFYWdQSzdNL3BwaXhIOVJYYVNwS0MzUFNaSm9iZFhhUlJ3VENwVTJYSnd2YmM0cU56YkhXelF6TE4vbnlDK2Vuc1lqOCs3Ti81VW5hZXRhMWYzMU1LTVBHQ0cwYUZESWFzMVpZQmVMRkErVkxXRHZUUzdLZFZVM2Z1NExBaFJvdk9mY3lscEg4emYzTnFGdHNHNzVBRlo5SGk1Mi8wcXQ1TVNZbEczV0wzRXJOWXdzTXJZckdWTVRtVnkzTGJRbk5iODZtVmxKc3Z4a3IrVTRXYUN2QkZ6R1FvamdxSnc4V09VcDYzL3dVbmQ3ODFaVVpQY0RLQnB5OC9ob3hPbm5acWM5c1FST0lWK0FlSUg3WmJvWlcyM3o3RGJrMithdHY1dE50a0YxOXJoQ1NqVXU4bHpGckV4WHZ0Z24rTWxxUnFOZXZ3Q04yTjZiYmdmYUNrVisyMU9pYXg0RE5GeGQxQzd3TkpYUlBwSUh3YjByZHI3WnNtY21JYVdla3ZlRk41U2FkOXZPOEtCU0E4UVRBYXZyOXhoOHRiSk8ybEVwdlo2cUM4eG9lbGdTQnc1YnlvRjNIZ0ZodWk1OVFyS29mYnVBVHA3YS9FWUdZYjlUQ2c3OTF3ZndlVXQvWk5NTHozY0kzNXlhaDlsbDA5YTd0Ulo1NSs1L0cwUXI4aTFORUhud0VxSEVWRjZYQmJ4bEZFN3o0aENwcEtwdDQ5Z1BFR3BTVFpXazVYOWN2QjE3ZUswaTJmSnMrYTdja2JNdnFNSG1xUXN5dnVrc2lTSVpGY01nNHVHNFNFbEdQcjNrdVZjZ1FlSHE5ZHpxSnZweDcwd3lLdTRVaGpjQ3lNeDBVV3RPVnRrQjRwRmhnc08wejZoZWlXTDJxUGVDVHFMYURpRW9LZWdGcGdMbUFTN281THNqRi9BWHViNUpNNm9KMGdtOTdhdjM4MDd6YjZPbm9OOGxRelNwTlpTVExCZXVvVXVDT1BYRjlIbU1mNytDVFhZdjlDOS82NlF4WTVVd0lIdEJTOW5hS2tOcjVkazFVNFV3RVVtbWVNTUl3ejZ1VXptcmdvRkhkMit1SFN4bkdEcjI5OW1EMlkwNmowYkRQeEN3NFNTaXZCSHpTYkkxRGFIMFZzb3NZTk0vOFV6eDNNZXpyWktXWDdNQ0FkQkhFNW1Wb0hIUlFIb2RVbldaV1p5UllCZGd2cFB2Q2pjbDQxNFRiWFJobDJCeUdaa1U3cmhOTGxHNmxtMzJ2OXVjVjI2TjhZOWtVZFhZQlNmZmg0RlZ2bUtNMnk5akt1MGE3N1pwdld2UzluUG8rT1RTbjdsVmJQVUNsdm9pbklHTnpDa1NkZmt5V0o1ZDd6NStuWjYrbk15RS8wWEdUUHh5dzFPMDZLRk1PVmJxbzFpaGdsYVpNS1RkVk1RYXRnU1JjbTBwcXBWK010dmxMYUpUMmdFUXRvRnVlZXVDTmVXMzNLNnBpUko2dUk1djVWTEtUZUlkQ0lxdkZpQlR5Y3VOdlR6WjhRSUsya1RaUmZQSmZFaEhwV3UyNzNVOUJKaGR0elVLTW5UWk5yVm9ITUZ6RmczOGl2dG9TQkhMYWxiVzBiVkVWdEFQOWVkUTNGWk9VQVdOYVFGeCt3N3UxamJZVW14QVZoSkRtTUFpVm1oOGpLbXg2ZHE5NVdNZVhwNE5EaUxJU0xGZTg2VmFpRzhZb3Nsc2MvS1MvRFdWVVpteGVMNEk5U0Q4R2JpWktGVndYdlN4SFdFb1Z0UXJrM1IxOTVWdytod1RsbVNvQk80Y2h4M0RIV2ZmOWhDRVhLK2ZKL004eVpWcklCWjFpMEc5VHpwRFZrUXdjUEtHS1ZkdWNrdTlIMWZRS1IwTURNWDdjTmxFSSszaXpjMmxWbERPTTA1R1ozTkNaVnZNQW9GanBEWkk4d04rTVFrN2lMWTRXWXhnK2d0d0p3UDBpaFowQ1JzSnJmUjRPQTZ5TGI3TWc2MzR2VU9UdVBVbzFJS25odXN3ZGNKekxkeFNFL01RS0Z1WC9qb3FTa1FNTCtmckljZEZpRDBmQzdURFZ1WVlualZESWp6WjBoQWJ4aVVIcnBYTmJaNUxxbWxEQW03WEJRc1llcUtkNUtOWEhPMCtuWjNsVEFiazZ4QVJBTis0RnNHSGkrVlM2TktNRXR4LzBOWEFpcUwwUUFxZzVPNkVGQzVNc1ZoQTFJL0hUS2ZNVHRDTThha0t5NkU4RWhxekdyTmg0ZElkVTJOOFVCUUhXWGxWQlVOSDFIU2E5bERQY0ZwYjNhUTBnSy9ybHlqUFQ3bGFlcTlDYnVPSWhuWU5EOEp4MzdTaVB0WVZtMjREblhsNHRPRlB2Uk0xOWFhNmd0RXA0eWNyNmdnbnlYb2VRWHcvN1NlcjdRUytucEg0T28rOVc5b2lObDNLK2RtRnRETEhrMjcyVEw4RnBWRElBS2J6dWdXTWNWY3RDOFNKYU1BMkRJZ2ZCOUI2VENlbUhDcWFRT1lWZEpmU0VsQkZXUkFQa3ltZmVkWG5qaEF3L2I4WnZDeWJkV3ArNm9acHlPRXBnbGp3aUo5TE1FWStrSVYxREVQNkx5WWx6WkRDa2V6b2JxN0d6T0FnM2oxZHdISmd3eXpIRHpJb2VHVUg2SFNHNWZOaWZRVnQ5NkFJU1BpTWJ4aHozaFpPd3BnaGVsMlRZeVBTR1ZQMnpRWVFpdldjb0M5cnNqZUNVVkUyR0FOb05TZ0V2c0dvY2NSSkNheFM4VFdnQUU5WlRwWHY3ZmFZNzVQNVBEa0ZxUm9UWERXZkV6bEpxYjYzaW14K2hBSGtNWVhrckFQWVNTcytrRGNiSm1MNnMrUk41SmJueHZKNmN5MHphaG5WcVo0anJ0b2hPbExFNkNnNUFnbHZTNjlsVVRWSlpGNmlLcjF2dGdDZjBDMUkvSkw3RHdzRmQ3L1ZvUU4wdWpxc2c1a0FDemxRdnNMeEgwTTZRTldaRUo2WGJiMW80QnQ0K0V5OTNTcCtEclpxOCtVYmN1blJLMEpIT0QyYjZDaFh0aTFSYkpndmtFbDYxaEtEbnc5TnVFb0N1elFzU0V3UERrWkQ0ckRYcUJhc3BnbEFiV0x1dkd5eE5kWWIwMXd0dTJ6ckE2SHJ4WmxOc3phcGlLMEtOMnZkcVJ5dnlaWUFydG9laDhxd0lycXdHOVUxL2JhekkwMWlMb05Lbjh1eHM1Q0Y4Z05mTUtqWVpnZ08zVlNhM21zRGlDWVpYSjlSWUJic0xOTG5MTnlGMXZVZTBUTzlkOGhUaTJJWElIbFNzQmxPa1g2alhDVHo3TEtrZ2crNElTWjRUay9LbEFEN1ZMS1BTTmNWNlpvdGxjRlhqejNXNDM2Q055bW1aUjhRYm00V3hxOUk5QkhJcy9QaEo5THphbGhvRnB6MHoyMlpCTkhsUE4xcmRqdU5NM1pvYytuRGNYaXBKUnRBbFFIeE1TN1FQaXNrWDdXQVJTSlQ5a3lTekFOd2czU01ka0JzTTU4bDlIZGxyWmFiV21EUjRRWUhNMDV6ckM3NWlPb3FzZ3pyajc2SUpOdzA4Q1NlcWpJaDZKRDFNTThHcnMyemE1ZVZDNlJnN0tmV0tYUWxYd05lSlh6anVXdlI5MkNpNlJtL3F1b3cyMFc3OE5sUkZYVjBLVkxaOHYxYnhHT1gyR2Rpd0NjMzNTaU9SUDc1ZmFJNHNGOXdQWElYRFM0WlVCbEdhSjJFakFkVzZRVlYza0VpTEZLNVFITnZmTzhleXFhVjlNb3F5S3g4b0JPWU00NHlGeFZlUzNsczl1TmF1dUw5NWdRSGF6Ym4vNmF1UmJyMVBrYzQxeW9nYmJSdHB4eitSd3ZMZHpJdDZ6djVrR0svOUNFL3FCaldKMXdUSGxUWkpNWWRIMm54R0N5Rk9iU1U0RU4wNzZkS1ZWY1BRaE96d2xqVjdkYjBEY0FxM3BtaEs1Vkc0RG12WDg4aDBTbzBGMWlsNzRGZkR0YzNqQnVTNXl5MHZpWEI4UThSRXZ1RkFFNGgwdC8xRHpEMkZmWHllYVh6dUVwVmVsR2w1TXJTVnBlVXlydFlDbzV2Rm1CZmNyWDNydXZtdVZRY2Rja2lPcW5TRFhvUmloUHpVaC9FODNBRkJtemRUTlo0ZnNBRWRMVHVpN2JRL3pXNGFNUWV5am9zTnJ5UlNWWXRWOFR4RDBCdndjRFR6bFBZRnBWQzgyNGtoNEMwT2I1TGZQMjkwc2w5OUw1RDdIc1RTendFeWZ3VzNKMjg5aDV5K0pLUkJJcDBmMUd3YWx0Z09leFIvZllhanE5M2tpcFVsMG5rME1PTWhLMjlXV0VCSjQyMmNKVTVJSk1QTzJSZlN3cXNzU1ZBMG9NczVIQzhDVUZVTlMxSFQ2Z1Y2T0kvSGJWMUZwVkhUZ0NXY0lPUmVHajhjVWR2N05ua0xHQWVFS0FaNTZvMUE0cmU3VU9vTjBZQmVKUElhWU82NmVVRUQ5a0Z0a0hzM1RCemlITldhdy9CaVV3VmhDMGxDMldyOUlodmtGOG93bGFpWGl2Y3JiQlppdjdRZUdlamZTa25hUlBMK1BaR3RIMjg5cjJnVHJxQm5LMndnYW44ZG94cE5kWHNoaGk3OHR0MlRXVEZtVzNzNWZmRm93NDIyVXdFZndWUWZzQlV2bVAyaUp4bmNVQTAzZVB3Z3ZtUkNNaG1hMFZ2N1grYlB1VEE5NVBCcUoyTGRQWEV4ZURBQkhrVTZIUEY5RC9BV3Z1bVFpQkw4bWN1VG1JbUFjU3hUU0xxbk9EN1B3ejdKL2FVcVFtQ0kyL0s0cXpLQjRLUmpHbUwxZEpHVms2Um9BYWEwK2lMc2s2WU9ySWZrNnM4YUVmcEI2TWlBU2VKN2ozL0xiNmFiUDNIWmFvcUJTUzhKejlkTDBOYXF4UmhYZ21yV2lqSzhrY204UERmYXFkOVdxRWJIKzRwVDJJRGlJVC8vUkY4aVRZRDZQZ0VjeUVhUHBWQzQ4SlpNWVZtK0wrRnhReDQ0T3NHd1piT3ptcjFoRzlEK2ZFM3pUV1lLa2poZW9QdlNzeWF3Q1VQNGs3LzRMK1ltQzhIODh6NnhQTmY0L3hNTjNvYmdRVVZVS1gvQmJmWFI5aDdHZ3Q5MmFVaCtYS3pXZjBHUWd3cnFPRE91ZUhvK2p5L0FsMmlTdlQyaU1xd3hBNllIektTeUNBc0w4endFZjBZZUx0eTBXRUV1ajBFYnJXMTZhOStKTnZJSTZKTmxaU3l6Y1ZNRWZVa0NjSWpXaElRUWxzOVVQTzFOMHpMc2VlMUNrODRsNmg0aW8vVDU4SExvcXVHUDNrUk9SQW12SHdjTlRHYXc1TTZLd0wrcmdLMlZXSm9GVjNLWnEyaWVnNnZsL3cveTB5OGk2aTRnUDdyNi9uSlI2RjVQTERSM1J3NUs1SThwb2pLVUYzdEFnQm1jYjlzaDFDTm9LWTRUUjQ3UE9VaEZXbHBNbk90V1VGOThNTVBocnpKYTVuTTg2dTNYcXhpVE5FZW9EV2grd1BCQndWZlFVWm5qTE5yS0dmY2RrVDBqUzZJak42U3R6YTFiaE1PTVlTVFMzNnBUcXNXKzdMSVBUak5BSzhYOFVScDZKeVhLU1FDcFVXQmRNM2ZvcDNFd3NxdERxWjk4Z2VPY1RpTHNFQmo5VEk4eDBmQ3pyN0FZeXI3bW5hUnZveHVkS3hQWk1BMzN3eUxaTkxSb01qY0VuOGkrV3pwNTE4RCsvWVA1b1JxTittTGYxdFd1aVdIa3pac0dRbTM4Ky8yalFhaVhlN1BtR24yRmlReFE1TWRlbGwzZzNOM1dnSXRIakxvaGVaM3pld2hjcFNXL1lQbVNvZ1poeHRhWU92cTVPTG1tcVkrNmJQelQyd1pHTFF3eG0vUDRHaFlwWFBhMEVqeDhWYlR2b0JnYVFRL2tqZ0ZpcjhzazlaWGdtWS9sN1BRV3ZIN1BKcjNPVTJIWDVkSXNtbEZOSmlacGpicVovM2tTbHFpSTBUbGxoRko2M041eTMzc2s0KzhpazFyYlRDanZUNWdHZS9GMkdlbjFxdk83ZURzS0I5K3U3cWdRWjZ3Zm1yVFpHemxQcWVSMHRYNGoxUDJRakpjRHdlSXU3N3FJZ0Q0WFhoY3ZIZzM3Rlc3MEZuWEdsaXJDZkNUaytrYXZUclBuRW9wZFRDRzNENUpheUVJODVqYWFXbEdSdjI1a1o5RXJaM3JHRGpsYmt5ekZxNjBBWVNlbUpjbFpFeXY2SUdROEIzZDdqYVVGbnFPdkxnMk9YNURBWGtJbXl2aUxiL2VBUGJJVHhwd2lUNDl5ZUNPL3hXNi9VNDFxUmNrRGZLbWpnWmw5dCtwcllUQWtTV0V5WmdVSEpNU2JWanNLOS9OR0wyS1BNTk1rM2pIcmEwU0t6YUNlSWN1SkJZYS8ranBEVlBJa01YckVjT253WTBIZXlzYkRGd2YvUFpKbTU2S0N1WTFxeGkxUU1TM05DTDhORExYTlZxUTdrZVhEL2JpWnBqd0Q0bXpaM21saHNvNmxsRjBLaVBJYmlhSnZMeTRNMU5PWGFscjlwd1RNampkZDdZb3ZMQ2tONG1ERnY5d1BySkxhTytiUTBFOERldnNWMVFyaDdYZVgyTXNhTTRlWmRCcm4xdENnL2piQVFLUzhLYnVURnhRbllWaE1zeDRHZWZ1YkVmeDUyQTBzMVYzaTNrMTR0QmNrenNLNkhqVkg1MXZzMUtpb3AxYWtSUDdSNW5ycFJicU9ZZU1PWW9INjhONll5aTByakh0Vm1VcnpvWk9pbnhPMFFRU2RPMW56RGtpaEZ5REE3am1KQzNaZERLMkNoZGQxMUtNQkZ0VFhhSTR4UzJWM3hFc1NjMThvZll2V1hiZW5yZXNOMlYzN1hTSXhtaUdnSWZ3SjRSWjJNRm1pcEdRbHhocTBwNmozdjV6eFRWM0VnamxYQ1FvekdUOEdscHhveFFIQUhYK3NiT093emtRYXVSY2FyYXFtVGhwMWZNWE5qcVNaZnlBem4yK3lGbVlUUjJRSzcrcW9nS3lZdmpGNE5XYzcwbnBNVllTU3RnMnJIS3ZiWTFjbHZpVksyNVQ0anpKWk5QTE9WdG4zcVlrMnlqMXhjdjh6dHU5YTRQZVcwWVlRTHI3aXA2VGVraEo4VnB2d1RvcE5TRFlxVVpaUFJXV1kxZEhWRnlBRjR5c2NiVmlnUWlKaktGbHVUb3pCVEZWcndLeVJ2eENpNnByU0ZXL0V0cmt2YndhMXRHZmhiRTUwaHhzUUJEd250OGRwOG05SkRyTWFXVEFENWFTRmFubmlmSitzYmdnOERNNW01cFl0a0V5bXpzelhubGlrUXlmNURBdW5XODJKTWdEMUZwNnlQdHBNOE50RmRZQVR2VlFrd2xHY2dmcjU2KzlkbGNkSDhSTHNpNHkydElxRDdWakhjT1NCTzdoUi9JbWxmckdkaFVFbXhYRUJsUU9jUldDTTR5L3A0eWFqdDB2bURxeWR3SzI3MXlkbmRXVjBGRlJVZmQvTzNKNXlqcEMzUkJrMWhvMncrdnEzUkJmWUk2SW1KdXlNZzZWcXVSMC8zSE5iR0E0S2pyclAyNGovbVZCNmJXS2ZITEc4OWVxRE1nMytCTHVpeFdBNy9hQysvN0NxWGxnUDJBRVhLc2IzWExEZjdmNmJVMldFMDZhdHJHOUgzU2l0T2lpNXgydHFHV1RIa3h1TkxMeW43TzBCVGNGcGdoQkdNK05qSjNldGFqZ2pucXFvb2lsT2p1dWdzMmhFa3g5d1ZxSDJ0TGFFR0twTkI3UmFtRWIyMFNWNVoxYTA3Lys3U3pwOEZOaDVUZ3VQOW9WMWNOcTM0Sy9uaGxiSXM1dVE1TlozRG1IK0FydmhHSHduZmx4WUtpK0ZReXZ1OWlLUXJuWXFMc1ViT2VyVFU1Yi9tSXg2a2lDS09acXFEakNUVzNCU0Zsam9RbzV6dW1ObWxNb1oxRWNGdzZUOHJwcXN4OUxxZGx5STlNK2VZNFI2d0xNZW80UDJWcG1MQUdtSzU3MlArVjRQalgvYlRNRjVVdENNeHJvbUhnSWhxdFNVUWhoTXlja1NzRUVSNElzTjVnSUpyUlNwS3g0UGRta1FyOVlnQTd4eDdzdGs4aEpENmpJWVJ4NHdvSmlocGVkQXdtNmRMMzNST2c3TlM2QXdvaGZjdHZUZFU0N1BQUWpqZy9xUldtWmZNakl0SnA5dWRqQjJuZ0lGaVNQRG1iTTlWc2pJS3lHQmpvRmwxVDh4OGtqM1I2VWQxQTl6Tm9kdy9pWGhyOHYxU29tdW9TSHBSUWhldmJROHp2VDc3Y05mL21IYm9aU0IvMTJEWHdWaXdmQjJ3TENTNVJzYmxZUEF3NjA1bG9SUjN0bWJ0MXAwZ2ZRVXEvZE9JYk00ajRSQm9CS3BHZXY1NjFWUGk3Wkd3L0VvUTRMSnZWdFZRcXZHMW1sUGRrelgza1Z6ek5BbkNES0M5Nmpoa1pNem1nZTZ3ZWNRMHhjNkFhNkVzNnpUYWNiOHlReWNtTmVXakRxVGRkMlhrVXpLVUh6MEl1bmVnNzlKazlZMXhmVzhMYnM0cHRKcnRkNXJIRUxGKzRmZFI1VUM2TXVBZ1BxRm5ZQ1BjUU9EOVJxWjFXM3dHSk1QaEpBcTlZdmlVNzMzY20wb2c3dlBNKy9qNmtzeDJMQzVzbU1BNThnS1l4VkdZTWIvVmJ4L2VsaGJGL0U4QW16QjU5RzNUdnMrQkMxRFZsc01SR0I0YzhPdldoY3E5V3V5M25XM2dTRFV0ZFZ1Q25TbE1HSDRXMHI1d0EyS0F0Q05CRjFGeHdEdlQ0TWMyTktpSzU4SjJjNExnVElZVlUrNHRmSmJvWWsvMG8yd092NnRrSGZKcTRWVnd2RWg4dC9tSUJJR04vNjA3ZW9ucHVzUVVIQ20rczZtcVFMTVN1YzhmQzhWTExNSzBUekxYcy81TVYzUkZiT0t2bjFPYmJqcEpBY0t2ditIWFFaU0huWEZFSG02UE1DeWxvZEdFUU8xSm0rUzFZdlRPOStaa3dXRHdRbWREUGt1ajQ4V0dEL1VIMTRxNFhsSEprYitqZTltRDFNTjRmcWdZRFB1cERUVXkzRTF6MkxHTkcxWWQxYWgxVm41ZWlaTDdPOHovR0JYTXhmNlZhSFZYeVI0aWNiR0lWNE9PMzdLOHJlRjNZOFVnQzA0WGo4ZDdzT1NQTk1DZkRFend6NHF6WmNzaWRvbjB3THBuS0VlUTVDOVVJd3NOczN0UVBpaHVBdlFpeTJ3ejJjL1VLb3VZRm90N0hKZC9sVGJZQWN0MHlkQzl5aUJZaUVDK1Z3SFhJOFFndjJoMmh3RlN1TVU3TllNbkt3ZjE4cndIT3NBNVM2bjVKZHlOSENhTnh3THh5b1JJSlpjMXJQZURzbTJpWHIzYW1oOE4zbHFhcE9GS0lWK1RMOG1IUm8zbnFvRmhLVldPa201RkE0UlNDOXB4czY0YWhXa01Hejg1TUJ2amdYVWxRZ1JmUmFqNWlhL3ROb1lWZitJZXFTdXhjS3VCa29TSDdEbXloL2FFK3ZHbGhSUWduTDEvbFFxUitBQ1B2R0tCV2ZDZHFUQlVtMDdNNDM4dGZtMnNTWllsRlZvVG9oYTRLOVdkWVdVNFAwSGxGUXF4cUNFWk1oS3h0bkk5Qm9UN2kzWC9EcGdPTUdFRk1oZHJEeUR6NzFIakFubVBveHoreDFnL1RESHVuazJwcE11Q3A2cWV6eHIyNG8wTUhWK0JsMXltVTNoaFZJdlpFY1d3MElFQjQ5bWMyS01zdkY5bzB4QXJpRFJubUYxek13M0FDaGxaaGdMekVJM25vTFc2cnVYd1Bpa2NoeTRXQ2o4YnM4RlZRQ1JsUGtPby9vNWhYSGpVYmJUaENaNm5WbGYzNDhTSERvV1lENk1FczNpQWVVSHdNVlFTL1pvS2JtVnpuVDlxSnJjL2toNWFsMlk1cytBSnZNUzVzNHh5Wk1oOGZGeGdDS2xHNHpPWDNZTElaUGZWRzBnUTBJRXprMlJPYXRGeVBpVjBGZVRSNUpuZmxRSWY2alpVUVNxM2JtZEZXclNESzFQZ21waXVPeE14cUlFZ296QWhla2FzL0ZMSSthMzZzeU1aWFIrb0hnTUdFczYrSGlsSStwU0EvalJFcGlqeFhUNnQ4ZXBITTRaKy9IUUdqKzMvYTdOd01oQXMybFMvUkFNbVdiUlZiRExlUjNJYWxvTGFNUzd1a08xQ29hWlhWT05ZUERtM2JPdnM0cG1leFNMY01CYXpNMlZoZlNOalp2RkpFc2JWcmV5Z1lQNVRYZFgyM0dTa3I1QkNKQ0pvdlcyQ2l6ZlhxOXVkSFlqanQycFNROFNFSklUdlJXV00xT1BHVWNpMWt4YThoemVidkg2bnB4MGQxdUYvc3Z4UUEvdmZhWVNFc1RwNUJadmxIWFJpT21qMzgraWNvbmdsTk1QQXZJSVFzdzRzZWVPUUdxNmZzRXlJNi9PbUF6QUJMdUtDUXpoc3MzVlZJNVhLK0tIUUVPUkNlWE5SdUR5bWg0eHRQU0w4RDBMUzVqcU5rTWZIRXMxMzlIV3NuOEkwRFVCTlJQZ0p2SXhrdTYySzJOb2s3WkNmK1dYaG13TWJTQXNNczNyQSs5WW5EVGhHOUhxT0ozeWYrZExxa0wxS2djVm5PSVh0T3lQMTNCcGpzNEN4SHI5bTVqeDI0U0VBNjVRVlJ6LzJ2U2FxU0VTdlNKVkNzMFlZVG8yL05TMFZqVmliZUlTNXk0T0UwWFVtckZTZzVyR3NpelVKTE1HZEo3V3pQYis1R3RqbTkzMmRvNFdjTS9zckg1UFpUOFhYNnBHUnYwL3owMDNXMFhtRU5vYlNqMkNVTU5kMm04NDU4WmVDeE1sdWVhYkMzSyt0elZKQXZkSnQ4eWpmWm41Q1poQ25Ja1lTUmRSc1RVckZZdDFNZk5uSWovdXU4VXZwd2MyM1FZcnBFSmZyM0dEV3JUUHpQRi9CK3JKUUphV0I0MW1DYWdwR2ZlN2tyc3AvWHNCVW1jeDJjREdpelZiZlZKSmN0SmpXaVhOUW11VFVPc3p4dDlPVWJ3RUdYMXg3L0ViTGRHd0k1WmtQT2ZpQTNQdlIxSFRXZi9VWlFGcCt1cnVQUkk0blVzRGlxL3hNS3h6VklLSjBIaStWdjlRVzViZys1dXdSekdyNTlJQXg4cGdyczhiVVFjbDJOR1BvOS9jOUZrTEY4a0xIVnQ5RTJFRUJtOVdhVmc4RjJwa0xQUURyU0lIZ1p5bjJ6Vk9pU2t6ZlJWUzdwVjFQWlNJSE0rdjZ3cHVHL0VtZGtNdWJHVnVGWGtNSTNnMkRFbUsxZFRBSktRaUduNUVZd1NzNEZLN01NVlVZdG5FT2EzQmRta2NBaDZ5RzFaNGtNTzA5UEVScWRvYjJ4cHJtZklLeUhOekJOMGE2VFBoMXZONWZHMFR5eUVZV0VweDhPYnBNdGtyTE1CMEdzdWViRm9XOE1qellCZTFWSUYyNHBqNmhMSnZTRUhyZWtlME1rZWlVbzdWNDk2MjRLMFZqcmx1cUpxdDRTRTVsdnZKN0Z5R2ZoL2w0NCtIRTlxV0tJUnNDZlk3NDNuYkthY1IzVlovQkpza1poY3BDejVIUWgzcTgxS2Fuc0xXVDhDNjRZWVRkcHd5ZTZjUGluYlhTdENuUmV2ZjlFRS9EZ3BqZFp5T2dFTUR2eWdMbDYyQjluNlJ6dFdsb3UwSWJhd1NCR3R5R3F0ZUxxVHhVTzJjL1lYMGVvMllGTXhNRUxlLzdJWis1SUo3WDNveGpnSWFyVG8vaWJ1Ykg3RGVTaUxnVzlOUXpGSmg4bnFLMzVzcjI1WDJWYm9aK0VHb0MvR2cxdWkvendtYTMyWEFuRUlPWDZHa1V3eWxSOGc4NlNuMjlMbDZiZTRETHpvcEw1UzhnbWlCTUordXBaR0FhTHBTYzZrV1NxbmdLWm1PUTNCS1NYeFV4cUQvckUzVkkvMnBnNys1ODN2aGxWdXBNS3VPalg0ekkxS1hZSHQ2U3hKUkNnSDByTE5Da3llZ3ZXT0xDVWkwR1NCMk1lMVJHYkR5ZXNMcWVINnJDa0s0NUNTSXlPRTNOeFl3cDR5bSs2S3JTeFU2d3ZDQnkyU1Vuamt3VitINFR1VDkvZnpRc3dmM3k4RzNjZERQY1g2OU91T0NiVW1IaXFKNnFBNWsvTERNUGdnWWdpeDNzZm5VV2l2dy8rNzR0SVRkcExDZUE3RWU1OFNMdnc3dDcwNzNKYStPS2RHdC9iT1VMd1M5SWFDVFRMU09YN01HWE5TNFZHTHZBMVpMRTMrTVhOUmRoaEltZFNUbDlOR2h1NmNHM3cxOUNKdjd2T2VYcnQ2NjRleFRjeTNNZTByZGFkMldXeHd5ajlpVzAxcnhablRVUS9xdE5XZ2dqQmpkK2gxVnhKL1NsVE5mQkZRSWRrc2RBTk51NkdVUXB3UnNzQ3M2YUNpek1KK0Z6WU1OUG9hWUdZcTh4SUxGcysrOTM5cWx5dkk3WEJ1NVpHS2NORlBKRE9ETEw3ZjlJR0ZrZkNDbFAxMHk0MFNxQ3ZZS205V3N6dHRaTWlvL1UzMHZhYjJjTC9CWkw2S29uRkdXc2ZkODh3RGR6NWNzbXBDVXh4VStrL1pyZnRBWHBPY01ZWUNRT2JOOFNWZFdaeHdFWmVDL1lNdzhpZktwZWRlWmtaa1ptMHhPVjdibXRHcWV4NHpqTHlrUldxcmI1ZGVINC9NaWZwKzZ5YXVpYlJRd1VxRDdIcmh1SDZvS1VGREY5SlREL1Y5N2pZVkV1MEJiY3VOZTF6MEFkSkNmV0JUSVhGUjQ2emh5a0NhUkxsUStXaWFhN0tBa2xVR1Q5TkRqekJ4djNmZzBIS1BUSGVkT0VXZkFNa0dFbEx2L0xaQnJRMXBoaTRmZHQ5NmwzYXQ3S3ZGdjR6WWxBT1NwSE1XTENTYk1NSENVMHFwUittWnNOc2pnaHBEc04vSWowQkFiUEVZZGMvU3hUNzhqcEhTZG05UHE1SUJacXBVcW43NmtUaWVmVkFTSSs4S0x5akZIUm1XSHNjZG9rMWNOYytuUU1JangvSEhVMExIc2RMc0h1MzNLR2FCNkNLcHk3STArb1lYVUNpR2UwY2lyZnhjVmdrVVJER3QrWUN6a2VnMzZwempnaUVPc0NURTM0bDZ1ajhzaVFXUW9zOVhqWk9sRzJPWk9HMVJkVXRQUzhxWFZxRnRVNDNqWTloNXBBc0w2eEZlZmdNNjBtaGJRdGlPSjZtWlBLNlBiVlEva2VDYVJPcFlWUmh6NzBJY1dXOVJWRkdLRzRnVUFuek5zb1Ywa050RExvcGx2RDlpL1NTUGhlais2Q0R5Z2ZuQy9aNjd1TWhoaGdJY09BU0l4OGNLV3ROZC9mWTJmMWM1Rm9jV2NyRWx2d1loQ2RsNVphZTdPMS9HQ29FWjBmdXRjRE41WndyQzVrbE8vWWF5SGZYbkQ4YmhqaFhpY0hLQytYNHNSV2NrZkpVVEc3cUU1UFdTVzJKendpZmdOVFFneVRpb3dMZ2FjdkRteDZydVk0SDNmUzBLbzJkdXg0L00yWXNyVjI0eHh4eER1Q2xUZ0ZtL1pRSFVlZDlyRTRQSUcwUVM3Ky9MYkpNQWZqN3ZlN1BYd05ZcmJPTTA2RTFOSWI4ek12RHpWUzdTcDhnRTdkMDlrSGRBWE5tYTVNcjYyRmhaUFlGTThhTjBycm5SSWlIaVFCL2F4M2M5ZGQ2K091enhCR0MrMVlKOWV3d1liMmVxM0xkQmlIQ1hERjZ3TFh3MlRGdG45R0lBZnh3cUd2bksrRFAwZUc2T2NaZ1lnb0R2b1pESTlOWXJ1SVFWNHdZMS9YaUY2ZVlIb0R5VCtLMWpJbTlpcCt2N1E5ZUw0UVhFcTZWbldHZUtreFRZbVYyampFL2tRMkRqRkJpNU41YWlRZDY4TGNiU2Q2RlM0U29Zei85d1I2NXluSlVnUk1kSHQvSGtTZEZtaHA5RU41SmJtZHlkOHE2bklxa2JabkdWcTF0eHZlWVVXT3hsTU9Yd3B0S0t3bVlOQzBsc3QyNHNPNW45RzdzWmwrYStVY21kSlRhRXVDaHdHODJHRlBvTTZyVCtJUUtzR20wVEdmNTVPb1NpNXZCUXBPTVE5cWZWbk1jUHdSTTFZeWFpRlNVM0VXWC9la2k4VEdwcUhHeHlTdXNyRWdkYjJRVFdtTUJMMFBYcyt1ZlErK0xtVmRWdW5mM05ibHh2S3dPNXFlM3hjM1pkZmpBVDQrbGx2cS9HenYvTlhQVGRyMHpVVFErUmxsT2xwM3pSczFkMnRSd1ZIZUFHQXJPVGdQamkyNTZhaGxIay9hZTZHWU1XWkZ3TkZFMEliYmozSktlYmVXVUI0NXJJVm9YbGcrMFlndWpnT0hrVDdBaUZTYWhIdEtxaUFhZmQ5dmR3TVROVndsREZLaHZ5Ry8yWlRvRWltQXlFSXh2NWxPdHd1TkVIUmNJaWJ2UU9USXE4R0NsNzUzTUh1NkJWcUg4M0tmVk4vc0ZZblQwSEZKQ0lBZ1BvT1VDQ2ptb2NMWEtQdGkvNS9aRGkyZmdadVBvQlJmUWZkNzQzZjBHV25KSit0eW1WNDJ1SzNraHgvMUtOa0Ewa21kVmlsV3pQeGFFYlU5NEpaM0lyRS93TVJFdlc4Q1JMUXEyWXhZc2dqN0JFbFJ6a2dIalErY0lKVElvUW1pNWV2VEswQUN6K3AyZGNBeC9XUnZTRm5NMVhGbTcrcFgzM1hPNExvcGlvaGcvMk1vdk1qQ1lmYitoWVEwRU9rbVNiZjFuV0k3bHBBYjJDYXVOL2xnZEZ2d0MvRzZwOExGV1phYk04RllqdkRlOUU4alMvT2xsOVUrckx3d2VUaGtMcm10eGJpdGpuSFFsRnNIVkpYQ29Dd1JFV0tzeVMwNnNweHMyWGV4ZVdiV2VDTVRIR0x3T0hCd1ZMemNIMmluMCttQWNZRkNZcGtkTnhIaHI2dEZ4WTJCVkhjT3pEejd3Z3NGdjJaLzUrSlMrU3hJWkZGSkZyMC83UWFnM1lZUU1QRUhPQkZUaFkzWU9vOUJCZ09LQytEbHlFbE1xdUl2QmZkcVBiOTdVVVYyRnpjMSt5dFJ3U1ZqWHFWSWtGOXpIeWhXWEp4aEhnZEtSOUlaSzdNckRHVWp5VlZIYnBOditnTTYzM2xXUXBzd1dHTERZSXJoZEhqRWhGVmwrS01HYjlUOThwK3RyNjRzZkpSS3dldnpRdmhjRDBoLzhOSzhRZVZMN0tsOTNBMWE2QmtXMFpob2ZWUEE5Qkg4a0hCQ1pKQWNXSS95VFJrQW8yRVlRc09BQkphcVBDdzNUVFRzNUUvVDlxanNpbUF5OTdyQUs2UUhleG9wSGpTOWMxZTVsU1ByUFh5Y0R2QnRKK08wSTFqaExrT1V4R1NOYUphY25Yd2haelRGV1V4V3ZaVGZjZ3FMOWpLYVg3WDNwZEo2eEFDTG9Ldlcya0N0TUtJWFd0dStGUDhxcUhOOUZzdmxtUzJaYnJlMXlNMHlCU1J1ejNtZVl4MGg5NEdmYXNmSW53dnpuL3cxY1BBMzdwSWdNNGtCMVIwQ2ZHUGpBaUFBUmZad0JicnFzT1liRzVhNHJoOW1HK3FRazRTdVJBRTVQYTFIQWh3QnV5TEl5UjE1dXhGc25hWTA0SGhnQXpXT2xsdTN6WlRtTWZ1U2pjTU1wSFRicEJwdTFJNi9MbGRwRlNJMmxOYXpsd1o0MCtJcTkzWWFRZXZ5a1dtYVovdk1XSGV5d1B5ZklrU3F2Uld6K3dLQTArQVh2Y2Q4UzNmN2JJWnRpYmZFQVZWdGdRUDU2UWdCamt4WlU2M2lGdHIxbysvUzVWU1dxRW9jY2lYRTJVaERSZDhnd2E1cGJEenByUW5wdzNDRDV6U2VwMGhGZ0twREMvZEdyeDNsNXpYOGx0Mlo1SmVXZE8vQm5pc05OVS9lRTRpYVdZVDZuZE0yV2h3VndkVHdVc2krQlptbk03dGwzUVJtQVoxaHlmUklva1NGdFlWQytKb3RjZUtkRmhrY1JFTjNzZ2Vrc1loeGhCMEkyNE1PQUluSnlCMzlOclNqOUZFVVVENXVlNlhLUnUzL1RnaXp6WFRFSDJCdDZNa3VDQ05RWjRnRFd4UHNNNkFBdmFIOFVrY0JyUVhLa0hid3BwOVpaRE1VSkhQakk4OVVwWUlXQytVcmNEVFNhbkt4VXZnQkNSU1lYN3ZFaXROL0FXTkxabldKVFlhdUtlQjZWdjBUTFZ0aTVmbHJUcnV5cjhMenhNZDlqV2VGcSsvQlg0QXo4c0lKZ01aUUh0cXA3Y25MWklodjh0azVTUFZ2dWpZTExxWjFYVGFxZnFBYm5VZ1FyU2RCRUx0MFpnQWtBaU5TSnAwNlBLS3BSWUpUTVlybHY4Z3h2Ykw4UnFNNUh0UnlMOUUxYmJOTGZNUi9PUWVqcjZMRHdtZ0NEaFRMZXdKQTRrMW13Q3FhOEFsQWk5R0dUbC9kNHdUcy9DalFDTDBQZjRYQksyRGFkYXFVdnpnZkVTU2JsdjE5VzBXSGphaTVzK1NIdzREcjRGNE0xWk44SDhBdWU2elliT3VMR1ZnUitzOEQ4TVFuU3pVWVNrb05pUTN3OVk5eFhnYm9xOWhGNlhNM295UWlEMGNhYURGNFZlVFFxdDRXeEtvYWluVHU1cmtqcyt4ME1wbGFhL3ZmZUNhenZWcXo1MzVxSGRRYzE2MVZtb1Z4cDJCYmJoTUR3QVFnZTBram00M3VJTDlrZVowMW40YTBPbXNwb0FIcWJzb2xFYnd2eitGMjZDOFI1eWNRRjNCanJrQWRkMm1LZWxnWnNoRVJvM0JEMFNPMUYwOWNOdUh1R2I0SFgwMXdPRFpvZU81dFJwZUNEQWFEVllKY2g1cnFSTDFXcy9Ya0JCUXFSazlLbU5ObkhhWExBamxqUTAvTzlxMHNMamJWaHBiS00rZ1dtRTlITmVrcUdwc3FSTGhlY1hDMm95Nm1pZTJrSlM2ejliS0VyNEFyaFZxMXpSWGxOa2duakRKWi9GemRrQkJ3dHRHNXIzQXpBUlNSYUF2aTdUMXJCQm15bkplUDQ0d3d4bXZIOHVvQTFjam1uVnd2S2xKZk9xd2Z6RUdzb2F6d1JDTFVTNlhySTJ5Z2FndjJkYnFidkJySE9FcURDNEpYcGZPVWM1UmsvbTZPOFlBZ3JJdEU5bUtxblk2U3FDYmFaZVFMaGw2eEdublFkaFhpN3psbTNITGVRd3Z0emVGNkpub0xpNkV4ZWZ6dU80MS8xSTJXTi85VDFNTnVnMGFvU0doU1lPakhuNGl5Y3FwLzBXZGRMTXM2d3pBZVNlUlFPcUFId1ozeWVNYXQzZjZNbjdHQnpaaTdvZEYzYTFqNGxKM1hSUnFmT3ZVK1ZxUmJOU1RNUGZCWlB6dW9zNlRxWFQzNDBYdmc1ayttZG9icEJxNldXdStjcnlIbG03bENBbU9YZmtPeXNEdFRHR3NwN0RGaGdrNGg0L09oNE9xOFdLRUVMWjcyUHVaWlB6MFZmczNBTmo0Yk9ZNEFrQTE5Ui9SaFpDNTR0ejNaMjZJNWR6RXdzQnJmRFZ0WGlMOEw3T0JNR0dxUStCS1JVeFUrWCt3clhJa1RGa1lweUMzUE9LSGZRMytvd0RxTWk4UVM3RnpYR1BCMHZ6SjhrYXEvSEVoUmllN0dITk1qcHFhMnRXUTVCTDR4ZFI2bU5rbytWUW5iZ2hQWGUrVnlkL05CRC8rYTRDeHU1Wkw3czRPYkdPVDE1Z3R1Z0VqODBOckdKNUxXeUtMM1ZlVi93UTNTcGJSVzJJNmxlczEzWmhuclpuQ2k1NzVaNTdReE9WOStQVEwzREIwUThJY3ZadmNiTTNhSUVUNXdYVlFYMXJyUFBxaDVHS09pYU5LYTFvM25YNHQvWHBqRU45MzYxc28wWnJCMzBtNXRLbE54MEI0L21TejEzNTlnR096cmZnckI0cFJIZ2RIMlBHOEhRUkVkUmJzZkpjS1VHek5VWU04NXJkQVZoWjlkajFYZzRmUm9reTdvVnNBMXNEenlVd3IvWjFvNW01eDg0NTFib3YrenlJRkFzN1YvcGxrOS8xWE0zVy9NVmdmQ0l3L2ZjTWhhdkNLcTFyOGZMRXVENUxBYzRxYXl1eS95b0MwQkp0UnZZR2ZsUXBPME9HeDc3USs1c08vbFJSV2F5dDRRNUZRZHByT2tkL1EvSDM0SitOb2xoeCtoK2FDc056U2lYZ1JZbkhCRS9iYzloNGFSUDA5WVpPRlZiQks4ZjdwWk9sR1AzNW5MSHFSV25aWWVVcytnVkVmZ2Y3V2I1UHlKNSt3a1MrYUFLVW1sRHRkNUowOVNUeHl6UHl4MTJVTzFtWHBYeXJBcVh4WWRGODRMcjZsL2hLNVBtZDFsNERsZmxzNUpmZnBEZ2lzU202clhxZUlXSkJFMk5ZTFJXNDNnbXhObWJUaUtMalFpTkVGWHdjczNLdlRwb0tqTzlNOVF0WG1sRzlWNUI5OGtsS3lTOUhPUUd4SlhNQlhYT3JrdDhjSldjK3o0TzZxc3UzZGxXdm0yMDZtcm9DNjBHaUVaSWtKRU9vcmdMdUxhSGlSSEVsRnRVT2lNdTl1V0ZUUytVdzhHUXVBYjd0TW5wcnF0NlJTTTJKdSt3bDB1ZGgrbFpUcG12UVpjNVR0dXlaNXN5NUZqRkhGV21mL241azQ4RERhcldJanhNWmJBcHNVb01BU0kxcnRIY2M5V3h0WWl3QTYwM2w4QkZsZENxMkZyK1FhWVI4bEx5Nmc4aHRKZ05VWW9OVjNvZVo3aVlESHhDNXovZEorWU5qWXFzaXBHMWJTR21oZ21zenRzNC8wNU9KUm9tUis2c2pBMVlBSFh4b2ZzWTBPampSSVdPNXVUUXFuMDl3Wncxa1RtRHNDcUlZeXlRSy9ENy91Y2pPaStGSlBsNklIOHJ4Rk5vcDNNdVVGQTB5ay9BVjBYejhrNXNqTGt1Y210cFZzaGR2TzhLQ2F4UmtjVm1kQzBGekwzdnNmM0ZZRnowZDkySnZ6dnhUK1FYTi85K3JVWnEwb2JFSVdpU1lWK0xYK0JmNDBzN0R5T3J0Yzdad2F5alhEMTlJa3M3Zmg5QkJpclVXMFVjWGM3dXhCYWNuaTMvMHdKUjV4U2FXUXRBSFFDWVlsRjREUUFKWjZhNlpTL2VRNXJsTGxISGdXU0FzQ2VMRHlHVVFMSjVaMlRhOEREcFZlVXVIbkI5dEViTURsYVAwNzRZanpmc3JZRFZnempjVzN0c2RMZklPcmJlR2orVk11dFQzZXJkVzc5b21NOHdqTFJsUE5hOC9xTnhmRmM0MUFhL0pPRTNmOGtrTzh5QmhaUzA0R0lqcUk4ZmVLZ2lzYS9jWDE5RFZQUXhuWUJ6TEorbVhTeUxNWGV0NXJURFI2L0psNFpTeVJoWTAxUFRqWWl4ZktmU0Rvdk4vQjlWUXdQTXdZQ1hCb1pteTNZRFJNZjN5dk5tRXRLbXljd0dIQ0p1aExsWTlZUDVVRTN4bTJIcnBDMzRWaXlCbC9UVzdyUXVaTmxzc1hQZ2pKY0t3YWhaUW53MURjK0tPMTc5VEE5ZC85N3MyWFI4WjFjYUNpUFA4VnlKakNXTVNzMTl0S0doMWJ0YWFaRHpJa3JRbnlmNDZ6WEUva21WbXd1Qnkvc0xNK3JEZkNGa20zV0NlaGRSTzc2ZzgxZSswSUNWdmMrVnJaSWNJOFVsSHUrdVUwV05ZWWFEc1djbXlqdjhwRWl1dVFNQmtsTkhmeUtMSUlFRGxvVVlhWWxwUko2YnZIanA5T3dWUnRKVEdFRkpSdDlMdzJtOEZlMTJ3d2syY0ZVK0JpeUMva1dFa2NReklkNEs3U0pIdG5xTWdZMmtvU3EwRkF4SmhqaUoxL1VBRjJ1WnFRV3pKdlEwSnYrVE9RQzJxZHdkYUZrUEdVMWt0KzNObWZWWjF2dDBmMWYvNXVLbTZHWmxJbFdBSUk3WW5GWUlGN3BJaGFaTVhQdzJxK2lPMFdRa3lMUFhoWGZCRjdkZTBsczQ4c2tLV1FOaEJIRnlMbDl6Wm9PT1MzV2Z3UnpNSWhGeWp2ejhOMVl3emIrMStCaXp6aWt4dFJFL3FiWUsxQWwyb0Y5RFhUUEZRMitmYjJWelRSMmRPOGNTK1BNeEFQMkJyVmJRUkkxRjZtMHpRWUU5aWs5QXlLaEhIMEFCRGRvRXhlaVVhQXVhdTUrWjdEcmE5dnhLZGZEUlBIUXJndzdEVkpGYzFyVTloNVN0WU9FSHkvOFNOd1p6ckZ3bXF4UVBqNkFOVDBOalQ0azQzQXlhTWhqU0hUbWxrREY0UlJwQ0d6WmZnRjVyZHBNUjAvL1N2a1FwcS9UWDBORW9kTXR3UFFBTU5YU1lkbERzakdjQ2djV1lPVkx0d0ZwYU1IWlJwamlVOVFmaFMyRUZZVlJRWXJXU3prUTFCaXNpbEJlQzZIZSsva3hJRkNSa1crN1EzWEFPSWw4cm50bSs2OE5uZDI5SFY0MDhKeVlwa1E5SFh1M05oaEFNSEovRUNWaGNSRGVPS2xwT2NMcHNRMFpsak15N2s4RWR0WnNtMXdxamk0S05LQUlQbEp3QzNZZEtwS0xFcG9mbjk1Qk9pZGZDTWJQWXVBWERmV1lVM08zWkJrd05IOUpBOG9mWXZPWVlKK3p2MG1DdlJVQU05aTVEeXJGZzZRbVVNZE9HT0NqUHlGbzNvKzFVUjF1RVlGblhoemZUTWh6bW1aZk94akpjOHV4ZGh3My8yL1hDeUUzOEI2OUoyZ3U4dDUzb2ZVMDNqZlRMZGdwTFNWMXc2Qm1JelBrWHFINnovSHhmNzZpeENDZm9MR1hwVU5RTkh1UG9LM3MvaHNTUlRZcjdydW92TFkzakd5WmZWZjhjQUF2OC9YbmhJbVVuakRUK2FaTDVzWFplcDk3czlyY3FzZzR3NzB6L1ZhbFlJZHEwZ3hsOElBVmlOeFBtc2lQbUt4RmtpSHUydWZpWTJVK2lMZ1k4QVV0ajN3VmgvTXVDQ05WeHlRV0VOMGQrcHR0a3RsMDlqdzlzTG1EUnlFWGR5VEhwbVl2Yk4rdzM1a0FkK3ZnK0t3b1VHaWJ6VldTemhlN1p3TjYxdEZ4Njl2eFNWNGhjMm93ellEMnlRRmNLMHQ5eGp4V1RDdk1HSjRPNm9HZVYrWXg1aUE2aHRDcW95bDQvVkQzYkt0c3o3U0htKzhiZXAxc25xMzJKQTFiaXA0cHRrYWdIdUZIUmI1NHNXVm8vTkU0TzZTdFFxZlVFSjBvRnBGTTN5NmsvV3hRdXRYVW0xbWRPc0tTK2pxeXlSMENWazFMMDVqRGZ5bnJmVUZKOGVlakxKZnZCVU5Tek96cjFWdzNaN1Z6Q2JSUHpoK3Z5dkxrQ3hmQzhNM2g2VEpiRVZmL0FROXVGcXQxR2h4aDlsN1NmcUFLZXV1RTlicWZBNGlhcS9mazJsem40UUVEeVdtdEVqTkZPSDN0OUFNdGFFT1hkUTBwYUxqYXZOWjFGWEZNSE9OcWVoa2hJRVBNT1JYRVRqbnRRTC9HNFM5YlI2ZFdUMTlKSUxhZzZ5Vis0VlNLWkcxT2tiM1ZLdURqZ3o0aVFsV1YxYXJqL3ZTTDRMdHpUdStBbjdQZkxTTlpHb2h0OEhyYzAzRmJsZDNBRGIwK1RkSlM3RDF4bGM0bS96b2lsanJPN1ZyMUI2NTN0bFNHN25yQlA0UnBneDJpeXhPeTQwRnJ1S3dTbzUzSEwxQzV4N1hjWXM2WVRPTTVVU2swcEtjUEtpTnNwRk9kRTl6VnNoaGlGTFcvL3huYVpweDczRDBROWxGSjdlSi9wVEg2QWR0a1VDUThtUi8yekdCUUhqQm5VVHNTYTAxRW4xMDYwUGY3WGZwUjRNUWFtckVHNjY5WjdScTRDZDg5dFNvMUJwYXNqQ1ZHS09KNE90eFpqcWZ4NVdZYWFYMG13UFpXaUFBd2g1SENFZWZVOVQzN1lCcmRQZERnV3VxWlBoY09XZ1RBZ3lRR2NLcUFpdmdzWDBLeWY3OTNYZng1MkZLT09UaFZoRC9aemEyRXg2Mlg1S2d2S01mMlZtWUNGUXJYZzJQOWhTM04zOXdhUUtBdThqZWZLeG92VWRZNGN5WlE3dituMnA5QWNjMmFkTDNTd3ppMUZYcVJYSzNkeWtGOWl3cVhjc0ovVW11K0thZzd4amxiNmphNVlwZ1o3TFpVSDJKb1huSFA4OG92VUFSSkt5N1cwb1pJbnN4Qms2KzdGemREYWMvV2VYVTFhbEdPOXB0bnp3Nk9sVGJNYnQzTy9LZytkRVNQUWVEK2ZXbkRIL0NUeUd4bmxRR212cVNDMzNCZlNNaXRoUG1uelZIc0ZYRnhkYUl4L0UzZmRWa0JqZk9WUllzK0Npc3ZnNkk5VDVTVnM4eFZlQzViVmFyYjFJN0sraVh3VDNPK29ROGE1QlRGdG9LSjJqN0VkMjJwbnppZElpdno3Rm9rdkFFcitTQUNReDFJQkNYKzRsVWFoV2QvdndLTWhZcGlYRTZTNE1xMWtjMlU3MGNqelhxK29Qb0g1N09JdzI3eEdUZVlrUmc5ajArbjJ4VUpOQm9xbmI3UU1nb2VpSUN3dmhReEE3SWNialh4TmNSU1FrV1h4aUo5WjdaMk1SYWhKU01uOUFsK3hHR3hBVldRaGR5RG5oYkxNZW55Q01pb28xVzViRUxsSkcvUEQvcDdkdzBQR3ZHelJIMmJXYysrQlNJKzF4dFBRejBvU3NYVVQ1MFUvVk03TmtKZ2hlODRlMlhHQzZiSVZhWS9hNis2QmN6SEhFRzZzcVhhNzJYb29hZTI2KysvbWxnWE41MjYxa0pnRHJ4a3oyT2VlQkpFQ3h4TTJmMkl3aFpacU14ZXZJb2lPRGpJeFppRHkrSUp3Z3UwNjJSUUtwWTdHUDNVMDlqQTI2cU11Yk9zQXJvUm1oVktjRDJldW1xT3A3alJML2toQXJmbm11bUl6QXVuU2VMUVRzTFNmWUxRYnlya1gwL3l6K2FvcSt3YlZPWE44Rm95TXBLKzdmdVordnlFemJVOGp6aUpxalhUSG1DVmxiL21BSlZDNDdmdldlWXh0dlZ4MFZFL1Q4TlVzYVcydzZwNk03RU83TWpLekxsM2JsSkp0ekx1L2N5VVpzbjA5S0dXQkh0cEc4aEtHVldZTE9tWkcrWHRPOWw3eXJ2NU82R1pheUgzbk5RUFpkSHBVV1hzbVFJN05uTUI2bEsrQTgxMmJUL0ZiSThCSVZPYXEwQ1B2S1ljUzZrTkZnOFpyRGVZeURMNU5DcVBLa0VsUThrQkRtd0tvaDFDcHRISEdjZ3J6ay9hMXVzNk9PWWRnTElzVnlJNDhOd2NCendjNXc5WUF1ZWxuNGgxVzdra0Z6TGUwVGlqd3N2OEU1RUdpcGYxcmhUeSswVW9IOWt3cWtudDFuVEFuZlhWaHlvaXlLMnpuVTViTUU1em82VXdCdGRyK0JXV200OG5UZlpEVXhZN09DNkp5NS8xQlBzc0RnQnBnYUlGZ2paOXBxbDVVcURzbzQ1WG5NN3l4RjlPWmVERWZlcU42dDZaQ3MzMUZVV0FMYklhNXUyUTZ4M2t0d3pCZklscEx1RjRIb3ZkbU9rTWhoMGh1N3lwUytpVVpuZkNSMXoydFBMWUI4MXVvTUpObDR1cmM3UUQvY2szNGkyTjdFZCs2N1RWNnVqY0FrMWZoYVFxSXNaZXFlYWx6RkQ3UEd1WGpPeXA2alJFYVJDZHRvWXBDc1VNR3lRWEhuTUp1em4vcTlqVTVSVGl2Uy9heXBGemlQMkYwN3J5S3BSV3BYT2QydE9EcllYQ2hYNmRrU1pPd3JBektwdm93NENzVEhnZk1hSGU1VlZKNXFCekNZODZZWEwxWnV3aUw5bko2ZjFIQ2xWMXdqZENoTjJwdWlDdWhHMXBTSmhWVWYybDhDUlFnaGRJZzdYTHZrYkd2akhoWHJQUm16MlEyWEdiSGlUczlYQlR2V2poalJBa3RtbW1NV1F0RDVuUnlJeTFONHI0UGtyS0R3WHlIUkNTYTRxQlJIVUhmbE1RcXFFODR1WVF2MFJIZWczZTZQT1RDOFRuK1Y0RFh6WTA2TjE0VUN3WkF6K0p1c2dZT1g3YitpcFgyU3NTMlJYQmE0ZEh5VTRzZi8rMWtxMlFMbDFrNHBPM21lNkpONEVnK1IwTFlXTG1tVUxoVWc1dFNyOGZlR2xXRjZNN2R5VHd4RFA2NWNTVHFIa2FPVlR0VjYvZDZTRzNxa2ZKUkV6UzRpbml0UG1BeWkwTmdOUHN6YUJ5S09ueVdFcmdtUjhBUXdWWEh4TTlZMWdVZkUxc3VTYnY2bWFNR1pZM25KRWhZYjlGMllsaWUwRG92ZUFVNmNKMm9sTDYvTDdoUWw5WGhEUmZEaktPUFlaWnFZUytLZUxtU2t0bERFTEdIN0Nqd3hFUXZmV3Jab2p6Z0pzQ2RFUmVjMWdjQ1JKbk4zTWp3ZllzMUZSZ01FcXdsQlZ2QmVsdkMrTkVvS0tHUWlUMFBqN1hacHd3UnJ3dW52R3NzSm9TSThJNlFKajdLcTByVTVFZlNPU3NEMGNLTTl1RGlESlB6eGo0SkMvNHUrSStsU3VTT1ZydVNOUWpaOWJRZk4yTWtGaDhwbHZHanFZTkVRTjVEQkV3b0JTYWxVdHpmbXZueHlFaTFMNjlwTUFnOXBrcEpqbDcrUlZBWmhKdUxIWGRvZzVYS1ZrOHhzd1NDY0pFeUFjZERVTkpMOTduZzZJWkxUakxPeUIzZ09uRnEwUEJGMmIwTTRVN3haazh4Wk1LWHhBYVIyek94Um0yRUJsMEdUQTloY0ttTE50WkI2emYvVXVIRWplM2poQWE5ZTc5TGRnTm1senRzSzU0UFdmT1g1UzZSaXpDU0dKUUpiTnEra2F4Y2xDSVdOZVpVaENLeE80MENpQ0wyU0liUXdVaDJUTk1wUThDT3dlRGlWSzFZN3U1VDJlSFN1R1Y5TnpVMWhNeXlPdkViazhtYlpPejB2Q01RaGVya3oyUnhRV0JTSGpLRW5DVUJ2TFZmUEJXSEdpK3B3QTNZU1BXcGJpWWNRYzdQRWZFMlc4TzJ2YzdoVnR0T3RnT1krckw1ZVJCNGlUd215N3JJaGtQZHp5bHRMaSticm9LamZmY2VSdlIvL2M5a296aWthWE9JNG90NW5mVW01LzgxVjJ6NGtRTnR2ek1leWZtRFhVVitCSWR6MUs4RnF5VkVWZ3hoYVFDWmFFM1dTamo5RXkzOWFuZ3YxWmJhdGpxblpEdWFSMkZJS0pxWi8zblU2Tml2eWdsWkswcm5MOFk4WjVHakhjL2Z1M0lsWWQxTUNoVTkxVmVub1huWW82eGN3Mk5rRGJFVk5UeXcrZ0lkQXM3K0hLRVJGZDJMMU1rR1lKa2hGZHUxOHJBYWtxZ1Jma0VuOHEyQVorQzlSMWpMQlU5ZzlOOWp0dlU4ZTYvT0Z0UjB0UWY4ajBGMlROWjU5TTZqSXlXa2xjeHdkSVIwb1RNWlE3cnpjYXdydWdjZ2ZVTmp3YisxRFN3dXZ0bnVCdnFFMHpGZjRpQXVmSWJVU3pXNHpaMHlZUWEzNkVreDN5b0QvNXNFVEVsMzZnV3FIUGpxb1M4Y1A1Y1ltQVZ0M2kwV1RKVWtSdSt5Q3RJMzdDMHhVWm5tQXI3QlByZjdaTXEycWtpd2NlSjFrUkFFdVFyZWp1NHQ2cDNzVHRaTW1iT1ZUUjFNZDdiZXZHbWNqbGh3WW5jS0dYNGNmTysrcG0xL3paYTE4cHUwdzBJM1cwUHhRaXhJRTNQU3VKeE1oZU1hbUxHWXZZL28xZDRoZ1FzMGMzMEo1M1Bkd05rejVFTityYi9Ja2YzektSRk1iMnVsTjY2RDhsM1RYWVhiM3BOb0JYc2RoRTdrU3JKM0wwM0o3V1RNaVBvVERhbWkreTVCdERzVERYSTFTY1ZiRkl2Q3RNQmZpUEZHTWkxYm5Ub3FpaWEwdzdaenlhUlB0VGtWNkEyblB1OUxITmE3MGc2c2hmdDN0c1lLZm9NcUd0bGVldzR6c2hGYTdrblZyWVZOWWtLd2JyeU85S2pJNlZVY0ZQdWRiTS9kcHEvY28wQzNza0ozOUZKVEhrc0JiMHBtc05nSllKMXZ3THI2SHBqTUVzYnNJVzBVVnlmZENaQnJCS3NRYU5IVGtEaUkreEVqb2pFTTcrUHErOUc4Mk9mRDgxS0NwQ3FOSy9OU3FUeDA3UThoTmg0K0F4aWo3WW9yeHh5Z0M5d1JuKzV0K1QzaVNWUStoMHJzVmt0cWpqZzdaUlhzV1ZXUzJEdXFGMmp2VllTTUhQRDBaOEhncDVmSzJUN2dzTnhQY3dvSFNXRTQvTi8wWnArNFJodmJEZ0dpVU82Y211cUpFWmF4Ui9lcjdRZk1UUXoxTExCNE1QN0g1dzczeW9kQU4wT0hsY2plN0pDcTVMQnovNFVkekUwcFBjSm9WeG5zem00Tk42WEF6ZjJVRFVqeGhaUzZlSGFQcW9USjFJVTNyNys3VnNkbHE0UHF6NGFWSGcrbGdtKzRNRmJQUlo4bndrU09VK1A2c1VXYUpNd0ljMzVqVjlidS9wU3BaenV6ZU1DOUlpYXRVb2Jvb2w3OE1iQSs1RGpzS3pLTWgyMFluMU5VcWpjYTBCK0Vva01GdHdOQ0d5TW9jWnNzQi9XTURCTlNSYnBYWVg5aUczQ2diMFp2YllLNmxPSHN3TFRraWZQdjRtT2tzR2tQdmNGdWdvbVNtdDQ1ejFZdUI4dzk0cHBTcDVrRDZybkoyZzFzbStOdkcxRlVqNkZGU3h2alRHTTVvc1FjM0RZZVc4bnNTbVBjTVpUZEJmY1RoR29sY0VNcUtZcVFvNnRsYWxocjkwQnlQK3MxZy8zdXZmeG1EOEYxTDBoY3ZMbEpHYTdIOVhEZ3czS0RybHVMZ1VTQ2kwVWpRRjE4b3NQdS81ZlVvNnB0ZzUyZWZKQWkyOGt5NmFEUVlzd08xdnpoL0prOHBOaXNVckxZZGVoU0JVMUppdmhqWGtJOTA1RkswYVh3dXloa0s0QUtiR2dVbFZQNTI4NURkUUhtY0d3TDI1bTdGTmZqVW10QzIwZXJveENEQjRIay9tK05sYXBTSGdFNlVaQTRGaEFhMCtGU1JCTEI2MlhqYWpvd21UQmViOGlNN0RGSEtzYXV0MWxQcW1rZVo0L1p2b0tpS24zWnBsQyt0bjAzRnZIcDBIWm5LeEhQek56bU5HZ05heEs5NWtRRENGN2t2YzBzYWQ3ZDl2WHArUmhSWmdMd2cwaDZpemZSZkxLMVp3MkZUSzRFcWhzOGE1MWwyVXJ6Uml2bVFNaEdyRmo4dUxuQ1dSNTFIelprUDBaQ2FxaFdzZnM5WHdCWWdEWGxOOUpWQ2FoandTci9TQm94a1UyUmFjRDFOaHVma1ZZNmZwVjlGTncrd2krSTEzeWV5NGJQYXF5M2J1bWtqK3VPWDdIbEVWUC9RaEE2Z1doRzJLY092azdzc1Y2dVN6SmdMY2lyT09wblA5VzNsSXR4TU1XTWpBZURDMWFqalk3a0RrMVZMb0tTei9PWEV2K1dUTlAxVFFxUFZzMEVxMFlSWWEwa29wT3dnR3JTTUdGNmlvdnd5M21qeG5wSjEydkZUeDMvSTlLRVFDK05LWHZ2bUhmeDlKYVZwUFJMZ2l5NGMxWS8wTWZaejVDNEhtaUhSdUk3ZXppOXFsSXdGUHZVQms3dVVJVFEwQ0J4S3ZFMmxpTzg5M0lMV0VScFEwNkpMTXhlRW9oQmZSZmFaTG9PZzkzN2QzeERtTzJMalBmcnZYUllKQmRCcmIrRTdsZHlkRGQrcjUwNkRtZnhrZDhLMnBrbi81RDcyOTNwR3RWQlJYKzBvUWRtRHIxSW5KTnk4aEhIeXY4WXljYkFQbnpsdWNsL2VNMS9LTXVJVENkaE1wQWdqMkdpeEt5MFAwaGl4WVFkVzN1a2MweEJOeThabmRxbEgyUVp2TmpNT3Znb2YyWTJTWHBsQjRTN0FKR0pEZGVYUVBqcTQyZy9qYXJPOWFBZ2JEWFFEYkJycXN6ZGpCMW4wd2NBV0pja1ZFMnR2ekozNTVkdjR1ZGk0bUFrSENLTHlOSkJLVmpnN0xKYmpRSXlwVG1VTGlndU5ZaGhBM0hEMm44ZFlSSm5JVVdoelVHaEFCUWU1NElVSERnaW5JbnkzQ2hzREc0MGFMS3ZVVGF6RlVhT3cvd0ZQaHhHekN5T3JCazNEQy9Ha0hZTVRjZlRrQVRoOVhjVWhYcnliZlVqSjdBZW5mQkU4M0JoMW0zQXRrVU1HQ1pSTXZyeFlLVlZzZnpyTjFta2U0S2N3K1NsU0pzVVBRaCtRSXRFaUFqY0ExYVFMdzNWNFpoT1N2cFJIc1hMSDA0OGpnVUhxZVpjd2RUK1ZTWitqN1BHRXRvd2tKbHdkTU4zT1pmTVdOc0h0bmhMUEp1ZGhHMk1pNXJ3QXpZNTBRcUZDVjFoUTkvNnNTVzRwc0JVd2hyWU1RWjMvSTVRdlFuTjNndWg3SXk3Qmx3eFUyOWI2SlJRREl6TDlLdjhVRHlVeVVGdzFUY3BUcU9TdjJxODVrQW9VMXVzblhFRlE0aDFQbkR0UWt5SWExcUxHaFI2blRoVGN3WUMzVEZCUHNkLzcweTIyajI1dXpCczZ1RFdva3RsOGlpMXlEMkV2OHQ0ZEFCdkJBdElKcEJnMVRuOWdja2tUSEVKUVNMU1BnNzgrWEdrcVFpMUxibUVjMnpSNFo4QVZIc1Yyck5peDZKS2FKZGtVdXN6czBia3JwODl3YXV2QkxweW9WRzZFckRod0N6cFlVMHNJaDg1L1RObVZwUnBmc2xSbUVyUEZidEdiUHlJcGJLblVGSUxoYVpPRXVLRXYyWmhyWXpwS3NvakdzZy9jbmJ5NkRZKzFZc0lENkVrdTJOR1hqTDRzaTRqWkVjS1JJbUU4QVc5UE96eGhWbktWa1Fac2lPQVRDaVhkTm5UQ0w3Ulk4MC9GcUd5amZ1RzBhNzVGUS9pNmpFcTBXY21nVlNEVFI0RDN4ang2eml3NXpZSUtIQXBHb1VLRGZ1QnJGTThRUlEzSDVVY1ZMai96QmlFQ0JUZ2RHbW4xcmlxd0tmZmpWa3hlWVI1UUFJZGJDYS9RUFNsZ3VCME9BWk80U2RTMzJ2Y1hkdVdWYlNLNXlpTkR3RzF1ZjF4TWhxUVcxRHg0dnpBYUl2N1dSQ2dMUXpoYStrWGJmRDF1enVoUk1GbjllT0lucVJHYVIxZ0FyNWNIanVoTE9jd21CcitjbmdPc09leEVudUtxbDNRblY4WkxPbHVWTE9KT2VKS2V1ekttb0NzM21EaG9VSENMdlZ3S21laGZDVGxlV2pMRlZwL2NQbDNUVndweTFIeEpqMXJiTUZvUThEQlJnWVhxV1BpbUJhNGhVSWRHalJlaG9hZkNkQnJSNWhneDM5ZERhWXJjT1AyTDRlemR3NTNnem1tVjU5dmkzVmdDOTg0Uk91U0tDbmpacE1xTXBNZm13RUljc2Y0ODFhOUs1Rm81bDF6a3VvV0FYbGM3MFlkaHV6N3ovL3N4Ukc2S3hEM2x3aFdaMkhPRjNEV1Nkbmd2WU9nSFhLUFpHU1hiU1p0NUd0TnNCTjJLR2VwY3loY1JIMmhhbDRvZzNobHNLaFM1Y0g5a3dwSmFFcmltSW0rZ2d1UWFyQVRIYjcwZzJMOTdiRVZFODhiN3c1N3NzOFNERDhERXFhYWJOV2NOOEhSS3JDM3RLbmFlQjdPUlFrNUthWkxLZnhEczUxWkFQalJDaDlNaUQxdVo0SzdBeDZyOTBIOXFVVlkwUVJtN2J0WWdheGgyVk9lNTF4N1FtOGhHSzZBWG93RTNscnQvTHViQUNUaUtJUCs3T0lYbFNxbSt2NWNXRE50MU9rekJ6M1BVKzZ1Tm5DY3JTaUVrV2VHWGZqNUlZOURvWmxFRFBocHl6czVGUjE5ampEYWFaTG1vbnhFdjZxY0pDOWQzekxDVCtubVF1Z0Rha0FUaVNFZzVBbmtjbzZxcDJaYzgxOC9IRFJhaGwwVTBudzlFZHdnd3o3VGVmWUhNeG50RXpOQnlpK3RhUkoyN2M5eURSblQ0QlNPdHN1cVU4QVFML3ZEMGlDU1BZQktkWVpIUXlBMWR5TVI5eC9yY1EyOWFva2ZpZ29KQ0FCYXRsQ3hXRW1VYXdWdi8rYWdjQ3JBaVJLNjQyRDlqa2RTRy95MnlyMVN3TDVHRE1tN2d2eGNXVGdNdSt0SnpuMDI2bWtNMWd5ZEFMQzgxeG5GdDB6SzVzYWlHK21QOWU4Y2oySWcyQnVyOXB5MkNEdDVvWFZCY2NFNk9lU1A5c2Z1eHNaUjR6YS83S0N2VDR3VmhOemlUQzcvem9CWEVxSmZRMFgwdklMYk05bmV3dW50d0RIdzUxWFlXQ25OaEhFSUJvS0VmUi94VG1QdThZblBwNkY3MlZoNVkycm9lRFBhZG1GOVNCV29JZzRYb0lhTGFPMFIrSzhvNDg5clBRK05HaWJwcDNlNzN6U1VIbWg3QUk4U3BLTTVwTzd3ZksrUTFLQnhyNm9POUNtcUM3WDNUOTVuNlMxeUM4d1NLYWVJYktNemd0SWVQS3FJWGpOVGRGUUxOM2xLNlJGWmV5Nzgzb3AwZGhSZDJLYUxBd3hWZmpIVUNIajFoYWptc3VGRWxLOTlUUm9VeitxR0IxMHhmWlZIeVp6NHdTMHRFUlMveHFPc2t6Q1FVUVJjQVdlTzVROGhGRnJZZWswd1RYdXoybzFHM1NhMVBKWWdxOFB0Y01mSE9yYTQvd1FRMkRCcmNzRVJUOEQ0UGwySHAyR2ZBU3Q1Tlkrb1dBaytIeUdxK3h5VVArK1l0ZnFNcmdRVS9wbUtld1F1RGRTYzdBMmgyOXo5bHo1ZTYxWGVtSk83c1NBNHN3S0phWng5cEJ3MTZYM0Q1UXB1TEI0ZWJodFg5dzlJSnNpemZ5QzBGQ2VNbldpeGlmaVRGS0tJSTBmNThiaGkxb2NlVngrYldtT29yU3gwRjBNNEI2OStwNDBXNnpLVE01eUM3WGRrdWdtODkwTFlFNEpEWitadlkxOURrd04ySU1wM1BOK2Z1SEN1dWdlNytWYUVKZndvK1ZHd3F1dGNDckhGRW01aS9MMmI2SWt0SEJTWWJtVUZXc1FmajNhU283cFZYVE5MMlV0T1FPbFBZZXRmOG03OWNhL0J6OWV0d052QUkvV3RNQWZDOW1OQmd0YXZLTEJoMkJCdzVtU0ZFY2lkSEVsTWFVVVJFZjk4KzRKWFRuWjQwUGM3SWQyMGxUaDgwSFF4UFpIcW1NODFJVTBVaDh6eUpsT3dyMHFhU3VTUkFoeUttVHJ4VHRJS0ovMGlYdFNMbkhUalBrMW83UThvMjgrODdXeExDS0RpN3NnV09YUG43R05zakhOdXMremxTS1prMDFRS3NCbm1IQXVKQ2ZLZWtnK1hYR2ZpZlpWMVBVM3JmZWJqVkpNZDI5cWorQm9kaVZkbWVlM0RzS3lQaFFtREl3NDl3UkVjYmZpOXgvajZLSTk4YUVmaEE0UFBKWFRpR2xVelJ5VnZJaVV5b2ZwbnlKTUg1VldLeHhaMk90WlZEaWFLZU5uWHB3Z1hZNmVUZ0VlNGg5ZDZrVHJyK1MzbW5tRHF5Z0xpTTU1aW9ZeDZuODNRN05BU0RMMFhMdVJyYW1JT3JGMkVVNjJkb3E1ZmZoZ2dzM1NodUltcHRPNFFhSVo2Y3lwUW5BVUdCNEVIWTlseXEwWGJRV253Z0ZyTTZHOHlYaFJwakZnZHh1eHc0RmdWc3VudjB3QVVvejl1c2dRZE1Uc2NKeVZPaVRlajAwc0w4RnRJQ3RJMlErdC9lVWRIdnJSVWpveDFoQlBaT2JHS09qODZEZEFqVVlyaU81M3FoYU5XMlRDejRuSUZOTDJ0bHE1SUgzd0c4V3VxcEEvWm4zWDNBU1Y1aHJSdWJTZjRMRldiV2tyNlRSKzhsaWtvRnJ1RW9VZ2ltN1RTNXdkLzJodk5wK1p5WU54djk2RWxnejdRa1Y5ME1DMjdjTERuaytVM0U4dldublV3RHl1VElPVTQxbUVGN09jY0kvd3JmN1NXbHFvcHJyUFhyeTF1T3Q1NnFHMXhueHcxUzhlYUJQUzd1OEJvOWgwR2l2WlZJYUd6MWl4dGN0K3g3bjkvQ01QVEdsNXp3cm9SVyt4Zy9JWitlMGZZUFduT1BpNXIvUkMxZzdSVktvQUdlQUZuMVZlbTJHTFo1cEdrQkFsNHNrUzRTMHJFNXU1SU54K2xaN2puS2VHQTRNRHl3cTFIeXBBK3lNNVk3b1JpbGpQNWF0QTc5YjFzZmNFQ3oycmI0Vys0d3pEQnZGenU5bExRM0dkdlFGVTJWRVk0OE5tSEZ6Zkwra2hIWHh2d0tiMjNPYURVQnNoWGtyYkVHaThzQ3ZweWxMN2R4RGU2NFFqM1Z1aUFtMVFIejAxUnZmeEoxa3YzdGx0blp5NEl0aXdBbEYzMkk5UDVKZmFZL2ZYUVBXazBjbVp5RGhDTkhWbTkrd1B1MnhKakNBYWlsMmRaS2psNmF6alE5NWRHL0wwNzFhQTlpVGlDd2FUWHBwNk9PSzJibk5JOWxRTHVBb0ZWRnNVYVZETHoyaFpoNlc3L2poNHBJSnZsbmZnS0piYXEwakZySElQSkJTNFRxb1BKVWRWQ05TVDQvTzM5NmRmSTBnU0RzYXJrdlhpa3pjaWhnSHg3UXZraE5kRjlJTDNyRndLNzAzYnVjcFJEMGNVV05yL0g2RnN0Umd1eGZ6N3F3TSs5bWE2bDJGWUJHQVV5T0NxUUJRTkVLT3FyVDl1YWZzSlVLUmdmTXhabHA0VlUvdmJtM2J0dzloVGhQaFJueGJJL25oRUpJUHQ0R3hIdGhrckI3UUhnQWVSL2U3ZGtiZS9XQytEcmc0NXp3cjN2ckdMZ2FBdy9ycDNuWkM2NFN6NjZ6Q2d6WlpXbnpib2NvT0svcjlIMjY1b1hxRmRGeXpwc0JYMkxlbmJtSEtNS1p5em1uRFB0U0J0eVczY28xbzdyQXBYNUdLOVBDZzhXQWY2S1BSNCs0M3dzWlRWbXplUE9HSkJaTGdQMVNGd3h5cWl2dWcydEx1ekFHcHBHbUpscWNCMk1XOVFEdTRWT1NQRk1SUkl6SkJ6R2k5OHdpZVZKNEY0Y2tCc2E3M25VZHRBL3NadVhINjYyZm5MZmlabENkZ1ptMkJVb1U1dlF4OVo5c2hYQXduc0FwOVhMUFBCUTh5Y1N0UnkxVXd3WEtZNkpVazZaelAyek1pVjZ4QldXQ1picG1mSjBlbUNmOTJYSUJ1TktlQTZleTRaMkFBUEhicGVUZHFlYU94R1FabFNUeG9BZFVjSzdLanBRWmJRbkdrWEgwbDBxK2JsTkQ3WGxYaEFYaGtSS1RISTFWa0J3SzlGL094d3JWS1NVSC9Sc0ExcjhWNUdJdHpVeFBDc2tsK3B0dmpPU1Jqdk0wUmdWR0tud052MS94WDJ2ZkdVdFovN3RranhyVWJiSk50WWVNODE1cndLZXpUT0ZkNnU5WnJNdnA3ekZpTm43OExVbjNlT0tvT3VKODJBTFd2ZFJVQ3FBUzVIN1FwcWZGTFlsVUo2OEJMd0IrQVdiOU1wU0RoWDRqbFYvdHpLdkwwaDNDS2NRMUJINnFmYUZkYi8wVjNFY2FCWXZBYnZiSm04NjFZcTh1czJQZmpWRCsxUncvVWx1ZEt1V1JkT2YvWGcyNHh5NXdMdDhhenB0UG5jbHZyTEVJanBERmNlVzRUcUpidjU4cEM2NXN5dkROTGtUMGgrSWJ4Ujh2Z1VHN1FGT1FVbEp6Q0FzR0drdVdmaEhqZjhGSVpwRmlzSmxidWlycXZHam9XVGtuTDBDVC8vaWh4WG5lTWlESlFyKzJwK1NjQnN6QU45eWh5K3pqRytTc3NLdndtRWV3OGVibUF3L3FxMnNGbi9UbmZzMEFPazZMcjlnZWZGbWZZTUdzclRaRU5LZmdqK0hESEdvSFdaUVZJZEY5UzE3QzBuT2dFcGVYdVBuKzJDdlh6NWhUaERCSzkxTXRZWmRJUkJQOUpaVjhvVmsvQ0JCSjA3VURaM3hTdUxuTFhPK0VVdzYrY0lSMFhZbTVVblRyOXlqaGtFbUV3NDZPYmNzTURzdTErNmFOKzJBT0RwYVd6YWhRdyt3QVdMaDY3UUNRekY5dC9XZjY5dnc1SDFzRjZwSk5rV21Lc0FYWklvZm53RnJhK1hnK0V0QVhEdGc0TGlNdWdFUTdDRHkvKzZmaE5leGRCLzZEeXF0NWIxbkJkNHZDek5aUGZqUWd1RitoQnMxWlk3Vy8rcU5NYjV3VlViUVFmcXBaa2pCdDc3aGtkZkpnZTRhUDUzNjZ6dkMxMDJTVG1YK3QwQnlpbXYzamk3YllPdGpwdFNLS0lucE9DdXdnSHZNOXV2VjZxUE9nV2pXeHhKQVlOVmcweFNuNE1GLzZ2MFBpeWovWDJpMUZCbktUenhwbnlHR3ZUSzJ4UHVlbmhlNTgzeEplU0ZvaDhJcHBuTkQ3YmNSeVptVHZnOHlGdzFTV0l3cWlhQTF3RUYxVTFGUVdLWTNEamlHUmVUWUYxTVNWQ2wrT2pOL0c2U05Ta2FzVUZpcnRQaGZFRGxtMlRwaVdiWEtWRWk2dWk4RFlGWTVxbHVtZWUrOXROclVKVTdRZkhNaERROERXOFZoU2ZPUWhJTFhqa3BhL1JLN1NXY3R2T2VXK1krSFJoYU1wZEFGMzFPV20zU1Q1Ym93Ynh2aDEzVVUwQlJhbklVWHd3a2RVbzU3ZWt4aVlBR2RqMFJhSTU1NjE2alhCQy9IaldVK2hNODRYbG9sYTZKYXBjbDlKOHpHWXJJR2VXaDgvdExJd1h2ZFUwemNqUS95UFNPT0QvTDd6VGRZaERudU1FSTczbXQ0emZad29PSEJnVitPQTNVRjFYT05IU0NubGNyZi84WmpYZHpMREdHWUdhcmlKN0FlcStpa1VQNHJneDZ2UVdqbXNPSUMzRGI2cDVhV2NuNlBEU2dIQlRCRnBBSnplTzlBUHltc1VGYzNpdDV4OUR5Y0ptK0N4WVdKWnk2OTY1S3hoVFdtMlE0dG4vd2FPRmxWYWJ6bkI2dnF4aDJvWHMvaEZHanZRSVBmeVJydGt1dlJpQklQL1ZPUWR6dVJwN0h5SzJKMkFQN0RrN3FQNVJSZ2tvbldlWmF3cy9RdGdzU3dHVXVHWlh4RkxNaWExVlkvKzBQQldweDlXdm1LZzlvV0tVWEZGMzFnNHFXL3c3QjAxeXFCYStZU2drSjF2MGZsVXZtNG44a21VS0RDd2ZaK0s4SWx6VXVqdUZrWDI5ZmtuMjVMeHlKOGdKRGcxeG53SWdOSlBwTkE3NTVUM24wVEtXRWU3ekRBdnIvbW54cXpWNzc3dlJWbGphNWNvcGJ5Y01kaDNXdUp2V201Zll3QXYzWTZJRXVqYXFDNG91T1R6ZHpoMmIrbXQ4S0tSZk5USmo1TXRZc3BNV1hqa2Fld1dTRjNOSDI3Y1N0QjZiSTM3SnVpKzVNelJpOXdsbmtRZDVkTWJRYTF6VjF2VEY3VGJGbk5RKzI3UDh3WTlwbG4vb2lYeFZ3eitUTTdYQ0EreXF1WTlBOXdoQnBlSEJDaWtJUDN5ZkNFWG1uNk15UDZuT0lGQ3dpRWNJVEVRQ2I3VmtpM3Yya2djOFZHSDZ4a3V3TGlyTTR0UXdOY2tJVkE0cmxEelVKQ1labENRWVNwNDZzQmd2b29ReHd2eC9uSmJkbzZFOEcwTncrSDhGVkRoWWxQM282ZHk4MDdYamVXejlvWnRhemlTU1htVjFVR1VQdzVCVFVPd1hlVm5vU3M0TnJ1cW1jZGo4L0dzTU1uU2lTY1FzNjJjVTAzWFNqZldKVENJeHdnV1lhcEZ6YVR1TEpuVHFBbGdmdHIxZWhYUGl6enVsdjV4SlpOTnowVzNncFV1aGxpQVh0TEhUTVRmbG9MZkFNNm1oQmZLdXpSeXdlMUV4UEIwcDNlUVh3cVplNVRWb09oY1A2NjhZbjRPelBwVlhSZmVPQk5Ydys2QVJ1VHgvZncwSU9EZ0tHek8xK2IrZmk0aGExdHg0NGx2aFY1SVJVTC9SNENvZ2F6NVhBd2dNOTU3OEhid05aTm93WVhmaU9POWdpUk9sN3RjQlQyQkE1MGFJNGhxWmthNC9mUzdDVXlkbGNXMVp2Y0dOOG5naUUwRXdXbEkxaUdnNFdGZ3pFbU1YOHJqdEdJZTFhLzJLK214V3BpY2VoYk1OdmZjWU04ZHZvRVdpNUNVem42SjhCTHE3Z2Q5dDRiUDVPYlY5V0VDdDdoeEx1SVpQblVSeDRrdS90bHNGdFppRnVCcXlLMzVac0dZZVNRVlhJaTc4M2NKVTU4N2NXRHpJaWFkdUM4RysvNkFqYU1wbFl3ODBhY3E1aG5jTjI4N0FpZTl5aFI1eHJFQmVCaUhHMGRhTW5taGc3ZFBWOVhZdEcyTHpaaDJtVGpuSzdjdkxlanFWRVA4c0lmOUZ6QlhjR1E4c0haNGpIZHplaThmUGkydFA4ZkFpYk1XNXhmTDVhU3VnUW1QdlVKVFIwTGsxdml2WWxKc1MzLytJcG9sUlhIQXozYUNQQkdZSVM5cXN5eHZ1TjJxUG9CTEtQYzZjam50VUFpNUJqZi9nYnpFR1FmaEpBaU5YOWRTb1haMWd5M3lrWnZtTXZnV1dzSitnSE5UTkpEWXZaUWdtZDcrU2Z2SUVBaDRyVlgwQ3RZQzFCbTliMzJJdGVSMUppcjZqRGx0TDVrWVN6dTVIdnoyOSt2TTdwa0pRRklTUFZsVXBFYU9jOGl4OXQvYW04WUhJMHpuRi8wV1NuMzdMYndwMERtbmtqT2dQL2ROUjc1R0w1YXN4bjNPZjJ1Rk5xT1F6Z0hXRW05TndQbEhnd01haERwSG9zK2tKT2lFUjQyTE5vLzBOWGVSdkp6MEJSL0x1bGkxM0dzQ0F2VWtWZnRCYWJ2dWRWNjA3anVmTW9acllGbU1ScmUzWnJ4dnlEeXgxQ08wckVNV2x4S2JFM2NHTnhSZkdkQkZuc2FjV09VOFZLT2UzSTdQR3I5RjhvL1k2T3ZBaE94NzVjaHNvVkdEUFNvTGNPUlcyS1hZYXhlSmZpV3VGaWZscW45VVRKdVBOY2dUZnNiZ3hoOG8ydU04VXd1N1g2L3RKY0FMcjBmUTNuNzdidU80N3V5QjU5SUlBYjI1TG5nNHQzODdHdVVhUDRaM0lvM1kvQzJ1aUk1dTUyMi9QQlp0T0pSd1NHZVJ4eUNpaDlvTkFjbm14dERPd2JLZkRKVEIwTnRhYlFSUjU2bXNpamlkbjFyeFgweG9kVHNRSVh5UUZsdjhVeXdiNnlNcDl6NHVqQjdNMWd2NmV4ME50WmRhN2E2cXByQUcxODF0R09PcExndG1aTm9wODZxTzBxc2JnaXg0RGU2UnNkaDF6a3FIcnpmS2xXQmZZOUMrR0UyeGVna1k4aG5aRURuZ2E3YWRsaGtHQjhxbVJ6ZlFVTlVkaWlSaHo5V0dOS2JCR2c5UG5odzAyRmorMzMxOG5sRlpRQVVYR2NNb3RwK1dwdUY2bnFsNlJSenpZMFdHOGhJaXVwTFV3UDh1Y1hRTmIzaGw1a25QUGIyWTlIS1lRN3puU01kdmRvZkR1cGV0bFd5YlRHUXJUak45VTA0V3ZzZ0w3N3IvRUd5U0VMRmRXUk0wSVlwRHZlZnNVcmVaMyt2OWhZK0hMTitxR3B4NkxYOFMyNndMY3R1YXYrMDZGSWwxV2EwNVl5KzBsTVptajZLZkRBRDNPL3VZaGxON3hNRFdFZVdidHhTQTZycFhERklHN1JWL3NGQkV2UG5CaU1ETi9OcWFWQ3pOaERkdnZLb0Viak05MEJ4UGFDb3ZpdmdzMWJDSzdaajREWHB6NVpiRGxsNWpNclFzdHMydUpsUkZEZWxOK3hqTm1CZnRyL0dsMGNhV3hNNHhEZm53Q284QkJGR0NZelBqaWdTeUxhWEUraHhoK2c2RktnOUFsKzZDdnRCUVgxVWVLa0xKQlpSaHl4Y2ZEM1AxM0c1eWdrL3EwQVUzdEFDYnpHaTNycm14dC9UZW8rTTNoRXdwSjNzdDBQZ2JGMTdiaWVVYVdxa1lPUFQ5TFNORWROeU13SHV2Zm44aTdZUVV6U2hwbEt5N0ZhNWR5WnVOVmNlTjNraGRDN3pTdXJueTFNOXdKMWt6RE4zRlJQcTNXUUdEb3hoZEVlNmkxN1o4MEMzRkVwcnpZbnpqQUR2YUx5L3pabEV3UEZvU2VCODZ4RFNqNUhzS1puckFIZU5rM3hPbnBwbFJ5bTNKbHpNUG5FZ0Vidk1BMFoxU2NvMkRKYUlJQWMzYWRFSWlnbHpHYmZvV0JJVFZlVUM2T0ZPZ2tWRXh2LzlQbEJ6TWwyQ1RHcHJ4VXFycmFnaVRudnVjQnhsZmU2WXNzNGw4d3J2REZWV2RnUDZxekh6VlVDMFIxYVlSS1RBT3Zoa2twclV4UVBrV0Y3R2Y2eVNPbkJBankzYUlTK2tHa0hQQVJNbjRHRXo0a2dKSmU5VEY2bi9XR083UGt2eWY1enZSVE5xZzFSZ1ZmbnhpbEMrK0w4N0RDeWZVeVJveVBTU21UTy83WXBsRzc5eWdrcS9LQlVkUjZSY2pJNGlSVWFmREV1UUM1cEpML1VremlIbmFQbDRKTm80blJPRVVHMFpLODFzckN2ZVM2RExzZGphTFhrNlNnWnNUTlpkRiszS3NGUWc5UVFoNXgwNFFIU3lzeENyVU1RWjRGU21hV2RFL0tVSlplUGlaQUJNbXFZM3d0MUQ2YytUUkZsV1NLOG0yZzVuZ24xZVlYY0Y5WUplaW9WaWJRL2NTVlVzWkQ4VkovNkEzUXlLU3JSZGNjSDR5ZUkxZ0VLUTJBd2NucklpeGt4ampsem44NmFFMFlQa0xQaksvSGY2MHNpbWFOOVJnWTNvTlloZk52R0VVaXhDRm5teU5Na0x4K0EyeFpiN1NRbnlpVTF5aTBrcXkyMW5rYmN1MFUvcEZkYmd0U3VGbHlPMVlWTkJMMlhoUll4OTBpQWFjT1ZYOHFiU0E3anhMblpCS0dVa3REdWU4U0tYWFdFVTRzY2xrRTE3Zm9zWWI2b21EYnIxc3FRaXBlWUFIQ21ldVI1Vkd1angyaHRubzByeHlYdkFRL3FVcS9oYldlYVY0REtsQVVPZ3NRa0hvWm54VklxYlNqcFl2ZUlJMDVDb2ZqV0NaTUk5T3ZNUldzSnh6OUZXaHllbHBqaHUyM3RWOVdUYk9IaHBJNUhvOVVUVUloMjBkUXN6RHpDM0R6VlozQ0hWc1FwbXNHMjNRZzNLM0JUeEc3enFIQ3RXbXpybDJxQlhpTGJiNlV3ZDJDK0M1MHRrb0tKbklORVMrQzlWSDhCbS9zWHhFcG5PM2tsb1JYK2xOZndKNjJydFZ6U3BkbjU2RFJwZ1VXZ0I4SVRZc1FLQzJWV1BPQUVUdlRIZ3RGOWE3a2F6WURIOVo0RzNKQ0lITUYwZXYrZ1dSZzFFM1pwMm1semF1aUkzaWFMUHRQMnhHL0dYS2NiRGkyZEJKcXluUy9sTThnQWg5RXNNMStPYytvVHRkQlZMMWl6SitGQkl3UlJjUkZjVG11WU52SnpjbGV4S0VpUGJUZWEvR0lZaFpLa3NxOTBRd1dBMUhJVVp5RTZ6UzJHZmF6VERNNWJBb3RmVVlrVWxNWlk3UnFnSEFOWVhaN1JWRExNcVJiZWpSdUpFNlpjZWtNSWxKMjU5L011d0tKVEVLOGlJUnhFbklLRHgwdlhZYUNtUTNtQURhclB2MitjVXBFd0NVVUhVN1E2RHBJSU9pSVg3SDk4ZCtyd0c0WG5jZEpydnhHTGpweitMRzZtTVJydnVzRzVmSkJZNi91Z3k5R1B4OEFMYVcvQURjb1Vmc3lIcjhmTHI2MHVudmQvcWNWK0VFWGRLY2c2MVZleFBQcnQxRGdOdlFReHN3TnR5VzIvVUwyVTNGdVlIYnB5WlNtbmxPOHdKTmJCd2pxSWNyUTlFQzcvdXZQVnZBUnRTVEVsSnJvd01VWWFvcjUwN2xTeXlGR2JlaHRpVTdodFM0TVFHU0RMWVhKQkJtR1dMVEh0OGEyUHE1U2tVTFJGTVlaU01heHFESkE0WWNUeTBQelp4ZWdsWTJmaUZSbGtRcUhUNzZxRUNnMWo2TGZzZk1xbDl3eEFwTG1ldkRPZXNYMC9MbE5iVUR6WHhTTFNpcmdQV0UzUG16b2RXaHpnV3Q3bnZxSVpKblBSUHAvU0d1WGVuN0NPQTUva1VZMDNTbW1nWVFQZUpLVVd0UHV1akRuT3I5VUg1Wkp5OTF0MTRoZ05nQ3Z5ejJXWmh6SWN2aXB3aFRMcHQrM2NrLzlRK2dNZkFZVEU0bUV1QVdGNDZJT2lNaXVXdEdzRGhWVDJaN1JvbmZHeDB6dEI2SGg1QTlKc1JrTm9RbWF3Zm9qWVpRd2lFTlB4OXhIT0RQZW52d1NUZ3FkTHlGc0FxbE1ueksreU16Q2hmSFhnMUxpa2t2MXA3UHdIRksyR2dWckV1dXZOZlhJUGlOWVpNM2M1UWU5akVDcFgvSUFVNE54aG4xRUVIY0tCeDZPSU5TTTRaNVM2bzVDejFISkRGOEFQQTJ4S1c3cHdHbU9NRzZESEpNcGdEcHVjNk1LMXRpTVVzTVk1dk8yeUJVdVh2WUFNNFRqa0dmM2d2NXlEUUZZVjFTSC9ET1VzQWNIT0RGMXJJam9lRlo2a3BnMUNKOXpGR0M1RTYvakxjMXRNUUllMm9iYXQwa2hHbmpHS3poSVVFTFhudGliM3M5TDFTd054MUdpYmF3NVg5c3o3UW43VUp5QXhxQ2I1T0JIR0hGMjBoZVpOVG5ReWZ2dmxsekpaWXE4czlnVmpQb202cGpQY1lKVDhoRUJLZkY2RjFFYk5oT0dOY2x3RVUvSFRSMitmcjY2MnprdHVSN1ZWY0JHdnFhNlN1cExpNVpWTGZTc0c4R2s0S1VXYk93T1g5cnFEUTZoQ3MxU096dnJPTXlla256akIrNDZFN3Q3dFU3UzVXRXAvbXlvNWdnMFB5cjh4cllkOGMwMWZVMUFSTjNBdTlncHh4cWx4Rkx0aFJoRUJRdndhaHAwOVZZQVhuM0JsU1ZEK2dnRDlNNEtPTng3WDJVMjNmOExVUTlvcWRIU3JQeXB6MEdrNHhmL3dqMGY5OWFVSWxDclZEZCtDcFYxbXJqVElzWUI5RnVOLzFBZ205b1ZpSzVjWVVIWmc3c1paQUoyLytxenl3NCtqeXpKQ2g1RERFRmMxZk1oSmhWNnZLL3UvRlFjTmppSlBxa0tBQUk0RnRndE92eGV1WVRUa2NrNTkzeFpKRWJCOEhrVHRuZEJYK2ZiTGJpMFhzUUJzNDFUNndSUkxEbllDKzBkVDYyTkplYlAzTzdiUkk3bzVpM1RiK043SWVIMVpnbkJPYU1HS1luSGVWZ28xdFJmVndLdjRvbmkyVldYSVNGbE81N2xHY0JiQnl1ZUVuY08vbnpRTlplVWljM0xYMVZSTXFVZmZoTWtDWHN0Zy9FYUk4a015VWgxZVBRWjdlYUN1WTUrZ3A5NjdLTHNqSFBqU1hUcWNRQWlIUlJKZUpmQ0VWOExIS3VmRlpwcXJRSFVnR29BR09pR29uSEt5SjNyRkdCQkFlVUlMaGFqaThOWXRvMzRHTUhOQlNFUVBjbEZ4NkYvTWgyRzN4cUx0T1YzVlJIWDF3OCtKRnJPeXNmK3YzNEwzL2t0MlVSMWF1aHQrdUxkaHNLTXdRRVRIcW9vNFR0SXlybzczSnhDN2tXOE1CUzdIbXQ2dHlrWGpQMURZeGdyVFBpUlNjZ1gvaU1JcWsybWNKUlp5cHF2U2pTWjFhOGl0K2lCVms4c0pXQko0NU9WU215V01hMUw0eGM0SU1PdEpGeW1RWTdmeDRUSVZMajBlS2hsSXN6U1hEN3BQcTU3dVFVR0ZRNS9XMlFmcEZjaUNnZ0FyYUdDT2tuWk1VTW01cWtZNStpa0p3SjNuS1U4S0xKSG1wTEZQc25KRGNiVVRJNXNhcFR4VzBhUU5KL3h0QnhLQkdpT2xoK3A5SUtwVEFxZkJ3bjh3bWRRS2JubHZOYmxrVkFOaG5oMjB6ZU1pd3JhRU5mSnJKemRTMThFS1dlSTBwVm5BZGhqbENqdTdHcWRkWnhVRmhpL3VIMzJNcWZHdldGdDlPcjR4UXJFaDF5K3lLdVJ4Ny9wWjBzd0JNWnFFNlphajFMc01QV0FmOGZ1ekpBUXZIbFRGSDU0VzRva0Q4cTdzNTF0ZEJTcFNHWXlGTjRBTSt5YzQvQVJnYUVoR1UzdzhMMVpnME5pMUg0MFdSYXczUFc4VGdNejcwT3puZ2ZIbXM2QXVtcE5WcFZUMG9QbXV4NlNkbFB1ZkpXWXNyNC9IMjh2a3JkeUhKUlA0RHU5U2Q5enMzSE9UN2FITnNHQmdySHllc2YyVHRlendWRGZLVUNRbHZpanhERldvN2pNWFF0dC9kYlM3Y2FmMDhZR084emV5eCsvd3JuSzQ3b1M3WFp5NlI5WE5PMlVPVGh0UUtVWjlsOC9kclNveCtXZndyTS9MTTB6Z2xLY0FjWDFnNTBCekFLNG5wSkZBanlNYjh5OHlYS0dGRjVGZXY4L2lQVFBESkhUVnZrQ3hPbmZkQ01ydGdnamJOK3AxN0pyajJoTDV6N0lBTGtxRGFFbXZzMUN3SUxYSUttYk5QUnVFYmtEL200MUhjS0VsR2piVG5FbDhQN2Y3dFJrZjJGQ0xuSCsrYTBTbU95TWZYOVZWbGpMUk84RXYxSjBIN2M5d21wY2VhOWdRZzlDR1dNdVZVa3lJSnowN3YxcHRjNmJVTGgydytPOU53WlRSZFN0YlFpemNNTkRrRkdPa0hGV2JYV0tQblVjNUxFd2VuVFZMTktqOEhWYlRsejNjOFlvdHc3L1FsWml1NWs3bnVOeXFleXNSazlVTG55TXdONkRUbzlKTVFjMnFrcUdlb1hHajgyLzIzalBkRTBPVi9ScFNBSXl5Tmp3ZnlsTXltWnB0QXpMU1Jzb0hqWWgwcVpXSUxxYjUweFErQTRiLzJoMkVneEFmTTh3WEhSTGR2L1gyeVNrR0poR1ZJTksxcHI4bW14U1dsdzdQamhkZTErc3JaeUJoTmFYM2VRK05QMWFqRE5MU2RlQUlJbDRNZGNxWklreCsvR0dWcFhYNkNtZkk3QWx6bmFFS24xWWFDeE94eXMzR0M1OURscmljOXVPUWVZV2FLZkRjKzNFSjZwM3AwaE9qK3VkQjFIUDNYa0FYWUhtRmVjMTU5T1FZYWxtN3U2bWt5cE84UlBpSnBGRFBVQm5BNGhGOHRIZG80Tnc3YzBPcWhFUUprWFFoK2RQdm5VemZHYnhLRi9jZXdmS0lDVy9HSWl0N0M4TW9UU2Fxd0hsaVFHM2x6MHpPOWVhRFdnV2tnbFA3VU0zN1Z3RUwyUFVUalAyWUFuK2piUy9WZnpZTUY4RTFVODY4alkwb3FibHpJaTZTY0FJRDRJYjB3c3dKNUpBbXM4WUtMTlgwZlNwWHZBaG16VnN6dEJORTB1ZmdHdG52bmcyZkJadCs5OHRLQW1ZSDk0eDhJdU1XNEtiUkJiRExQNFA3T3RYai9ubWNabUNMOXNCQUVrenJYZmpqM2lUY0MwN1NhSXN4UGdnbkhXRVljNTc5VTRGYlRPaGNhWTg3dVZBTTh0Wm9ySExiQnpjcWxwTC8xemIwMW1QV3BOaVl4ZnVUTXJCOGh3cjNaUGdzdXgva1lGTXhtUHNPZk1vK3JsOTlvNDc0SnR6eWNxQzJNMGZOaEFNZjBTTmp0cWlZSnlyTGhjK1VETGVBQU9yRUh2cWM1Q2NucUk3MXFaWnBJNWFFblBUdUxnNFdXSThpTHVBcEZGK0hZbWMrc2dtaVJ2T2JGVzRMV2lTc1Uxcnp2cWExbThVaHJzYkpFQkxCSjFOaTRRVjNHckxrT21YWDA2OFl4clNYM2dhOTcza2pRQURDZnpsZTR1bzZWbTJOSVhleFRkazVFeGRRcGZERGsya3BCOWg2SjYrNCsvSGM5bkdvUWp5UHhiQW9mMytTa3ZhTzNGL0NEVWU5dDJZZkJnUVRtM3NFQ09FTElha3JlekQxTHBHL2Y0c1BtTUlJdWJpZkdtbG9OVFZSR0xxMjJyMXowUTQrYUlaZVJLanRpdEJaK2JFM0tlbENEODdFaE5ZVExQdjQ4S24vWEdUN2FJVUp1SEN0WkxzMEJjR1ArODIrVnQzZmlhRTZ0dHZ3ZWtaL1JNWmk4Z1N5TFQ5bEExWUYxTWlRaUVVUElSaVZlVzhmV3BqQmJxWlIrN0J5aHJ1eHlUY1hZUXpsV2xlTndSSlBBUDY5TW0wQkQzWlNKRFlHY2FOSGxMbUZ2dmpIRVozbGVvdjFUdjJHYVlXUUtsWmxUTGhjMXJsSGhSMGlPTURuNktUT0JXei8vaEcxRVNkdVhGRGQ2MUFobGNOMmVZeWlvS3ZDV1NTUjFGMTdpMUd4WDFhZ0JjUVBrWGZQOTVrRVdmYVljSWU1alVpWTB2M1RYV1VKM0ZLU3RjQnpPQUQxT0UxenFaOXF5VFBKRThZOHJHb3VObWhWS1NldGZwbXg0bnFuS3YyKzVGTzhpT21FZ0YrT1V3NXVVVUJ1QTIzM2d4Yk1nckFBTEZPQ25GaXJQNVV6eVJITU5sU1R5VE1jc0VJNFNsYkRnZlBHbXNHcEJCZnhYbU5rUUlpNjJFR201dWhHNHZlOFNzV2o2UUI0aFQyYUNXY1V6UFNCUkxtdXdpTHoxUWI4eGJuU093c0NadkxDcmRycWdlamYrdnBHZThuOVBWSWE5bXhqR2p3MWU5eHF2TTlNNzBlMitaK2NlNnlzVTd3TlpQaWllcFFENTJuSjhEcXZCanEvRWtybXI5R0t0THZpWGJ0VXZzVDBzVVpWVHVsaGpDa1ZnOXZFSHlNMWRTZDJqeC96RThKSEJNVmxQZERPd0hqaXBDUVBKWmpDVzM5K1hjZ1pnZHJuQWZXZFdiV2R3K3l3MkRkVUUyb2FtSW9scmNyRnhnblNpclcxL3ByOFZhaVVvUUVPY2FMTVI5QWFVQmcyRkpFUW9BWnRKb1kyRmJsbUJXSlF0NGxrQXVldTdtbml4RStLSEFlMXJOZ3ZnTC83bWVndUo3R2U0RTlkWmRDcTdacnErSXNiYVdhKzFpZi91Y3p0OUJLRUhLdmZpZjFvTlVTS0MrdnNtNU9kaHY3ZUZGNm5LZ1h3NXFiaUU0NkVBUHVJMVkzL3hEWEQ1WW1nR1V2KzBaaXFlNHgwZktTakpvT040R2V5Y0tibnlldmc0RGM0UjlBQnVwbWNxemNGZ1VxMERPeWJKczlZVVBrMzE1S1JMeTVaYWVYNUJ6ZEVKaHViaExhYVcrVk03Q1hrUzNZbzU0dWVzVXIxUVVENlFsSWtsYTVSQTlyMjF1c2o1b096Wm5abWp6MGh3dGNIVGhtYWJseE1pRk5GT0dXZGtQTkw0N01jVENqUkFtdUpXaXRiVjl5bEY4cFp6SlFxbHppc1REK1lHNjB3YjcvUXRhYXYzMGNrR2V5TUowS1I1dkhUQjlJbVp0T2pzMHoyNXk0TTgxSFFSSnZ0N0tGVzU5VUx6b3hLUlhQNHJVenZnOHRvaGp1REZkcGJyYUtHZnZZTjFYWCtqQTBCVGFaWVNnc2JUaTc5L3FpLzN6azJzVWRZYjhOaGhoRVlQYVozeFByTU41bzUwenV1M0FDWGtvelFpV1FlSUVlT2tZYUQ5UzYwZGpUZW5mSitjQThmaHMyaGk3dW8vUGY2ZjdueWhJSTF5Vy90UzZXMFJLWTNPalBISHNkNXBCakVMV1FGcUFVTE1XZ044SzVndDRxV3BsaFMyOVpiR2x6VnFkc00rSjh2YnBhV250cUdaYk9rQXRtbUZHdldWdzF1MWZ6N2lVTndxaUhOTTdTcCtYYStRdkJLcFNXSkFESlc1TmQ1ZzNxMlUxS3ptNVJwclBGS0N3Y2ViMEtLWEZRbENWT25DZG9Dd2NmSXNiV3YyZmxGb0VWTGtKdlgvRmhIM2I5Vnl5UTd3MmhNaHcxc2dvQlhTTzI4Qm5LS3JaOHdOdHQwNkNFdmxJYjdoSEVLRWUyM0UzeitsMERySjJwVFl6WjhRZnFJTllrbXBsZDFhNG04K0hibGU2MWpUY1Y2TTYxWkdCYjltL2ZSUFVna0NoUk44TnpVZ0FQRTZyMkdWZSt0VGlvWWkrQS9oYzRTVVlnZGJWdnMyOVN6d2hsZXk0YkdhNlI0UHlGaVRteTdIMjNLcUx1cVRVTjFxOFBPNXhKQ0JhSjRRbjZMeVFCN3dYa1ZZcWJNVXZWUnZlaFRQaFZWUS8weE1aOFRHa00rdWRuN0RmWVEwb3c2SzhVdjhZbEdXN3p2ZzZ4NGZkQUFOY05ZbGp4a25Iemw3aXhGRlppUlJYaUpkWVNpWTgzYVlycFJ4dkNlMFN1b1RLc3VqTDFNNHlVa1hwWXJGZVNnZ2ZGTUpTOUtGVzVINitzS0d4VWY4R1hLZHp6bDZUUDFwMTNzR2pSV3pMMldobUlvSEpUaW4vZFo4dmE1cnBTYmpURk9ldC8rQ29ITWIzTXJTdGJlc2FKcGluNkJFZGI5RVpTbm1ZcC9FTFdRcnNCbXJYNW5xMEQ2NGtwUEIxNHJvTG5nd3gvbTd5NHlWalVnYWFWcE9Vb003d0VPelVzN0ZqOG9VdklWdkpyb2U1TUxZbSt5dy8rWkI4QXJuYXZYczdPK09Ea3hHakppQVY3VG5PdTMvekI3QXFMMHJ5bGN2a2pTVXNmVGhkOEh3TFRvNFlvbWJ1MkFRUUZlQmdBS055aDVLN0tHL0hFSnBJZlhlS3Y4cDc5emtnTm1NZkdWay9hWUdQWUxNWFUvRzNGNXVJQTQwaDRYcXRIVDF6UHF5NGR6Q0tKUW9FRVVuZU9NWjhYaDczbUNIcEF2Z3RSSFVObitMZ1dNL3lwd1BaeDRhMUtnRjNOSlZBY29vVnd0Y295Mi81MmY5UUtnMWZEOEpZcTU3ckxBNTQvdXIwb3FJQi9iZ0hKMjFoQ1YxZ1JJR09kNHRZTVRpbjZmMnhiVVJiQUI0S0l0S3NFd21IWXlEVVNEYW5MNmFBS1V0Wmx2QWdwaWs3djJqRkxJVTJhWjdYS1NTRSt0ajk0VmFuTlVHdDVUL3htaFEvdjlJYlFnWmh5YnFaNjNieW5lZnZvbVJ4SUhoSG1sTllSMEp4Zi9pakhGemR3VXR2T0RmR25pRG5XSm9RYll3ZDJJOXBmZC9rYWs1d2RWUkUrU0dyK1N6N0VhVUN2MzNyK21OZ05PZkI1eWJWZWg2OUZwaURvRnpJWWF1TFBXWHlwVGl2N2R4K1hiVWpUdmNxZEpraXZXU3I3ZG1tbHQ4bGJiZFJ0cGkrdUV2d3U3TlJUS0xsNjZhaE9nZkxaUldXbk1DM2s2WUZVcWZSdm93bUVKbVBrTlB2SUVMLzhvUTd0VmNrVHoyWGViSFFUNVgySzIrRWVTTUFqTWdPa1M2dlluNDBlLzFUanZONUxLRyswZVZKWmE3US9wVzVoc0lsMk5KQmdsLytVRmFEUVVpR0pxbFZlWHVnY25wLzNXb2Z0TlJoS3hmWFV5ZUE2azUrMmlXUUwzN1JWRWl0ZjVFWWlFaS9JZmJGMGs5NUlyMEpTUWFLVkpxbklucDkyZHZvMTgrTkEweG1BMXZ0TjRoTnIvTzUrZVlOT2F6ekpmcUo5RVNCTDhLTUNoenc0OThSdWVESGRTQWhqWWhDaUdVa1MrY1dnMWJaQWFuSzZMeW5HSHNhaUp5MW5OWEZaaHUrazR5ZHQ1VmY4S1FiTGJJUytLcjZjbk1vMjY2M2ZTWkNnbUh3OXhUbWlmSHFLczd3aEFRNmM4eUF0U2ZXVXdkSXdNaUhJK3BYWkpuaitSV0YzelpYOThYREpGbTJHc3lBRVdmV3cyQmsrUDJJMnRMUlNka2FZbHU0eStQdWRjZXJZdXlEZDJpOG5DbVZhcnZ3dWtrclJLa00rK0E0TzVaVUYyOTN3SjBqMUk2cER3dXg4TUg4UVNHWnM2Q3YvWGVtc1JZaEs2bjZPZlFDUHVRQ0c0Um13ODB5SXliQVNtckVCbldQbnVYOXdiZDhGTm1XQ0ZSNWJWdEdGVWNrWGYzSEwzS216bGxNNUtmY05JZUNyNEd6c0xoNE9uYkR5ekttanAvNkdra0NZak9GanQ0MWYwK2ZSVVpmSjV3bWdyOVNHQXBBQVNXbUF5UUU4bnpwek9GYjl6bmUzQ29TZ1dQSHZLWjFSaHFKd0Y3cVgzblVqQm5iTS8rNElLNEF4VFdENU5qME1rN1AwZUpsd3NRZFVLTG9mbnBhdEdBUWJRZ1B0S3ZxOWFjQjNjd2k1UnhmbGUrMlZjblpQVWJyY25HRGt3M2txb1B2dzl0QTdaYVUxTkl1MFpOVzJWQ250MmRVYUFxUVhxSkJKY0lNV21xcGtuVWpDYmRZcEluL0tqOGVYL1liSTVWVkRQVWRVZWlVcXp2bC9UU0Q1cVo1NTNURmQxdXBScFdkUVJHM2szQkN6Y2xUR1pMaXVqRlJzMFQ5c2xydFg0NHczMzdsUERmbHpoQWVHU1Q2Ykx3dmsvZGgxNDU2ZHdrd3l6bC9iZWZXbTdtMGJTckF4dFJ4QzQ3YkdtbmRpbjdrclRyU2dFbzBDVFBKNCtWbGYvbUFUbzRNdmo3V3FFL0drWHVzOCtHNkZkclZpVGQwMmsvcEFLNEVXUldCb3VZd0lYUHVyMC9mTnlhUTJiR0dzSXVtQjAwNy8wclE4ZWtSRU83Z2lGVFJlM0hwWmtyb2xGMjFKS0dRVDVBZGIyNHFKWmJOWE5vYlpEMVNaOFgwcWF3cGlSV1g3U2t3UndmWGQ4NkpRZkVSZGFWR1ZVSnpEbE4wRTdWY3pGNW9hUXVWMklzNWkwQXQyK3FoUHhPTHoxT3RqeDhxd0x1dkNnL2UvenZHVlhuZmduMVBtZlV2ZWlvNTBLS2ZtQVlSUjBpSWIzci9tMURSZERSRjQxNWUrS1cvYmtjbGRRYUdXd1FjQmVzVkFhT3l0Wm90ZStuNXZ3cTJuWHVRcUVhb1RWMUpFb25iS2xONjV5YkU4a3hJbWtwSUc3eHZRYTJMakpaRXpFWVA0T1ZQU2xxQlYrdFBObmFCVWl2OXRxL3E3dFd5WlBXcDV2S1VLK09pVzNDV3hWd2JhSlVNS2hzTG1LRk1iVkt0cUFpMzFGV25Wc3BGZFVIblVpanFoOGhFV0JqdWdxQm8rT3RTUDBqZ0FTNmYrZjNocVpzSnNTSXlMUFVYbmhUZW9EbDV5TzBwc0MzbDMxUUpzZDJVdXBPMXIvMG1pZVJIajI2L0RNckd1VjN5QjhPSmhYNEhJK05uZUVWbUJyWDhlTHRyU0U5Mm1OUk9lQzVvU0lmZDBiRDhOODFLN3g0bTB5ais2TDJSbWVSZHJPeEx4UitUeGdpVkRtQjZVNCtNMzMvdW0zdXEvTWpMNXFsakh4TjFqS1hickdjWExoNkpCenRrSGZxVUkwWEZhOVdYeE1zRkxVcEV0b09CK0ZFNjRQekpzV2QxbC9zSGVOcUYwU3d3dzB5cnJRV3V2TkxRcXhBcjZDVTg0YUU0bDU4eWJDYWRkVnZFVHZZSEM5VUk3U2lGM1cxL3YyMzNaUUtQY3NQVk5ycml3SmhBUkphZ3g3bVdDUUMxZnpJUFk2YVlPQ3pyaTZBNU9PNTArK3l0L2JQWHRlclVzNnYwM0RRaGRXQTlzUU5zNEZ6eXg2QnJYSTVjb3lSUGtpQVhSSjFRNFRwcFdBRWp6OVBBSXlCZU1ab2ZQQnp4VSs5K1VLQVF1TW5NNzBoSjR3K0FJM1pnOEx0RFhCWlRVWFo3eTBaRHRnakNpeXZLMlpSd2EyWUJsbUdJaTFrUHBGcHViOWFoWnUyTWhyU2xyemhrMDFiSWpneC85T0dpWmQ2Z042SVpRcjROYUlSUjhlODdYeUFvaHo5RmR0Q3JwOVliN3dYdDdzcm5MSW8wbDcvQjViUFpIelNGK2hWbWdHTEkrdFQzWE54Qm5TVWY1aFRMNDE1SEFmODh3cGpGL0tUQml1VGVmYm42S3VZM2dsd2ZzZEhqRU40NXA1bzB5RkhadUhXNVVUaUFqVHYzSnB6bjZ1RWJPZEZOb1JJV0RwdlhENEdEN2I5L2ErWm94OEJMbll2RDRKODFqUmJrVWV2RGEwUWRnVk92M20rVkZleDYyZjdqN2I2a0hlMXIzRFpZVGoyRitsMkpXT2p1YUlqYi9JcnN4MnhIanpMVHJYeTlYTWlST2l6amIzSlV6WHJ4dXZ6MUl6V2R1eDFqT29iZVhTa0ljTE5mL05KcHFuQThjcUJJejlLa1gyVlA5UlRWSmpZL0RqeG1ZN1FTcVVmY2VabnNHdTRTRThjUUdyaWI4SnVVemFoK3lnWk85dGwyOStkeHVhbmxTcmJTYlBKOXdibU9Ea0xRQjk0NGxvRTYwMWQxU21LMXNGZDB6NWduNXRiOTZHdGkzNlpJQlJqQzRQTXR5dDJUbzlDYkpocktLRUFqSmg0a00wR0lyUHc0N3lOK0tYelJyZUhBeUlMZDJkamE3SEdWQ3FDK3VJcmVsUUdWb3dhaU15TzFtT05wV3IvNlRGaHFzbXFrQVcwVVhCR0VmL0tsUmNLdWtGc2RIbUkydWZ0a1pXNmd6VmR3bk9CYUkvanI2NXM0b0UzaG1iNklSOXlNdkFDWnA2R21zczl4NjY4Z084M0ZIV214OXVmQUcxd3MyYUt4YmsxTUd0VkJtd3Y5MDRvQ21odWlwTndvS0tsMG91bkJNT0hmY2JFK016RXpDY2Rhdjl0SS9KeitFYVArSm9JTCtyNGpldmdmQVdkdmZJUUN1VHg4SmZmYjFOWGtJWXdxV2ZnQlBqSlc3T2FMWEVvWXB1UFZIb3IvUTVxeWtCbjhvNklORnRUQkx2dDIzWDA1dFIvKzVEMm1BMW9paHY5eTRQa2lyUk8rVDVOMUlBNWFVa05DR05MT0JwSXlQbml4TTd0Z2hGVmZyTnZCWWhYQUlXZGthdm1EdHZIV2g5QUFjbjhkcTFhM3cvK1hTM1JnRERwekhmdmFvaG4yRUljcWdEMk96bzhZcTA5KzlEQWd4KzR5YkF0czlJdUMranRpM3NuZGF6bGZCSldXTU5Zb3VMNk1sY0NYMXEvK2FwMDBiUW40WEloaS85ZFlVak11TWpDRFZKQWFWV3B0QW9TMWowcFQ4WUVzKzdUaVZrbmxBQmF3ZDRGYXI0SG1CcjVtYTZPV0UvOEx2TmQ3RURXay9xWXQ3WC83aUM4eXNrSVI2V2ZxcWdBbWRmWkNaejE3RHMrU3IySi85bE1Cd1JQaG9WSkUxTEk4Nm5ZYjBQNzhNKzdWNWNUTkJsRkMrdlZWaTlhd2FqT1dubkFyekFPQWQzN21OQ3FPWGZGdXh1YTFJSVUrcHVGajR4ODIrVEdDdDUrYlhZZm1rODdCSFdjYXJEL3ROOFpGUGNnTWduc25ZY0wzeEVlRWxUZFd0WEFEZGZzVmgyVmR3djY1OUFDdnI1Q1pySi9FdjdhU2E5ZWMwaTRFbW5oNHJTbzlmaHlINitQVGJvaW45VnVUSlRlZzcvS1lFYnFRZW5iZ2FSRzY2QW9PSkNOSENCWi9nMHFxMWRtRW5zTkZsb05ZSGpkUEJZNFRiSW03bDBodGRyTjN3aG1STGFzM2ovR2VkZXRweVpjRWZKVVB2YXFKWkZ0elV4U1ZUUkRBMGJSeDJibzE2NmVwRVdHQUo1S2JJWFU4cjh0S29rOEhFMDZ6R1NtZHRidlNZRXBjejE0UGtHRHhWNlZpT3pqc3ZhcDZHdkltRDh0aERKQ1hjZVpJSi84ZC9jd1NjRDdkMS9RRTUxdUYxRE9FeHZlMUxFcFJORno0QzNRVGVqYm52K3pXR0lRMEJIWG4xc1NiVFFKYU5RMExTTFg0RXdrcWlXYXhvQWRzODk1U21TSW8yMkIrUEN0QXk5ZGQrazcvZUZuWGNyMkhJNkx2Ly8vdVdqeWxBQXZjSC83RlV2bW56L3N3UWwxVU90enk3a09IUldDNFpxbHJQVlVCZkU1Zm9oamdWQXVDa0ZDbDk1QUN6NUlqY1NXRDEzeE9Td0hYbkpZaFBTakhIL2hiNWliWXBiZWlzcTVBbTJWK295aFlUcWVmWXArV2RNWlJpUGNRNitZREd2OFlML2s4Q0orcnN3ZUd3bGxIK1BuUFNGNVpiUjFkS0RxZWhHMkxYK1hGTGFMUjZRMkFvRjdFL00wZDlWT1F1YnNxV29QUC9tdDh3blBZM2hiOE1uODdHQjB2WnRxanIxM3RLK0FTSkRQdGlhUnEyc3BPRGhySk5PUjR4YjJCSlhudlZjMVhja3M1N2ozKzd2S1Y5enpCd2x3ZGZxMVR5K1lpMTFXRlc0VUZ0V3ZKTndaTno4dEkyU0VuTy9halo3ZXhMdG9qRkdnVGlDeXJ2azZpVk5GNjZQNGpPVXdUalJVWHpwTW54UzB6bHhuNHBPRGc5c2NKSGZpY2RBYlZ4anJjZVkxTW9QSzlsdkpHblc0dENuME9neFFOTitxZTQxMWJKK2FlYUNjbmxHKzhJcWlzQzVDdkVVNHJNTVduRkxyYy9obmc5ZWZqLzdSSVR0cU5UQU50TFZrMXQ1M25kQkg3RTVyc1d2QU1vZW1hTEFuQzlweDBHbVdiOWZ6SGpIVTRjdUNXdHByaGNDZW1HU2hCTTBFMHRiQW5hQWdoRDlvT2RyWktMZno4U21sMGJITjRZRmNqdTdmdi96MXlaODRIajF5UjNNQlB4UGkrRnZZMlZGd3dKNHJOL254WE5ZOHBITDdMMng1aENBbVdnUVprdytPb2E0ZERwcml5b1I1TVF2QVM0RFQ5dTRCU2ExT2dBRUxmbDJvamVnczNpTU4yU2paSHR0bU5GcEpRejJzSmUwOUc4ZjVkTXo1NlFjeG8xUUYvTFEvaWNRZ25vVDJ2RFdSSkNZU29tTXRaNVJDMHBvUVc5aVl2RVAwVWhNMmUwdnlRa1B2NS91N000YTJpbndpVlBRVkFYMmg4WmFtRUMzQ2lVbUNldEZSa3N2QTVBeUxkRkVETmFmeUZoUDVWejBCbXRwTnJiZzJuRmFwM2lWWFVpbGZKR2hXaUQzVytwL0EzRjNpUVlUNkNpdUpEWDhDKys3ZUdjb0RmSGc0SkJkK1lyT1ZhTDFrc2NhZC9vNUZlalNZcGF5K3JEa1ZBN05yVWhlaFJzYlEzd2FRcndmWDhzYlFjSUJDYTN3ZDltbnI5WkRzZnA2ellQTTQ3WlRmTmFNalhjdW5vcnVTU3ZLOEZGZ3d1ZUJtdU53QkZIeXVEdS9rN0d1ZTU0Wk1kVndjZ2NOaGY4ZHlGdENZVVlpUTA5emcrVU5YaDV3ZEc1eEJzRVhCYVh6a1VxM1V3TlJpT1lLNXZPbjhDSERIRlg2MEg2M1dydjFpWXExMVpKQVFqL3d1MEZNZ3ByKzhCNm9ocWwzdVFKMmFCSVlVRHo1dUJ6WGpha1pHWGhqRnlLOGJoN0ZabDRkWHdaMVR6b1dmWTQwSTFZNG95Q3dUamRJZ25FTEE0Rjh4YTlUUENBQjhFY3ljenV3NVB5UGp4UDlydWR3M0srbHl4SXRoQTFuRTZ1bmtQcFFuRTlOTmtiVXBQbXFLdnQ1OUQxWmo0SEUyYUtIUUVIWU45SFRHbWZpN0VYMGplM0FxRjlZOEo5SmpFNi9QdjV6UjBwcUxqdVNwdG85RUM5eU8xQ3lRL2dSZml5aUhsR3RWWlluSXJzN3NXbnhmVFVSZDIrUXFGS2dnSXg4OWdGd2VOVHJoREFldmRML2Z3YS80aUNZNCsrNEtQUW9rK1Z6T3Evb2tObXJ4ckFZMGF2TlF1SWNRVzd4aUlDTlZJanZVazdram1obGpGK2U5UUgyZXlDZ09pQlNnTHV1ekdiYVIybUpMa1RvTU4zZm95UHprWUNGRUVGKzhsNWhHVVpwc0VkN2VmRWpGckxNY1FuR0tGNU41Nm5hbXFzRjhvTVd6T2RaSVlERHh3VXJpemx6YVRTNnVzM0tCckxjUGtXTG5SMFc5ZlNySkhLQnMxc290aEwrU09JOHVXZk14a3hacndUOFN6aVZIMlpqR1lSYXI4YXRIOEFBcHB1cDBoMDNEMlhyM3J0ZkwrTXpOZzZpTWljdWNTb2U5c0tpdDljbTk5MDRQZ3Y3OW5DRFh1UXdZQUYwL2Y0cWQyZ1ZHRk5yc2hwMjBNd3lBeFN2QytPUHVXOWJ3cThSYjVEU3lSMTRmMUwyb1JJMjB1dzNBYmJYaTNKdVRpSFlLNlBSTHFSc0Z4dW85N1hHVmEvWmJGMkExQTRlZzNxNjdhZzMzM1grY1JOSDFrR1F5SzhTRDVRWFA0dzVCb1cxQjBQUzJyTUxkcXdCQ0R2KzM0aWdCbHVLSlpUVTgwNXc1T09mR1ZLTmxUN3JrUUtqQndHTWd4MklhNjRkRDlDK3FYVFRRTHQ2RUFtemg2Mml3em1mY1oyalo2OWM0M0I3YU9BOEdNaVVrWVRwU1hWekFsVFhlNncrTGxWK3pmWlBrNlNSc3lHdERtMVdvdHdaSE9KSTEvTE5qZGxzenIyR0E3dC9CdGFjOWg0TUh1NUJ0L21LMnhFMGxaTUxtUlVIaUphVTBRN3RRbWEzTzJ3OVl6ZzVxTzhBNkt6N0t3WHdOK2FybkJHdGxLeVhuNThOeHNDL01aQkp0RHJ5SHJZL01MYm81UjQ3QUdmQ1NyS2kzcko0emlzZm5hUExobjB4QzB1Z3I4MHg0dXlFR0tsQ3AxOHlFU2lHT3k4QWpOWHNoZEQ2M21GeUF5L2NmazBYUjZCTGk4R2dRZG9ocVFTd0M1b2I3Z1ZQQ2VzZzNnR1BQcnpybEMvaXJ4bkNHYUVYbG5ZSnl2QXNEQkRnYVIrZ1d5TTE4UlpsRU51cHZiQy9vR25rb0JOU2hMRUMwaDZnQVVHRlZPdTl5MUpxR0wxU3M1cFdxbmEwZVJTL1FleFpSN3QrSXB4Vi9BS0NmNXNha2VyeE9DTGdtYXErU2o2SlFHTS9rRnRvMFFZcEh6QkdjR3VObG9hZlNOT0VTdm9JUGt6aWV5eFQ3Ujc0Wlg2ZnRjSHBiMnB4a3VrcUp3OFBwYzExdy96SUhYWTU0SkxRS3ZDVU0vRW1KSVREUkxpczZVL1NCSmx1TmdsRXpjSkhDUjg5N0l5aE83NVZ4S05pbnNSNFplbGtsY2ZUNFlMaWxKWGt6TzhFNG1raTRkZkZOd3dkaGdlRnVVdTNtSnZMcWNuOVdnNVcyUEdtYkU0Wkp1d3paNTJUWkplRTdQY2lsVkhFTzdjOUZlcjNmazRQN0oyWlhoQ1ZKelRLL3BlWC9WSnUrZEwvdVZqYTZ4eDYva3prQXZ5MVhiV1lENGZBVVo1NHN5Ymw0UXNRcEVwdm9qazBFOTYzdG9QdCtJM29YdDM1UnN1UUc5MzdNY0VzNUZ2ZXpwZzVHQXhkZzIyakVvOGF2eURrQk80eCtHWEFIZnRacDk0eXVnMFhIdmFzMnYwa2d1dlRzN2xPVStkVXR6RVptc0pZYUI0Y0R5L1FmeUtidEVaNkQzbkxZdGxIVWdiZzhDYU5BT3IzLzQ0dzZjSjNITk1uZXRhNGlrUjI5SEpoRlo5Z2V5czhSaGNldlJWSHREL1lMVXAraW1oMmNyQjllRzBOU0czc254RC9JMVB2REwrQ0VGaGZLcE1CNVpWQzJQNXNodElmb05VZ3AzQzRlMlp1NHl5NFRGMmF5cWVReWVUQmdEbCtVeHRGU05YNnRhWXVUUFdsRzlteE9NM1pyUmE5UkZ6b1VrTTY4MTVqWGZoaFdUZHhPQTV3RTVESGpIVGNwK05Xc0Rva29JdTRUWVY2c0ZFZU5VZnRmUGJma2tZanEwRi9TNDhUVEFTSEtaaXBpVWdLWVJ5YVlLMWtZZERsYzZVMlZNbk9aak9Lek9YUmxEWU9RZWt3SUZhaUJONnVMTHVoeTFrc0xlTFJoak9DOTcwSGl5N0x3MXhVVnBxdTkzR2pCOGdwZUJtZHl4UE10RmxiTGpDa3diSUNTSXBxQVZMRlplSW93VGZJeVE2djlZalpnZHpRb2pTWnBOc0toQ0RXU1NqRVdiSVhvLzNzNGc1a3VUVnpDcXAwUEhKaExGOU1ZeVBCZFY5Yys1eXhsMENuTDhxYXVDdTFyQzE0TkVncnhMLytUQzdYNG50dXlDTjhOazZ6NDFQRmZqczFnVEpldy96SHBqdmM2L2gvTENIdGlCeFoxS1Zadzh2V1oyUW1TQ0tLUjE0Mm1XdlVKMDh3dXBqdWdvTzA3Q21MdXR0ZFRUSTZ0SFppUURpMitRNWhHNWtJM09FMENYSmxmdEwrOVY2Y0l0bmdPL2h2bjErbnMxM3IvUHU5N2F4Q0NkMGFCemxQdnQxOFdXZVE1OC9YNjFKZ3ZhTDVwSlVVVDdtZXlXTDNWZDJINDFtRm9Zai9tV2VVUEx4QnBlRnlOMzI3NnhzdTVhejF5OVNoSjBzQWxQMWhJVm43N3N3K0d3TlA3SEpRYS9GYkpHNkRLd1BGaVQvc1hkR3k4ZU95VnVJYll4emJXamVLbWNkRWdVY2VxamhweVgvQnJ0Mks4U3NXckcveEhBQTgvOXdWN3dnWGl6ZEtnMnNsS0RzU3JaaHZJbHZSQWpJNXVTeEhSWmhTSXkxR2xyb29TNzZPWHNNUXg1YW9NeUlqalUwaWx1emRCV2lrcFlVazlvKys2NjMxVEtpQVRiNGRmc3A3UUJoSWRVMGMrZTZBa0toOEh2ZFIzeUFMenZOVVZUN1VleTFrN2NmcVpDNk93NExqUmdVYi9od3lYNmY1YkxGbHdUYlJQT3FRdExmdFljaU5aYW9IYTYyMzJUOHdEbnJUNzQvTHErT1daYWdYRVdBOGVicklwODIwa2I5WjhwSW5KYUVDUUcrWi9YeWgzSk11V3lZTENWemJZZjlHUE5ZdmYrOEZHbFE3ZFlxSE5DNmFtN0M2K2V5OVFwQlhBdDFUaXpSQVYxRmRIWnplM3U0OTlkY2lPdXBJNnFuZWZDdVlGRHNLVkViSURoRldiUTFjV1dNeEc0SFNJSk50TllsRy9uZlBMcjVNZ3ZubW52U2d5UUxwV3ZMS3gxL3ppRnBDZ1ZQb3FtOGxpUmRBSGZ5MnR1ak45VklGa2lQOU9qVHZmQzVJempNZVZBNmpGVms5T2lCVDU0eFBqKzY0RGl2SitHZDZCbkhuSkp5ZkEzUDBIQ2FvSUtra0sxcjFkYjFpVFlaQzVKY3RJREMrb2ttaXlaY0lpYUNXaDNybUNWR0VpcHFHTGEzUFJJL3FJSmJDdk02RnVFUFhZYTZZQ0JjbFdkcDNJSnErV2dZc2t3bW95REJGNm4vUC9pbFh1NXc0K3hyRlRWWWszN3dkZ3d3cVROamVSMVpJQ25jWTRNK2xGMGF4U0t2R2YzQXVQRSsvZDdrSjVraWU4UW83bkpFeEJoS04zQlpBcEJxRm9OVFJhcmxrQmkvVHFSay9EWlJhVVBsSEYzUU9veW96MTdEYlEwV1hvQWNwR2N3S3RoUnk3QkwyTHEyOHo0dnkrdlBobmFWdjZaK3VPeUd1TDFXeUpQNldPTGs5NVk2NjFHUzA3VlNDTjZ0ZFRiSkgzell0ckU3eUJNMTVHS3R6Mytrd05Ka0QxWHpPOW8ycFByNG1UMzYwM0xabjl1MHI0eHVObXpCVXo5OTNwWEJPS3RFQzJwRyswc0xoM0VIdWNqNGEwTWp2WG04RVpBR1ZoeForVXBVY3JCVjU4NU1jL2hzUmJXaENtbHZsTDEydXNKYkdwNU9sMjdmd1F4VVhOTmRwS1NqbnlnZkwxd0FOL29NcmpRcmtxQjJhamlRbjN3R0ZXUGpOUzFETEIyakhkVldrVFBnN3RwaHlINzRtYVZSV0o5cEVWNHllVzh0RUhwQXdsUmZNZ2NBdFJmRFdkcVhTWTBtcFYxaUdZVnNLQmc0VDVNN29tdzBhQVRsc3dtYzZnOUszOGNRR0dndFhmY1JFU0g3UWU1T2VBOGdYMWpBaVl1RjFtRFljeFZzQWd1WTRzWlRTVkhubkNGZHZCaG1hcWxJeDlmYk0rR3JrQlhFU1ErNSs1RDJtaE5SbGgrMm1TUzZsc0lGNWtpVlRHMFFFMXVSUUF3QXl0cEY1UkpOandKYUEyN0lWODA2bk5odXhxWDJqY1RVYmxOOFFYSUQxckxaWVEyUWFpS09DZjJlOWNHUnNZWU05MkN4Ym4yQ0N6WUg3aDVTWklqQkhUcnZBTmR6MGxqa0xRVm9EMkZ4a253QnU1OWF0N0x3M0R5eXNDM3FRUCtENHl1cGk3aDhzT1NFTnBFZGIwTzRxbWl3cnhXUE9RV2dNQXJHNklqVUkxK241dHhZRzc0bXJvZVRON0x3YlhwVlhGaWFwMDBLeTJxN29ob3NWZk9OWWNJZ3VwbmprNERReGFTVUViT2hsQUdPYTZOR3RSSWdtMVhUa0FxdktlQ3pKRFM3dXNxYm5FeU5qR2wvLzJ5Y0NOM3lPR05aUFRiVjRmY2M0WW11Qzhvd2R3dDR6QUFRcXdpTXlUYi9HY0FmemIxa2IxRUtlYlptTEt2RkcrNzNkVWNzNlVrdjV5QXZVRFNmcXNqQnF3djVJSklYUkpXSXNmWkwrZzlkR3l0V2psZTUyd2o0ZGFrZFlYU1NFRWFrZVlDeFM5K04wb2hEaWhNUitKWlZLQzEzeG9yQi9BRFhqWkFOOExzLzNnckkyY2wzcjB3eUZCd1kreVVDeEhwT3JKaVkwckJnUTBrSnkzcTRsaWQzSGdpQm5wUlVDZklVWHFnVkRldVhDOGhNNlhubHY5Y1k1Y3AxWFpaeVlRa1ExZmtzQ0xjVkZ1V2YzSGdidWJjQlo4R0h4MUJlZGsxSFg2b1B1UTA3MGpwdkxLNGhIdGEvdkUzNXhsTU1VTG93WHhlUUx4NXRHUXJZcU8wOHY5dkxSY1k5d1hTTVV4VTJiV3pUaEtzd095OGdqL25OUjk4NjZBcVlDcU52U2h0Y1BRenVCWDBWWWJINGVWMFIzcksrZ21UOS9rZzkrc1M4K1BHazdDQ0w0eHhFMEZBNVR4Uk5QRVZDV0xIUUFTNHV3MjhBc0ZGUlJoc1lEeWRLY0lMSEd5Y2lEYUxQT1dnRjlrdUtyRXBqR0hGSEQ1UENrUTBiTEUzcUxzeFhHV3Q2WkxRWkVPU2JIYW1CaXZHaUorN2hBdDVLenBoMjRVNHRCRW5XTy9UVWk3b0d1WkRrZjM4RGNWZW5uUUU0QnVyeHhzTVM0bStQdHQwTEltYW5FODlkWnhSMHE4cFJwQUh6bVk1WTByNWJzaTlXbWJBR2ZxVmV3T2MveVF5WENXY2dZaGhkWGF6MXpoZk1BUnZ6N1Q1QXcxUHl6cWFZNnhTQXBLWEtBbkdQbzZkLzVxbi9Ucm5GWUdqSTVCd1JOeXB1UGVJYXEyWEFBRGJUQm9GZDJacms5cktFSHRXVlB3NTBoTW9IbUFDZ2JhNktiZTdIdzhNL3ljK2xQZDVTd2EwTmRzNUVqWHI3OElhd1I3bUluM1FzOG5ISzZGRXlUZXpLcGE4K0Jtcm9GNWJEdnc2aDhiWCtjRXVGWEN1T2cybFhKdVpQam9wdzhFN2l6RzZ2RUxZYm5aVGFJc0dyUFU3Nm9lZVFXcDN2bm8wMXREaHF4d2txYUpKRXYzY0czTjR0Mnp0SjAyTHd6bFdHVkExS1d0RGN4a2tXSHBTS0NPcXo5QkYwNzduYjlMTFI4ZlIrOTVQd2VoVXFNVnBIeXNqczYwZUJ6ZHNSWFQrQjJlKzAyYmIvbjFXZUlFcHA0cE4vVnRlRWdrbTdyd21DK1JPN2Y4NWM2U000NnF6bjJZWmMyUGxmKzdsQ0dnVlJzUnk2bnA3MUdUVmYwUVJGQ1NlampOTXdJMjUxdnY5UWIreXBuNlNCNm8wVEsvSDVXWStnRkgvN3B0U1FJTVI0M3ZRRmtaTUwvbTdkeHg3cUY2cDAxVDB3Unord3gwMVpwa1RpNnlwQVdKKzl3V0Ewa21Ea2RqN096T3lYejJHak1SeUN3djg4OXVlaEhrWnc4ekt5V3pYbjdXQ3hOc2dkZ2lvc3NOdUZKOUVWRWtJVC85YTV3RGwvMEg2M0wvalE1WVFBQjNoSGE2a0ROV0lKMXc3aUtRWi94VXJEYUs1MElPVVdIU0ZnckNRVkhRck84UXF3Q2NVSFJST2k3TisraU56MnhwbEZPdUxaek12UFZYSFVvNG12eWVQS0tFcjBxQjNmWnJhRExhRG5EekJWYWoxcm9xSnlBTlB4ME5WdHU1am92clVXeVVpdHUwN1ZMZ0ZVM01yTjQxdU9nb2pNVG8zZW9sc2ZSTmhtNnBicWEvNnBxRkdpZ09wWGE5WXFURWRCdWJUUCtHOEtXa0RTdWx0VmltRWhQM2ZqcWhqNS93WmVxQ0Z1OXhtZVFlNUVRa25lWW9MeDJST1hweHF6K0xtZ1NmN0FKQzN5NFc4YUVOeVdsUVE4UVllSjZQaDBRV05lb3NUWHFTVXdncGZrTUU5MzhIQ2xCMURyaG5tUlptOE9HU2s2S0lqUlUyV0xiV3NoeEc3d0pjRkpzS0Njb1o2ZVBBK3B0T3FvWXpIUFQ0ZSs3OEg2aFlpeVUvQjFKVGZGV1pkcHRxdjVla0JjWmxicTlBakRjdG5mcm5UdFIxRHhFaU5SZ1JzT0lEbm95d0llN2JTUElEQ1RUNldWcnAxdm5lcENsUzV2eVhKSHFQdVFLZi9reFdlbjlpalJOS1NSWnZLSVlISUw4NWk5VWpiMThNNVcxZlhOTW8zL3YrRUk2bklSMHRHTVQ2UllqbVdkYUFGQ1o2bXJMRFFOOWVlbDBpQ0pGOEloTTdnK28wL3p3enNTRVB1VFZGSjdBQ1c2cHhjTzd2SVNEWVpaKys3LzN1WHBWNlZNdHZLN0x3Qjd1NjZldjEvUGdyRjg2WWtrK0NJNXhpMGxSTCtvcnZaR1ZUMElrdU96SFlxTXRSVU1TU2ZlSGpSK0JVUytxTld5NFE1ZWJVZlBrYTZ2V3ZEVFJIK2Z3cTlXSnJKKzY0b2MwWXlTQ2VjS3QxTEtlNzlKU0IyK0NQS1BJK1hKZ2JVa1BtRVNsSnN6amRtM2xNTHZ0YXllQklnMFNVTGtQZHNmWnFkTUp5VithQzhTS2JZSm84cHlsdENtTDI2aVBEM0J4R2F5bU1lNElZcGpWZ3NmbnBCbWlXMEc0Z3ZuczVXbUI4aVNlZ1lHc0dPdHc0cXo5eXRBNUVKT1hzc01CaGNLMDNjak9MR3JYQnJkTUlsR2NNSWJGZmlVNmE5WUFMVm5UanZ4ZE9mc3BnQ0hycDJSRklPWnY0Y0xEWWdlMlhNS0NSaXBxMjQ5V3djaXVOMWwraDBtL1ZYMndpdVBRRVF4TzlYK3NlL0xqaHE0dkFwZmlwZHN0akh5RmdYWFgrTjJVdzhJdmxKZWdPemxCMG5haXhtdGZmR01odlpVZjd0T2h1K3NoQ28vZE9YRWdWZ3I0OU1yMDFXWkJXZ1ZIZk42MG5CeXJ6RzZlRVpjcnMrZitIY3E5a0ZhOWhxQXNQZVJQaWwyQ2RBMmtTZWJRdUo5dEVVandsTngva0xkUi9PMjlCcHZJRk95QlB2MkNqZGJPb095M0JsR2d5dWJoMGxJTjVPUGlraWs5ZGVYckpscmpuUjNJTkEwZkczMGgrMi9qS0NBOWMxUlMrQU9ZNWdOTUpVSExSN3ZJOTlUMHdJU0lYQ1VjenMxQzlqRFovQiswenVPZW5pMWEvVjhTWjVXdFBRUGsvK3czTUtqUFZVajd3YUxEaFptb0RnZ1RMRFF3UEhMbmQrd3plV3NBeGhSNVM3NVlRSzlMd3ZVeU95RTJvZXBWZzJPbjlSYTRVaHJzWE9IZnlQM0tMVkJTMGl0UFY4eTFZU2kyYVp5bXYzR080S2Vhc1hSZm00S2JMenRWS2N3dGpDVmZsU2preVpKSEp4dXpSTmRjbHU4dTBmb0ZaNEMxWTFBU1N0OTk0MHMxQjdrVmhCa1kwTEhTQVo3WFMxMXZJK2VkVFRKRlBWUmQyZ1Q0Tzl2MjFEN1pVVWlDclJDTWowdnBnL1JmSitYL1JmdDJ0djdUWEVPR01CYVdTR3ZEZFVQTEg4U01xckJBaHZCeVNEUzhvQzU4T3JBQkJ4U2g4NnRFeU90dXN5YWtlUjhiNTl3MU5hVnozbTNMaDBmSkFRZzlnank4bmVQQ1RlaFdIMDN6Wmh5Ly9LMlRvVEN5YmgzM1I0VkE2aHRyOVE0MXFrMy9lWlFJcDFMWXk0dGRBSmRIcE0vWTBncFl5bzIzVVdmbm8xV3J1NUp2MEFUSTdnUDhXKzRFME04UUtETnh4RlUrRzdNb0dESWR3K0JKYXdnK2EwZDVYYXZUOWtLdlNXbWJMK0Ftay9JZTRmT0tzWXZBYndVOEFjWmthSkxsUGhrbGc2c0dSMTd6Nm9TbGRpbkI2VTJGWVZpM2FnTi9yMVJGUFJFY3Y2S2lpWUxQQWdud0J4NXFpTXdzZWJHSzlNcXpkaitJeUhEM1VUUjk0WkVoSkl1SkNob2lyVm55cFVyOG13OW1laUFQRHZtZWY2OWhLMGI4aUZIdFdEQ0Rzd0p2bVlVVkdubExxRXNZT0xWODNPcHJGRytvTXZuN28wQkNxU1R6NnRJVXpjeVRMb01iVjhOemszaGY1OGxvcnJjbTZjelNzNDUzektBUVJtMklHRUVtdkluYzVwQjJjZUplNW1meWxJRjQwaWVGZVBsbnd2Qzc2OXJJTldlSG00ZExSY2thNVZTWFFNbjhoWHR1KzVoK3ZsQXE4NnV5K1JRWkl6QVF6NnVGL0ZsUEE2V0dvRlUrZExrV2hma1plWEtFT3RFdjJUZkJGbE55VTFGMFNvbjFqMWhlelhVZUV6a3ZxcGUrcHNFNFBla1JFMGY1YlorM0FRRHpCKzRZR2QyYUx1eEw1c2J0aStLVitPdTJ0d20zUW9tZThNWVVaZ09CL2wvREhOd2wxRXNDcU9IWmRURFpWWE1hWk1OT2xzWVVoOU5wTExLQmk5YlFyVWZJN1JpK1ZsRU5QL3A3MVh6TVB1bmp3RHZEWU1CemF0cGNwbTJXempHNmZSSXQyZGNPeXJkeFprd0czNU54QURUaDJFWGFFMGpiRjErUFhlNlpmZnV0c0ZaVi9ndUVnOC8zbmVrQ3hscEFyS0lKNlppSExrRTduT0EzQ1NPTVpyc1pJU0FpTk1GMkVOblEwU01iWGxLcFl0RmMyQ252WVA4N3ZPL0FrY2ZkQ0JKV3IxQmlYQTBjVi8zQmlWckNjdGVxQ2VwbUhDWjNpcDZFNE5oZ2N5N3Y5YmY5cWNRdEc5amxCUHhUTGY1L1ViZVRteW5FY2xtQmpZajF3ZXdlQ1BaTzlXVHd1Q0QwcXBmeHRPV2JFRU10WjJJMFZGaXlXbUVLTXF5Z1dLZk0rTWdWRUdMd29GL0ZPUGJkVlN3elRHckt2K3NtbnRuR2NXWFlaSXp4RFpxcVlkMi9Fck0xWURCNXp2TDlzTG1EaGsrMEVwMVRmcXFPdU1DZnQ0aGoxTTBLdy9rSi9uMkJ1czlLQmhqUGpTSDZlNllYeHJQaWx0d29TVjVhYTc0YlJHaGdvSlpxVXpkUVpNQ0gzckNQaWwvU0hSdlU0WWdNcVJlSG5RZk91UktxVU1SRjNhSDQ0b3R4bGd4Z2hHVjh3SWFxRWYrQUtwSDdLMy9yVUI0eUh4RXRjNk54bExFSU5sNkZmQldOcDNMc1pob1gvbXBLNktFKzRNcDg3UmZaMFZBTnhPZFdCQTRrY3JPb2gxWnlDVkhENVpnMVc5dGd4MzFnc0svN0haVG04S2ZBWVRQN1BtRE5JUEQzcmpYNkU0K0JVNVAxVFFpMjJiTG1TMXdtWmMxSGo0RmJOUkxydjkyczB4elZVenZDZVpJYXo5RkFqRGxSY0JNYUJEWFM3bE1YWVpBRWdmbXRLYm5hTTlVMFE3UXE4N3htU0l3aXhEUUxLbWRJTGtKWXFIT2VqWEZ2aytSdDJuay9HRDk1UGlIUUhRcXhUYVA3UFF4OTliV01TblB3cXNNQXJ6dFRRdXRqSmZaQ21vOGJkbFN4b3N0ZFZDQXFDZVBmdTVhcWEwYms1cUY4aVo0YVVNTm5oZlhLYmxWczFhK3JXUFZYbExGTzdCTGwvYkNhZTZBUEVaUnY5dnpvSHFLWlBIVFpMUTRHaVJuUU9Hd1ZLU2RCd0VTT0poa2dFK2VCbEtaN1QwVkwxdG1UWUhKUDdGQTFYZGR1cTV2RW95VjZhNzVrRkk3ZTVCS2VWYzVzS3A0QndZbGZYaUdqd1hDeURRUDh0RVN3WFJBbkIwcTh1enM0RmhtY293bkxIMm9GMTBQVXVYZkpUaFJuckE0YytEQ1BsajRleWZNZkRXQlZKZFVac3Rjdk5RUEV3MkVTSktaaGpkMW5jSUQyVGk2QWdhTG1TT05rTmgzWjc4ZXNtUWJBSzM5Qjd1VnhLN2ZGMjdvWnlocVNaL1JiTEg2cFdZN0w1T1dJaUZLcU94a2dVeHVvT0hxck84NWl2blFySzFZNHFHY3Y5c0dQQ1oxTlZtL3V0am4ySXErZDlqdUUxSElqWVlUcUV6bEI2RVlpeGh5Q3NiQVlnWTJ3T0p0UWpHVWxTRGZiUldDNkRhWGdJblNkTHNCSkM4bU5xU1p6KzdlVlJQUTNmeTZaYkZ5ekVKOEpmd2tMc0hzT25VTlVldWY4OVVvc2NGeVdjeUVBUi9HYnVVUWl4ZWdkcWllMGROVE5FYVgyU0FnLzU3RDBjZks2clUvOUVtQVZaR1Y3Rm9ac1krbjcraWtrZWQ2Y3Zsd2ltb084WkdCWnp0b3JzYjUzSGczMjJHTUFsdkpiWFpvR0pWdDF1Y2V3NS95cDkxb2ZPWURLTW5PWEFuRENnSWFKVjE5V3lEU0FTelUrd1greXVXUFhEUTE4Q0EycTZvNmxtVTVFc2hFMEwxNjBXY2tZNVNhSWMzNEdmT2pxeUFMR0RTMFdTSlE5dTZ5M1ptUnhkYXRPaWpic2RibmRKVE5FeExWbng2RWhKSGZjN3FYL1hiKzlkMXJibWIyUmFjTmw1SnQ1TFdzMVJlSGJ4L1k2b2UzTWFZbTIyZEMya1BhYkZBSFpxMzl3VERMYUhENVVGWmZVTjVLUlVoSkhRbkdRL214VW5DLzNwV1RiL2pMbTZncm53V3crc1h0ZFA5N05sL0tZQlBMdTA1eXpsVktrcnNGck92d1UyWUNRYld6QW12aFpSREJyVmVTT0tvNU1SM1lPTVRLYmRhTGNnb1RGNE1taDBYNUZqdjVuRW1nNDNXdTdCbDNFcmlscVhMNEx5TkduaEhjNmR2Z21Rbk1YUkRXZ1I2SEpXRit1cldaWGRCR0F0Z1BhRVdpdER4MVY4aGZtVDBnNDh3UGhyaXBNSElnTWtkUlRkQWdpZEVTdDFHb0VoN1NHbUxjbmRkYURMb2RnM0xsajdxL3plN0RFb2Z1K1BqK1hCREVCQXJWK0F1ejFzd1Bvd3lkUzNHcmtpRzZBdkhVQUVMUGEwNEZaUmYxSEcvd2M2MDFVSzJkRVRLN3g1cWdnTXIvbnBaV3FETVplaEIxTEluREsrb3Q5c1hQMVJHcy9OamNiR2JybHhhUVVZdjNkN0V5NFI1MThkcnI1SmFMMjMrZVp4SlQ3aXpqYk1oQzJDYVpXZzZhMnMyR2RPTllSY2RmMTFpcmNDWmg3c0U3cUhRRUZocTZxaStLdmtoaUxFNnh2RDluUkU4RXQ1VGs3YWpzN3p2bXN3RXdPdi85ZGZSQ0pJa1ZYVmdaL3Y4NUNyUStCYkRGZXh0eWtCUzdGb2tkRUVNNnAxRWwySnFXb0wzNXZOaThiaW8rQkk5K2lmcjczcWhQV1piby9IaldraFR5NUl1YmU1TnliSTlTMzFwZDNXYm95Zmo4ZUY1ZnU2NW95eEcrSllGSHNEWEprcWdBbUZDY3d2Z1B4Qnh5dGN1b1RrOGFkelEvcWd3MkZmclpwdDh5cll2ZytPUmdNTUtCa1JwMDgxRUQwNUtGMjlZY3JvTGNQT0dNS1BTeG5pOHdQcktHb2lWaC9qcUFlazhUM1NJSC9VWHVBZEdKVFVrcjdjcGFoeXZ0cDZBb0d4STFIK3VYeXFkQk43M1IyMHpBVUo5eWNLeWZPcW9oWXFFRkpFQUpJTS9WVkVXNEdGZ1QwQ1hTaDZkOFJKTSs0d2MwdTZhU0oxditsdG9UdnlWSkkxcWJwdDdJL2s5UitiVnNhSWU1T081dThiTGlDbnFHbFgxeUF5TXpkb2FFOXJFSEtqc1hzQlRmUGJRNSttbmFjTzZpN3RDanZORitDeDB0T2swTUNTYXVabkdnUzVQeEsycWNsWXBGWmNKbzMyekk0VnducEdOQTNMZFJXS2U5VFNYTW9lRjI0Y1huVFZHdkRUNTFiQm5BYVo1U1JtdVU0eVI5ZWp2UG1yK3NDV1V3TVVJeUhOY1dSYkdZdEJtNFRwSUd2SjdFRnFTOEpVMWdOV24zczJKM1dUdzFXc1E0Rmc0OHM2Vy9yMUR4T3dBajVKaVVScGdnUjdiQ3FiLytsTnZQRkUzZDJQYWcyd2RjWW5rc2RnMTFaSFNYeFBSSElpMFV3R2x4SmJkQ1gyako0cStEZkRGc2pCc3ZpV21KSnlIeExLOWd6T2owaUdPODlXYTlXSTNkSzE2MkJlbjN4QUFjbllUVHpHdlVGZ0hORHdmZVhjV29jaGV4M3pEaW41WHZHbks5ZEVPR0dvaEg3VUxJbml2eGlIMHJhMVhJalRFWHp0ME1aTW5lQ1gyOGF4MDhyeHpvYU1wdkJleDFTQ3FLbXdpOTZzeU83L0xCUVZNaEVqOGowWmp0SDA5SjlpWU1MR3dDSmtKLzh3dE51QmUyZkViYUkraERUM2pUNTloVThkR0s0Qm9sd3RvaDNZWnBMUmplY1Z1blEwSDdhcDd0U1NaVVd6bGFkdEVIM0hlMk9tWG1DclJ1N1VXZGxEY1hLMHQ1ZmJwcEdrSEhlZjRxNWhsZGxiWEc1amJMRTBNc0FVTWpkU3lHcSt3MXAzeUJ5VXhjU1JRcU1rVnpkV0ZFQkVmZTdqOGIzd2NMNkRHaFhKVUVkaWZwVWdvTGZET3J5aCtadkZ0SVZrUDM2N3VaWkt5cUwzWVh5L0xqNWRJcEw5OG14OVplWjFvMXZUbXd0eVdmK2VpWGdibkxuN0hiMmxaUUUzY3hYYTNxcnE5Q1Qra1NNUlNJKys4Z0wxL2VGVjhwTmgwNG5wWEdTd1MyTCtKdzhvaGtWdmJKQk1vbktzLzc0TGRxaG9JbWtBanpVeWdmdUQxbElpZG1NUFVrNzhIZ2NoUElTSTJSbmhDc1ZQZW5wdEFZZWJPcmNhMzZNMktnOEJKU0VDUk5WVm5ya1pDdDRpQ2RxcFh5MU5tazFIMVE5Z1JTMjkyWVdLTkltdE9WdDR1a3I0NUQzQlN3dHBrMjRIaTNwajdmVVpzQ3FUdStKdy9NUGRxVmFiaEt6Wm5qVVkzY0NkZFBVZWtNU1BQN1k4MHQzc2lFckk0ZWRMNDJ2dzRqY2EzQ0hZTThQa2VkTWNaZDFHUGVEdVdqOEl6SHplZ1R1aDVmOGdvdU8xTlNUT2tLMkVVckNmQ1gyK29QaG5TcllGSk0wRllFNFNKNWtpTzRUaHpraWNKNktaM2xLM2k5Uk5ZM0txNGFQUGJUNW5sY1BaQzhnT0w5YVVRNkJ2Q000Z0Y0MmN3WHJJTXliSVdzdU9hWkVmVGczK1Vwb2Y2WFhSaHZ1Z0hNSWJXUSthL25heFY0ZVZLZzI4S2IwdkRxQTAxM1lRSnhFT0lsZzZZK0tuSnJvTmk1UmFoR1BTS0ZDOVJsOGh3bmNTY25NL3JQVnRFUUlqZ2kvbEl6UjMraURVcm9pdEFOTFdwUWdaZnNEbUVUNXVvc21wTXNJanRtSXR5QzZNenFhbkZET3BlK3BLdG9FWTBSeGROR2x6NXJjTTJYWHlQZDRmTWRyWTZicWxEekFzaXBidFdFVjRYSDkwbkxrd3pFRmZsYzVib3AxNE9IU01leUI2UnhTbHpPZ3RFZTNGNGhTdXAvSko5dnZ6ZmlYNHYzOVp4WVVzQUlaUC8ybmN3bktTWUlub3Bmb1ZhblZJdndoYmJGRlk1M0ttTEQzZURPazlBZEN4RTJRR21iMWlvZEE0dW1NTmxrMVZEWWlBa0pFRG1NSkZkRlNwU2ZCWkNCT2l6SVVua20rckw4TWVFcTh6Q3hGWUxWbFhBQi9rUDBXWXovQmtrOEtwT1Y5RHVTUEZiKzQzNmRmWkhCaHdnUS9OTFlGUk80TSszaE1zUGZwMzg0TFhiRXN2MHRsMkZpOVdZUTJOa21jTGVoUzVzYjQ1eTJlVXVBWnhIcWNpd0hiNXpaQzhMWEZiczRkdmZYQ2puRHlWK0ZlTTJqdmdhSUJZcU9xZ2I1YlJuZytXK28rOU9weit3d2pSVzdLNkpkdVRZYkJES2lLZDVza2pOeGlNQXhUT29ObG5nMEZCSDhxSGo3RWJEOTNVR1h3bzRPOVZBVVBKbkJ4dDR4Z3Z1ekUxb1NqV3NybXJyckM3b1RoWWRCUEs4WS9vOGJYUVNCK3hheDFPUTJjZmJiWWRTMWY3d0RKZVQ5Uk1Id2wvdEhvYlNZaTNoUC9CaU4wNzBxTms3ZExHczJJOC9yUkZXUTRZazJnakd6RjM1UzlyK285RDNlckpMU0NVdkF0QlZOc2tleFR6aU1OZzYrWFRhb3dud3IxL1E1QjA3WkEzc2ZWSWJBY3lENjFaS255K1RIMjhEaitKM040VVN0eGRMdkRoK2thaURFYWJWa0R1SGJBS2VtckRxc0FMUUtid2pXMUVWMmFrY3dCb3liZHg4ZWU2Mk1DRWozR0QyMVFnVTNoUXhBeEloY1BJZ20rSXc1SXZYN0RGVldTSmw0Z25haHp3T3ZxcUxDRkM3SmR5bW9IRzQvNEZnNytxeXZkd1c0UncvSXVqWW5kMW1FaHpqUXBuUzA3S0U2Mjc3VnBMcDliNGF2RU5RL0FCUnhtZk5LV0RMeUV6b3AyTndtKzhQemtwVUh1QjNTNnd0aXpkVmRodmNzcmdVY0RCaVhXMXFBOElqMXpadnNCTm9ZdWp0N251V285ZVR3anV3enJQVGJsMDNoUmFzVnhxYkVOaVowZGphcmNiL0N1MVpiejRMa2NIQks3OXFZdGhQRmVuU2MrNERDREp3TVpFOFd4WkVLRGd2b3hPTjdROXJuTGJzd0xsSjYrbWhlSzVhcUduU3o5Ulg0UFQwNzVraytzblROdklySXFzeGZGVnJuZU5WaldYazd3d0QyUW9nRU9XeW52RXV5Z0pka2hjRitYUGc1djA4SG9peitTVTgxRGVUREJydHQveGlMWDhrL1hrZ0FtUkxkNzB4SzlnSkhiMkpyV0J5Um9UUStFVDcyLzc0L0Z4NVJWQ2FZZlRNRkZqdTkzQmhBeEFMZTN6RW1GRTUzbkpxYWVYeHpKR1FRQjR1M0NRUzVCeWtlSjlzbjBLZ1lWZ0FRYkptV04yVjBNa0d5QWw0YXQyc3lLUUF5SnlXajRLNzNTMTNsZ1p3ZEQzSmRadWkzd0FMUDRNTWhQSk9TSnhpejNMamQ3S1R0VjR2cm1HeFlDS1hoZ0VFRi8remVjbVh6eHJCR3NWcW40T2dBMUhQMXAvTGFmVG42aGg5OU1CUUxrWXNlNS8vS0VnZjF3STJhdGRhUFpYano0RVdDMEk5N29FRktnVUp1WU8vdnNoVDJaNFlOUG5QK01wTDRiMkl6bGw0TGM1R3N4bjBDZU1ITDM5Qm04V3pPVW9NMFpuNS9vTjQweHUxZGo3aHhVZXdXTDlBTUUzcUF4aGkwTk1TWkczMjRTbkNNTW5ERXpCQWFCKzF0TmVIYXZ4YVZlZ2JYdjdkQ0tnNWV6aGlLdHJMOC9CeDhCY0lZRkFXSTUwMGdmSmVtcW9XdnM2U0gzKzhxOHFrVzJWNlEzVmFhQkQwSU53T2xsUmRkU1dmVzJqR1FFZDZLWExmYnZzMktqUWhpNmltRUlTSWV4OTJuN0lYNktjbzdjOG1QdFljOG1CNElPTzV2SC9zSW9XM283TzdhZkpBN2JjaittZ2Q5cUpRTDl1Snc3MDZYbWxPNDVhV0hYN0tkVmlEV3lwSUVheVpjeTNPbGJCaEMwRlR1N0pSQW5HTXFIVEJjRUdTaHdPK0paWnhiTHlHUmlLQjgycWp2Rm1rVUMvQ1BTdGtidjVJZXhvd1J5c2hEVUZlZGcwdFJBbUNhZmZwRFhQeXJXS1ozK1F6Y3AxSW5QZlFKdVQ3RlhKeG4zSE54SUcyN1N6RjJNVU9zem4vZFJKYTN6dGdmTEV2VFhWeVg5aENONWJiN0V5aTRpMkc0SXNqL3hpK1JmbGFyQnRQTDQ1QiswZElXaGNIWjNsQ1lsTC85ZXBoQUxEOGNaWG9JS3JVU3pTaXQ0UmcxcGRHNlZIbHVLYUxBRUJmcGJNQklQSkFhdU5zZlFRYmlOZGlqeDhQZy9GNzMya2VFdzJEby9WS3VaVDl4eFpCd3l1ZTBBL0lMYW0yaGNpNkxDTTg2TTJHL3lROXVxbVB0OVZlaitGelhkSk1YaTk1Vjl5cFBybFo2YXYzS0F4VUFicnRwOVdHUmh4TTQ2aXNmSCtpbXJYUWJwQTNtdzVHRXpMdjNXSUczTW01aWttUVo1RWtkSXdzY091ay9VeHpXaWJtdFpzYU9mdmo0dkZNN1RuOVJuTUVnemR5aE1VRWdSd2hRYlY5aGUxM0R6OHR0K3hYOVNKaThUVUFmL1hVQk5ZLzlsL0tFbHVkMmVLekNibHZUaU5CTmpMZ0RGVU5zdWplQktMQyt4bEEvWmttUVBLeUNuNkozYmFoZVp0OExLSmVsK21aU1Q1U04ybyt3cnRhNkI4eks0WFRPNGQxVUpmTXp4L0ZzZ0ltbk14V2FyRmR0VkFGbmxHV2tOVWwyZmJWRWdjTWFDemRUbzdIQ3lNUk5KeWw1M25sVUR0eHhSd2Q3OFdTWm9PVlJqeHo4V3Y4aUZ5bTRZTzZ6ci9Ud0w5a2d4M3VPM2gyQ1gwSVN6S0syRlNMMHJDVi9JRCtSQjQwUHRrTURYMnN2THlOVEFPZnlHK0JCOWdwR1VhemtmTjMvRW9sZW9YSTdqZ1JySjEyMXdkQ1lpNHhHcERXcDA3aHpzSnltbXlsWnZMMkh1SUN6OUlGTTl4UjRmbW05Z3hxZ1JuZGtmVjhVNHl5YUI0WmQxZ08xZ013eTY2ZnNCTzYzWUg3M08zTzZGRGpIdklYUzVqMWVYTkc5ZTZrcktRTTlWR1Jxc3hYQmw4Wjh6cy91THk4RlZleXhvaUFRZlhXeGNPTUMycFNrVmpaZWNPSCtkYXBVaWxkUW1EdzFvc2xKR1hEa1hXLzdGVlhYOXh1enc4OVF3Y2MyV3pFSVZ3T2VRdEVGTHM3VDJ0SDI4WTNzSklWbWRraUxRV3Y0R0pOOE5ONFJsVk1XNDV5cjYybG5sTjl4VlpTVWdqb3hmV0xla0o4VmQ4bnBDZE1HaFhHanVpMnJFdmh4UXNWL09BalZGSFpwY0N3T0M0WHkwUmFKcUdMRHdZRTF4anhpTENlN0dYL01YTDQwbUs1VjNIdG1xLzlLUCtiWW9lUEkwa0M3RTErTWc5eC9qV2tmZlltbDJZdmVyWFdDUEo5QTRDb2R4bGN5c3p5bmZ4azdOVk9rNlZZRnpZdFBUQ1kxakg0eUE2TDF3eXZHTWRKS090ZFd2NndZaC9hak5Rc2w5T2ZzaFJkRDVTbUE2UDYxbGg3b0Nqc1U4Y2htTEhQNXYyL1dmQkNsTWtCOGJkTEorSDFqVmlkYXdWc2Y1eTQ1SGI5cnVwWDQ2UG5TMW96UXA5WDVkQ1poaGZNSzJrMmFMQmpCWlhZT2RDcEM4cGdQMnM2STdQU0JtMHk5b3hyRkdrWmFsTlZZaVQ3T2NXb1k5UGEzY2pHcXRubURUNjZ4YXQ2Wlp5TjVKSXVBMTFSZ0c5U014eWJzei9RMW94b3QzYXc3VjJGWlNXT2RuNGVSRFFQdTUxQXFFQzFlNk9HaHdONGxSMDhRcy9pR2NpcWt2YkkrN3BxZlY4RW5tRWNWN3RrZnl0d3piZ0ZkUks2eTNnVCtUeXJaclRyc2RqNzNMempLbnQ1SUxUVUo3SFVtNzRHTUZRNHFHa3FEWkNXRkNPdnRmU2ZUeXRiRVFjUlllcVQrSXR0SDBteStUUExUaUJsRlMxbkM3U2o0NWpyYms5SEQ5ZlZKY1VxWkFVNEJ3b0p2WWxBZS9YcWticWxTSC9RUkQxMXlJTkNBNGszWDdYTUZLWFNVTjA1UDFmWTFFWTBnOERqeDkyaTE0dUJNNVpsTHlDMjVucE5iK3lJRFFhNHBzZ2xNanpHRmRCYWhwNzAvbFpobjlhTVZPeXJQMHJnTUJ3ODA0TEtIeVdWY21oMUdqMUZ5c3N5cnhoS1ovN3hmVGVoeHpLbjlsUUtnUTNmWnhUWkYwZjNqWENseWxWQ3d1OG9YL2I3UnpoUk5BekNTOS9JcVVndW4xbWprZjQxcUZXQzUxN0ZWT29hMWdFVXFydEs0VjZYMXdPMmd6K1pRaS9qNXQ1aHQ5b2tNNXpvT1lHTFozaEw5QW9FSmZtMUNyYy85SE43Vkd0WklHR2x5b0p1ZjVJamg0YnE3LzRnREhpelJpbWRMcEFXQ1gxTWQwTjlONWIxakFpcWplM0pydzdiQzZzeGxnWGltSnp4ZW83b1JZNVVEdWpOTnJoUUpWbWFFQnA4TW52QVZUblpNMTZWay9LaWYvcEhYWjFVRGg2MGRIazVvZStqYitySTYzYU12SG9FOHZOdmtBZjJRVHRlVGsvZVloWWs5S0t2b0s1dDlnczlyMFhIREF5Mzc4U3NqbzBhYjV1Q1hwNFcyb3VBSzg2VnZmWUYzdHJhaGpWOEtleGJvTG9qcXdudWNYV0xkL003VkVnTzdKeTl5ajJvbUxtWHRoSHo2YUJsTytjRWsxMmFWR1BRVXIxNUhYc1J4VVBzeDdhMGx1c3VETWtDMFQvLzc4Rkptc01ZT2VIQ2E0VU5tNENobUFPMk5EdkVyYjRiTHRua1N5OGZmSU92SG55K2dUY01zMEcwTkdCTHY2YW5tRlpNeWk4d1ZaV0wxZ3lGYUp3WmV1b1M4SzBsRGFBWURKUnFSN0UrK25OanFFUFFuMHBRUFhnbm5Wc3ljVG5YSGdaWVNDZDVhK25XUWF6VitEU29ER3Nmbm5mRHAxZ1lzOERCMVk0MzdTSFVsNnZ3MjM3UVgwZ2dkRGpuQXZySkprS0ZMQWhjZEdtVHNlTWJDSjlkZ2xzTmpLaXYvK3FGSTFFTzZZeXZ5bmMxamtmV0lnS1Vsc281ZUI2dzU3dFYwZlNVVXNIOXI1eklPdVByTjRScThGV1lsZmdHeUs0QUdkR3NNK2dqL3lwZXJkY0V4SnhyaFJkcUcyNmRpdUs2NGRxWmE0dlVseEw2UGFnY0o3MnZOakVVZHg2OWp2SHgwdXdiaHVNZUpmL2ROVVpEZllkQ1NvSXZzQjl0T1lMSEl2bDROdS9XOG5XTmpHNjNJV0Z1b2dVL1hIRGxrRzU4V1ZDVkhOWTllcHhxVWt1SUs2SWt5ckdXSFV6QTVqd3JHMnpQUkFzTVpOb0J6MmFvbGcrYkdEVzlBclJ2UExBSWh1SUlHVjlBcUh2emxQYjJ6VnhRQXh2MG84TzR4dTh0Kzh0WjhsazdOZjg5bGhpb1F2WlhZQitwVXBreUIxUGMvZ3pxUzlRRHpQalNidHRXSlJ2c2FxVjNwM21sWnFBQVhpK29veDNQQ3paS1BNd2ZvbFk0eWQ2Vk05RVJ4VjI4NEhtcGcxUXl6NEtzUnY0V3p0cUlEb2VlMkFVZ1VKMFA4NTZkbVgrSGY1elZrOE0zcEE4SkhGZm4rc01qZGM4eGlKZ0JvQ090UDg1bzg1T0dJUnlpaUdPczlud090WWFydFpBeXNHK0RWK1FDZmZBV3JhdzdjbEpHdldESjZJM21TVUNORjhydGRlMzM4d1hpODVEa2Z3WjNVYWpaWFlCTlIvSUJ6UUVEK0xnblRQRk4xTU5GeXoybWRVTjdiV0xTby9TY0ROdHJsRm9pbkdscHg4elZsb0J6SVVEaXYwVkpDKzlXSFhyMjJFVDB4SWtvVExWRnRPNVFKaWp2Y0o0dzI5cDlqb240Z2l5L0JYNU9QZURsZkxoaENOVHkyZ2llcFJ6TUxRVmpjanA3VzhscGNieGlydVE3emh6aXdJUnc0b0drMkhzRUNlODJlNlFwdGd4M0VMWEpWZE0xcTBsZ2puTFBobTRKU0hZaUl1bEZ6U1RCL3hTQmJFT2kvV1c5Vm1TcHBkblhwclpxYWtnVjRRUUV0eCtWR0lja004Q0c2dElMdDg0LzJ6R2ZHSkU2RXREeDZjV1VkcEJGL2pNWFd1Uyt4aEpiT3hSY2JLS1BGdnVWRkVhems5MTdRNzM5K01hL3lhbDErVGhXa09JWmJmeVgxM2dyclVIb29PUUxpUm5qRXA2SHprKzhNQXdWTnNSODhtcGNPNklkbmpETWcyT25BNS9WbmJ6OHZvVThTK2dxUFJVb1lLZk5rR1VrTnNNKzQ0RGFmckZpdmtlU2dNRkxFQ1lSUkJ0WXhzcHZuQUJUQmdzcWV5dmR6RWVDTzMvNkpUdmNNSDhDR2UzTzJwWlZockpjbUZLZkl0NU5nSmdtVUl0QWEzZktlU2t3bFlvZ0Q0STQ1bTFlcFJnei9HV2ZTeW9TcmkwMkhRMkZENmNtcG5RNGg3cE5iQ3B0eFNuQW45UVA3NmcrREs0L2ZBMWovWGVad1NaSTlmYzRxZWJTZHpaQkYvSm1iYSt4K1ZSdnhGdDd6SEo4VzJLbk5oSXhaa3p2SFU0UzFWSDVzM1ZSdnQ1ZEJjc3JsQllNcHpvS1FpaGpEUDBsNVRFYlJaRjZCREJuL2VkWUg0eVMrS1BzQ3BYKzNjSitTa01oMWNPWEpXWEJnNXF4b0tNZzkzN1hzTTRuSlRpTnFvTzRzMGh6SldlM1dvcyt4ZE9IQW1kWDdvZTh3TndEL1BnZHdCUXFVYjdzV25OUlpXOEVWdzZ1Z0c3RTFsRTJ5SFdZZ0txelk1T2E1VSs2cTdEM2RSMHNSM21mN1J0cHlFS2N1OFR4RkVGbU5sM2NBR1owT3l5U1YwM0M1WkdoVWU4dUJQbzdmd012UmZ4SlkwU3JlOTdMWjl6ekRWN3lDS1pmMjhYQVg1S1RTZklsZ3E5TkJVMGg2RmZtMUFhYUtZUVphVzhZbjlhbFByTVpiRHFDb1dyUjgzTnlTTmFvR0FxdzIzZUdZY1I4K1pxbXJIdHhva0U0RStubTMyZFVsYzJYZTUzTVZSek5icEZCVmhLdkpqbXk3ckVIL3QzaDhVQ0ZLQ1RkU1QwV2hUTW1qVXNjQ1VkSGFJdm52QmJDa3dtTi9Xcys0UTlUU01yclVWRnJWSWF3Q2JYSk1yYWFEdmsvdHF6Q29wSGh3MlNlWVB4NFBrSFc1eEdOSWdiOUM1anVHWWFPTFVGbjJyMmZuNGRrdjZ6M1NrbE9OUmJZT0cvaldIN1lCSVZsa2kzNzN6VVlJejFpTEsyRWRENjBENkFVbTVidVNCV3JOcUtNMFVqWTA3c25mVWtuVG5Bc2JRSCtvQktxd2MyWi9ycitzTFlvUVJoOUtTOGl6cUtlOWg2N0xGU0lHNkxzVFNYRXV3bW1nSnJLYzdPZDNObi9ObDF2cXdvbUtSR3EvQVdVTnJ2K0c5Unl0MEdyVlB5UlVPVnk1TjhacFY5c0RSQXBJM2tkTGxCdkZiWk43VUc5cVZJR2RuUjB1d3Erb0lDUjRsSDMrOWJZSXlESGtDK0hkeTlpZ2lLMGM3bnBUSUIxTmpFUWwyUGI4eUs5OWtFd0hLM3hCV2lhSXhxVE80clV2UTVTK05zT1M3cTZjZThHRGNodmVJVU1WQUoyUHdaTmZxbFRaYzNpNkJrSWhoN0VoaFFwLzBMTnc1blA3VVVMQVh5U2pSNTNrd1dzcWx4cnVzQXBXYnhwWGFlZ2hTQlFackNWUkFBRG9VYzRtOGthbHNMbmN6QTRBZ2dUZXZYN1preEZ1QWZyTlBrMEh6Rmtidk91dVhOYmhqUGl4dTA2bU5YV01KbUVXcDI5OGNMbFI2RFMrcXIwREthUnRJQThIdnZCWDhYSURBZExOSWF5cHRTbk1qZWErSUtvcE9EU0duR2hNbGsxT1FnT2NmYU9TZk5nMGxoYml1aVlsNkVOTHp0N2hqdWpabm5SNmNQNU1lYzZDSnVKTlhIcjRNVWJwTVlIYVV4OTFmVnd5bHNxUW9CN1dsNTc5TngzdUxNTXBIcHA2aktLN2tXM0FQb0xpN0oxc2crZHl3NEZ0cmY3RG4zc25RQzZxY2hZbkQ1SlV6VmpPalpMSDFBbjNRK3lTQWs1ekpLRkpOaHNFMW5DTDd6VWZmMVoxckRpQVhQZEJMZGVVQUkyamc1SFhsTUd6MERYNVJpVGk5dUtJbmdvTGFBNk5HS1g3NXhyTXB5cEl0U0JiN2R4ZUxrSnFhRXYybE93QUxHZm54OWcyZEFUY3gzQ0swV2p0a0IyRHo3U3ZobkZQMUdWR0twUkxHR2pSQlBoSmkxajFHN254RjJWdjI4ald6OWR4bHVlVDNEUHJMQkNuakRJRDRvT21Cb3FhKzN0ci9yTmVxMmNCbVRZcHBvR0xRc3BZQjViRzNWS2xIaEtkUC9UNW11SGo0UVVkZzZrNG9kYzlQYWdHM3d5UDY2ZjhlemdNajRIcXcrR1JERWpXRjlseTdaenNiN0IxMFZWS2Rma0phSHVUN1FzWGRweDJidXRvK3RzY2phNVFublJqa3lKdzVRZWMzUVQ0aHB3SWZ6RnJYSDVrVkQyUVFUNHkxSmQrVVRENUl5RFoxTjhWVFp4Q0szMDNiU2E2SEg5NU1HcTk5T3dRaWg0dFRTTm0weU9jVjMyamlTYko5Umgxd0Fub0dtb2QxTE5HNmxkaUhrdTZ0bmsrOWZNdVpqOGZOdHFoaXNwb1M4ZitmNHVIbktQYnoybTZRbjkwc1ZXdUgzOUpOM2habWhySHRmTk9INnZFeTczVlNkZ3EvRjBFVXpBdUp3NGVsQjU3YzEyWEo0dWlHM0lsM1padkRHc0hiRUFSZmxFV3Ryc0dJN01XZHU5eVVtS0daaGl2UjFlNld4YktVemFlbnZBaERnOVp3TElMUEtlRXpPQ2ZuUnVlQ0RNTW4xM3N4Z2FNREczZzQ4YWRNc29uajU0RklkcnJVRnVXN3diUkdFZEREeFE2S1A3VlE5RStzOWdmT3Vla2lNZkhUZ3ltOEMwcnBjeURpem1aL3Ixbk1McUJtSiszWUdja3IvTDR5MERaSGJvZXQ4ODlNMDlPV2xBMXVCcUlHZHVhSmhmOGhjbnFRRnkxYU4xYmd4M0tLanc1S3FEWE50WTI5cEl3bUdlSzdxbmxZMWpXcWkwWnorSjRqNzdCTWhkK3YrYlk1N0laNWRQZm9xekNyNG5XcUJ5SWE0UkpvQ1ppK1dNVy9zem1Xc1ZwQnJsS2dGY09KSXdSbnpUUDNqbFIzVXlIekk1QWI4NG1taENTdWVxZkh0cE9CSDZPaFFCZVUrb3BGT1JBSGg0K05vL3h4c2luMnp0cnZWV3VzeWFyQVY4WlRqNzE4emU3MWp1M2hMcGQzc3BXS2c5cWRZNjBmazRaYThpSmtEeU9EQ1Q4V1owZ2tEL3MwZzhFL0NXMWsyZFVkQ0ZJTmdia05mSkQ3RnBBcmVkV0FKSTBZZ1BENytWWVJMUE9tTEVKRWZlMWsySzhtcHU5WGVzaFJEb29leE0wWE5pWVdzdDNUWnlRT1lVWkU2MEZTMEE2bjRYYnNmOGNub3NQbjFnZ0V1UHZ3NENKTjBsNkxuMXhSL0VTQjJGVjYyZkYveWlpYXArSDdSbkdrMEJ0eFVPTmtHVEJJM0ZjckU4VjhQTWZlSEZQNmFSUGdsZmV2K1hVMU4zUW9ZaUo4SmovOHNqY1ZjY1hFVklLdEdIV2xkYmVrVXBqZHV5am1SYXJNQzBFR0w2VGttR0tIUXNvRDR6aEM4ZUNJaWlGRXNSUGNQd1E0U21zcC9RZThTM1BUZFdCQ2dWYXErNzd3dWtNaFJPcmxzdTlMcHd4Snk2NEJxbXJVRjNORyt4NnVJSkV1SkdQV3h6eDdGZXlqUi9xeExjdFA1NmI2bU5iMG5ScitvSDFxU0NiY1R0TTl4a05Sc3p0NGZTeDNybHFjY0hsMzJVZ2tkL0NQVktwalFBL1laZ2F4aVZBVmZYQzR2TXNkNDVhTFRlWGpXMkhsdDZYTC9sUTFHZE1XelIwa3V6TUxKN3c1L2ZxcERoaXJEVng5SDhCakZHaXhYVEp5T3MzSXluMEgzRFVTTm9OK3ZkZjRPcnZlOGNVcVFPK2lURWRQK2hhTTlmWUFRczdKSkU5Lzl3b2JDQ1pHWlRxMVZhYnFDQWtBeHBKRE9sYW9FaFRDOGxFU2dLNGdqOS9GK2xCOE00Q1huNGpaVW5KY3VCZ1U4ai9vZm1lMUh3bXhuaUZ0dFB5SHZTSXI1R29YVDhxVHZPQkhCUXNkTUVVN2QvTENvQmhFSHo5RTVPVU5zRy9sSi9JcXpTQ0ZBd2xuL3lEVEw3anIxWnk0dEdYUWtCb0R0b09VeHNWeEV6MndCQmxqL3c2eG9LTVRtOE9JeUNrdDhLYjM4akdjalpFRjFpbTNMMi9lYlVtQTNyaHY5RTZ4ZWZrcTE1cktNaG9IZjQ3cVU5bDBnZm9lZi82N2MwbGxjUGMxQkk5bnhLZHk4UHRCenV3TEY5ZFMyVFVYNVdScnA3MUZ3blIweFhLakJWV3NIV2xFY0puR21XN1phaUZFVEV5dmd6Sy8zY3BublkxT3R1d0IwSkVTV2ZFMGJCWGdtTDluaW00RVhydGgxZFVwVlo3UXJqenFiT3BmdEFNcFMrWWVwMlpnd29QYlo3VHdsTkJUcTJxM2N0SVUrOHIxdk5TOEJqN3o2U1R2eDZINXI5TEVDVHRsMnV1Y3NVNk0vckdxb3I4aTF0WU1ZU3hueXk4WGJES3JxUDMxVUljQkZWY3RuUUtKVkUrMHdXQVR2eXhCZitiRjJDQzFqZGIrajc0c3JkbVpKRzdHSlhhUzZmYjlwU3A3M1VvRjlVT01WQzZvQlkwSXNUZ1FJWTJrb1VSRUNhYll5NjB4VkpzaHpYMlc2aEhwQTltMzJnVVdvV2FpbkprNWkzN3dyWmprd1J2VHFYL1pyV3VMeDJITUtYOEhjd291alpybFFudHlRaEpabUs5RVV6Sjc1dVFGeUoyZHdOZS9NdmUvQ1laSmdXWGpvZVpaWEZScUVzRFd2Q0hmZDBiUkhJSlVua2pVUWlNMUh0QnNDRGVtZnl3Ui9qL2JsVk9qdjFnL0psbGxodldEUTFSc2dTZWRGWU9LTGQ1Skd1dzBiZUs1OTNldTg3Um1ucjRhUVNHV3JRa2M5T3VVcGFUTWUrMEd4U3hMUWVURG01cGxjMlhnc1pQenZKTnRTZWU2d1ZYb0tFZ2lRQkpTV094cjI3V3pWRWNNbkRRU3E5UE9tVld2ZFBFOTRRZXQwZVJyQmw5ZmdueGlCelhZNmI5MWZ3V3BNamgvalRFckUyNFpTSjgvQW9lbyttT3BhbDhzOHdzdlRXdldYNFVnaHk0M0NCdEdhaG1IU2ZwVDkxT2kvaGtpUm1hRkUyQlpaazZwY04ydDNteXFwQ3BJTjZlTGNUcHJUOEdER1FUS0FFWjFzeGl3dnZ5bnE3WGZMMkdiUVh3MW4vTlE1OEJCa29ZMncyd1dlMnRtT1pZdGRQRWJpRTBWaEx3MnhmeHNzNU9iWTNTZndVS3R0cW5KNlBzd0tqU05ZY3hLQ09zWmRPaEt5Y2FLZktvVCtBWldRUUJ6MmY0TnFOTmlRVGl4SVRtWXBQQVlLV0NDbXB6eWRlNlNoZlRZQ2hDbjI4Y1NhMytmOWtjMHRDNFlHS0Fsek5HbUw0K1JnOTFxM01ZQy9KNE1ra3Q4TE5WNDY4VkxLdkI1VklLOS96TGcvZ0tjREdITkxMMy81UmFub251UTJNMWRlTkdIOGZFbHIxS0tZUldualNWQ0tCaWFHdXZaMmhBRFlpWnZBRVplWE41TVZxaWN0NmVubU12MnBQY2ROMU12aGxuVTcrV2VjVVhpYUtiZnVLVURDZnRuRTJGaC8xZ2lQWHNKV3ZpZWx2NjkybXpCaEo1WVZ4K084K1J1YWcwb3kweGNJRTJVRzNlaFVpRVVJK2lPb01wc1dLb3hlczBXTnhpVWdqQlhnUVMvQ1hrNHQ4dTVDVUY2L25XNVZQWGIxVFYrZzV3aEo4R1lybVVUWWpOTC85R3hWWnhJOTY1WWh3a3ZxRlMzU3lxYnRmdHp0S2RpWStrdzVkRDNjd1lINTVGSVNLMlQybHM2R0ZzTzN0NC9uRTRaU3doMUQ3OC9NTzZ4S3ZIV1lkeVNwVDg1dVpQZWc4NXJ6cXdoTDBvT0hxTzVvdmhwTzZ4ZmVwQ1BtUDJyZTVZRm9SUElWZ21UUG9RQlJ3K3VLVDFZK1Z5Z1hlL0FsUUxuREFMcGVuTi9EY09mZ2p0Y0dxa1N5TzNNZmJsMVVWbld4bjVjTlRSNElyYm9ld0JHNTFuSkRFaHJlYnhxa0V1VzFnd2RiL1pYWHN3TDMyZkI2b3ZZdHhoRjk0VVV3emRzczA2RkxHczVBTlluV0hwQkNwQi85aWtWVHJ0eXdac2ZRNk5FRzJIZEd4c2g0QjVaNTFaaWdVaTZLQXVjK1RYL1R1bWQrUGxUU3VuNzZia0Rwb0FYWFpBck5FejUzK3RmMTZPUEd0ZVFnYmJBY3Ruam51cTdBY1ljYWF3akE5K3J2ZWdlYWN3Zkd4c2FBQ3o0SlJOZDZNQ05VUWc5aUVydjdITWVyUnUvU1k2eTJVMXozQW1SVzZja3ZwSlJ0elRjVXl0dkd6azBxM0U5STVsdUJtUlc4eE9GOHdrZ214RUozS1lSNHV5YURsem03L1lPYmw4RXZCUlRkbEQzNTVseEdPVlM1SGtmcDFuM2FvN2pMZFphM0c1anpFMGRjd1JaQzZNZTA0ZlRLSnN0T1ZkVmdDU0UwSGhKNzdsdnpIeVBCK21sTW5OQXFhNFo5WFZiR2dmTklIT1FOSzltL3U0YUZ6UlZrZUxJWHpFSEN2a2o1UnJYTEp1ZDJMSWc3U1NZc0ZDQm5VdVRYNzVNdjF2d0ZVZm5kNGUrMEtRNEM2QmFGVkVPSHAzeEVxOUUycDFxcDAwU0thbDBLR2kvM3NJSUxaTDdhWFFTTkd5VHhnNU5BVE9lOE1oVC9UUkRYNno3S1IyS0ZWWDRUb3NFcHhTYVhqM08xWGNFSCtHckdrSEYrVVJDL3Z3TW1jZVcwT2loeWlldzdGOWZJY2Z0bVk3eWdsYW9VSzJWWlFKMWxJdlJtYkNrek9LV0MrNStFV1J6Sk10WlpKSGxUK3lRdGVsV3h6amQxaFB3UDFCdnUrK0Irdm5oNlhqTmhtWWpLbDl6T0g2d25Gc2gvQ2hmWVhmOHV5NWE4a21RbGljQWVIUXNyaW1sdEFoN1ZHSFdNYXlnb01MMExOWkRkT1hEMm1TcTd0N0pWV3RPMElEeDJWaU5aRUdCcklqaldmMG04WWhSMzFNWHJFWGdtbEVjOXRkcUs1V2o0SDU3NXRmWGk5N0thVzR6S0pURHNrWkMyOEVLK3JsL3h0d0NRQ1IrNWNnbUd1TFBwOVdzRmUxLzRIT3JLRGZRamVFT0lzZUl4cndRcWpHdUtwak5ZemdFbW4zY3ZEcWJ0WnlCbjRqV2VCYjAyMlVZaEVadi92MFlHSDlzaUdxaTVvbkdOQTVzV3g3Q0FRSWZNcUo3R2wrKzdwdmFWSnkvWWIvZGJqVTNPanVpUjFmdEh0VmFqQ2lnZExiM2cvMGJaZnNUNzlIUktrdk9VTlZOZXYxSTc1dEpiMmlwYU11eThoSTk1cmFOK1pUR1g1aEdXQ2dhL1JwWDVaWnFmUWcyZHhYSDlVVHh3MU1pV05ocFpUcEhwaHg4VndxeHJtNm1wcUl5NzFDenhjcjZTQlYwdkZpOFlxZ3R5R0Q2YSthUHVHSGg3SjcxVmlUMkt0dm4ydXJSV0Y5ZUJLMWJsUWVZczIrdStPN2Z2akJ1WW9sbjRzWThRVmdHOVNhT1NhaWVVZWxsSHhDdkswM2FRMDR2TnhSV0wycnNabSt0MHE5WVhVWjIxVUwyYmpRTmdKeXZMcXUxTDZyRVhodXlsT3VLbHU4ZVptcFFmRE5VYUd1Sm13YWIzUkVEV1huQ2xyeFRBVGRyMlZrVTZVb295d3pyZkEyMW9qMlhCd3RBNmlDeUdBRTJPZXNldjJrdFBhQXRVdU1rRkVyRkRZY1BWZ1JCZDRPbmw3am02WDVVVkM4WnVQMDQrWlNHQlFzdHY3N3ZqN0t2c0NwS3NpVTkxbExYYUpmUHovbG1WTWFwYldOd3ZCSmRXeDI5aTlmeHN1bDlERFR5eGNzVlJsZ3pjWjdwaWJUN2ltUnRzaHByemltQzJBUmlJMkZzSmhGekl6bDc0NWVraGJDTmZ3a1ltVXFQZ09zc1Jtek1zZXlGd3ZLcnE5eE9SQzVZNUd3ZnlneTRLKzNGR083QUFhY3djMUExdjdOR0xkWGQ1SHlMS1RLVklYSFYxTU9jR3VwUy9haEsvdjd3WVBybWovbmhBVDgray9GRTFoYUJxbjhGQlc1NUVzR1BzS3JyWHBsV0haOHRHY28veXhlS3RuV2Q5Sk82NGIwY0dFamVPcDlzeUoxK0JjMGVxOTZteER6MzlFajdLaUpIYk5FcTZPWXhOYWlYU1NsWFlkMTNlVUhDUHBhSlc5RmZyR0ppZklESm13eWFWTERYSmVLUElDQnpEMENsTzdJV0pyRGhMZUZJc1pySkZkOUY0S09iZ1hyN3lqOFNpSXV3YjE3NjJ4YzA5SWFNbUgybW9TdjRNMjdTRHFRQ3RGdmdwcEdjTnpreWliWEJvSWNLVnVveWZuUHd5elR2QmxYNXl4K3owRHlwVnd2YjgyZ1gxdG1UMEdJWVAvY2ZHckZGK1lPclNaZ2hDai9rQW8rQnpqQlI5am1VWkxrQUlhWndoQ3MrSUpReEp0V094MXVTMXgxYWJtUGxORDBaZDF2TnNHVkZrcUFwTjY2UEVscjBaY1JFVmxnQzZDU0lyWHpzTTRoTEsvYmVYM0lBZmMxNkhNVUk1Q0hBQzRoUVRFeXZvLzJVRmE5NkpneGhKU0dRR1FzLzR2ZEFCeVNmVFBZRmtYSTBwOThOWnpHaC9pR2EvSnFhV1A5b216N0hWZ0p2eWVINUdDV21YUnRWUUREZnNVL1d4TDJnRE53a1FqcG5ER3VEYXVZYldNbk1JY3BZeTUzMlJzRlBCb2xUMVpRSG43VU1zZmpGZytIbTVVN015dWZzQkg5WktqZFN4YXNYczl3c3cydkFLcy91emxUcVQySkpVeDVOTVBsQWtKQjBDQkdxTkYzMkJqUWVIQmZ0R3gyWWtmZ28weFBNQXJXSFVianJ0cDBLZmd1N0NqalhEc1k2RWNnTUlxTjNyanI1MmtETEVIbEJ5VnJZd0FUZVM1Y21EQm1vdlhLRHowSTRGYXJJWWpsTlFIc0UzY3lJS3BpdzhQMVZ3azEzb0t3OE1MdlhKcm0wV1gxV3l4MXhYNjUraXNzaDlVSnRWVjNMeXhnc0RFZFpMZHFHMVVZckFXazQzcWc4d056MUExYW5LOXlCNENBMzdqS01JVnNTNWxIWkV1K1VHcm81ODlzTk5OMkVYUXJpNGR3M0lKVS90RHVRd3JaaytMdXVzTURNUTVTUWxzR1gzdlJNYi91WDdsVTVrM0lYQ2tXanRGU1hIQjZiUmlZUGEwZTN3WnJnOGVwM21CM21UV2tXRG9MdzFPSnZ0TG1VNkhhZGVWQXlEN3p1eUxZTm5iaU1EQ1FsMHVLWGJaeldicXFoRDhWK1dSRlV5bkp2cXFicGdqTXI1ZWNkYkpCNlNVVk9qYlIzUTd5V0t6aTNyMUEzUGM5LytlTmdnaWpzYXVLS3A0RHRZdDQ4K2s3WXkrVEdDeEdNY0txSm9yUWVMMjhXWUdNNGJ3amxGTlEyb2x3ejNkSWdTT1cyRnBiaUxrMGZYMTFjVnRlTGRGSkFPa3hneDh6RmRoR1F3aXczdGV6K3F6NENTRThUSEpaV3VsN2ZUUngwS052NW1rU0lxSlR3bkY4QVBSL2ZjZ3V3OXhCeFlZbnQxSld3TzlyQXlhZVYralNYeG44aUFxTklPWlV4djJNa3dZamlKd3pvcTNDc2N0U3BEem1PT1BqYWtYVk9VS2ZpalZqTzkrTk5mWmIrR0wzOXVPQjMxelI3eFFZbXl1TWFYVGlhNUQwVjhteWNCcFVuZlhPMlVHV2wzcFMxcDBWcDM4UlhLbjFBaHNGc0pKdkljb2F4dU5TZHBzbG1CV1hTcXJTVEZkWDJYZTJmTnVLMGhSRGdpYm02YVZIL2J1WGVDZy9WMHpzQzJ6dDYxM29UTmN0c25jeXNqUU5SOWJIWXhUd1hTRmMzOWdBcm8vSmF0aEE1U3FKZW1oZFgyNCtUZXVzeG5GY3ZDUWo4RjRtUHRwRTJvQjA1ejNwNTBrai9od1FqV0dzWXkxSkxGUE8vU3FxblJFak1ZcXZWU3l4bnRSMFR2bjEycHF2UERtN21Ubnc3RWZLYTU2dzlsUCtxTGRYOTdBRWJKUVdjdFRYcHo4OWhGOXhNZlVqRW51aWJQM2F5YmFpdG1rY2NOSnBjK1F6R2ZnamM0VzFScCs1UEN2K0hiWSs1RWhqYU42RTZaTjZIMkU2RG01VGR5amMwQlNmZS8xUG9pVFBCSjdWWnRsemErNG9XL3E3MWI0aXUzSkQySUpsRTZZcThSZVAvdkxFaGxNekZmWmZPb2RIdWh6VTlQUTRyMFdhTitjNyttRnRBN2p2MVo2TUhmSXU4UXZLK3Y4bVpQNTYydFlhWDY5SS90SExhZnZkQ3p3L3M5L1NhY1dBOFlPWWRSb3BmR01RemZxYklwczFLTTJnYnZJVWEzd0gxbERGcHl2VGVWZDVMUmU4ZHo4bkRzUWxYSGtla3U1dlFIajgrZGNoN01XOFIrTXlJdzl3VjE3R0YzeFkrTFVHelNldHg1NE1DTnl4YVJiVTRLL3BYTk5JVlVwdGY0SGV4bk4zMElKa3p0OWl4VWtMQTJZV3cyNXJBdjJTUFA4QmhDRXROV0RwRmhpZTlzVUowQVRPUUNOSE1WNTNkdUlhS0lzTnFTOGo5RGNZaHVjbE11aHpZZVluSkpLZ0RxUjR5Uy9ucS8vK0JLZkZESWRjaE5iR2wxT2c3QkNkUnZySUxucjB2bGJyRlp2VnRxZmdQWVRETGhVeDFXNzZha1RBL0tuVE5idjdMMmZCR1VBMEJwejdZUlhYWlNuOHgzeFU0Zk9ySzRha3Nxc2dYc2hsRFBjR3ZQYnFRVnk0VlpGSzdtdlQ2SVVtOUNlMFdrUWIyaFRhWlNzY3pFS3J3YVFRZWJhcXJlNkc4b2J3bjRLOEZEd2czVDlTbUxwUnk3YlRKODBackp4TGFIWWpvTlgxbThtVUUrSFJ0MTVxOW1DMzZLd2F2NXdrbVl4U2tUT0xKTDkxLzJFYmdUekxvRGt2ZlhkZDlnQlc2d3JYOHhhZVpiME5vNGcvQ3ZsdS8yZ01wVjR6YnJEUnlKSGZML2ZadU16NTduRU1qU29yeXVldk9MMTRaRlpkQUtXSTBBZkMrUEZlRXFUK0FjMkh0QXMrLzdhTkdNNitEeWtEZ3F6YnAvcVViRTdFbUN2ZWowekt2T29Jb295aWZGZmxaMjkvc293TldPU25GTVowNExWRHJjL09MWXo3bVFQbWhjMzRWTGhpTHJqSVhGaWpjRFhoclZXeE5RT282YUh3RFVmeENRUjJpdUhLcnUvSWpiODBrUkhKYnR0Vmh1OEs5RFFxNnNHcCszNkdpdWVmOGRSRjJtZzBWdDRlL3ZtNEN1amxaTmxNeVV2Tm1ZWGMyUmViZEhweXM5SjZYbnp3M2hSVkNvNEoxd0VJMlBRcE1xOXoreVYzUTJ6QXpnc3R3RFQrc3BWLzkxYitHTG9FdmV2YlBVczRCakEycjliVUd5ZGZpYytyVitJWUJDUzV2dnBoM2JoVTlFNWo5aURBeTZtKzdqQUdOenY1eGJTbnBPOWViRUpLRU1qc3BWV1pnTUU2cHYvQTRrdFNidzdCZUh5Q2pGQXdvZDJkZDFMM2ZlOFM1VGlad09SbU1nWEFGVGt3OW5VVUNWT1dGZnZjOUNRdHVEWHpHc3VocndzazFwbCtuYy9lVXZNYlNTWXJjV0NrZ1o3RkU4NXZRaWFzSUtmSUY1OEdkbVpvUUpGbHdUQUVLaENuemRtQ2tGTk13QUVLNTJQZFVVV1lDTGY4VkpuR241aGRHU2hUL1lDS1BVVXpuUkJiOEZUaFRwbkRiWUlxRVJhaUorK0xadjVRT2lubVp6OUt2cjFzcDhPaW5oK1lqRHpaQkhUTDhkU0hraGM3ZHVtYXhNMHNNa0RNRmJZVkx1WEJHMk13Um9iQk5BSi8xSUw3eDBlMUpwZjRrTmg0SVFldkdiK1NkbVZld2N5NDMzUHBpRGdORjhoUGgrQldrRFBrczlQZWQxTFI2SmF1UGFmNmNYWTVUUUlzZzFBR1NLa2lUV0pIcFpUY2tkRmUranNZZU9EcVJIVDdKSGdTY3VsY2F3cFJ3b3lUdkdvRk1mUExEZlo5QndhUjZ4K0ozdU1NSlFWajMvUE1BQjZjM0FBWkR2RUVXR09ITC9IaHFmQkRMeTRlQzVucnB2RDczU1NoTjFxcEV2WWttaGpxYTg0c0lPR2Q5M2FVcUdwMnhwMzJ3MGVZYXJic2ROckFudGcxTDQ0NVRBbS9PVERybTNxY0dYOG9mdlhjOEdtbDk4dGRYd3BYVjY5aGV3Z1NyU3ZTa2dvZkllTDZNUmI5VG1GeHdxTTJVb1NYRlhCSWVQMkFYWXl1cUJTR3dualF1YzFtazNEdUpvb1YwVk9KdDQ2WGxRb3JBMUtMdkE1VEYyRCt4M0UySUlTYTNGeVZhNHk1WHlDbVV0QTlncjV3ZFBUVmdLRCtROFkva1NTTWxmRWVSTTBjT2Q3enFuRERQcFBvS295QlFUUXVCWEpERHN5WFBwZWdFaWdBY0NvWjF4N0hNLzdJNHIvU3JWdkhxL0J1Z1ZKdFlHNTZ6alRSYlNRb3BsZkFZUmNZY0JsUVlERnJGSE1hVHNJUWJsVVkrclB3VTNobjhHc3lUZEZqV0ZJeHdpQTMzaHh4N0EvbHJ6UDVXR0VuLy80SFpQNVVkL0V4dENxVm1WOWNERW5kNG5STlh5cUFtcmRlcldOVmhSMDRRVW5tcHgwbmVrcm1iN2lkaUttZHVSQ3BFTnJsWU9JdTRwYlRIU3BneU9QWUU0WDhOWlB5QUswZkpDUzJTSWk2Y0htdE1HeGl1MVd0bk1zOEZCWS9kTVd0dlRqdTNuRW5GUG5Eb29xUVo1cGtwR3FBcEVueTVsa1c3bWhNNHo4NVNzellFenh1N0VmN1pIUWFoRTBFbzhveEduYVZ4TXdubmRFZ0xjZGRpUHJjMzFlZUdFMXoxL2dFV1BJZ1Y5a1BMR1hPa2tDbStnWFp1R2xJWlhINDR1TUpldUxxM0MxdGZMdTAvMnpSeXVob2ZWM1B0cktmeHFoaVVLMkczTFFBc0MvV1ByeUNUZ3JvRi9vdTBjYmJIZUJLa2JqajErOENwb1lqK0kxQ01QNHhQNkVENlAwOXdETDJqV2ZmMkJLbUw2NmVUeStlNVRCOGRybGJCdTRFcWZpY2ZhaTlvdDNEZTNFdXgzY1pVR2RyVVhNWG9FWWE5WitDS0E0QmxSU2t5b3I1SFNXYjI4bm5pUUNWMFlLZkFvdnRMOEVyVFhmaDd4czlJZXhlTFdqdHN1ajJaUmwzOWZpakxadXZ6YUJ1TjQ1TjVINWZKQ0JoVWdkbFhPRGRzVExsWDVMYWM2bFRGZzYzL29CSjF6VzNLaHRuL0tGYzZVZ2VRU09mTytHem5aNHdLSURZVmplMU1VWi9wb1JxU3NNV0VIOGJHRlRNVkNwOXBEaXc0TGZDK21VTTdKSG5nWFF4MWhjQ0oyTjJqajRhVnBCTXU1V0xVekNiQ2p4bjFBMDlNS1R3Nm9hcFJYZ3Z5aDVYTlhFb1BSbnY1alRlRVVOTHJwOEJkaEhHT25IdVk5U3FyWkxaY0YzcEF3ejE0Y0djV2ZkSnVScG1uRGxzOEY5dGFiK3RmUTBnZEtuQVFHZUJvZVd4VGp3VTl2UUdmUUZabHRIak9nbHA2cHVZZ1ZLSFg4N1kxWjJReXNwM0ZSM25zTTVCTXlxbEtFUmc4NzB0bXI3aTBRcDMyNFJzZ3FSV003cnFvNGFLZTh4NDZ0Z3F4eGtwaUdHNkVsbWVzcndCd09WYU12M0JNcG52b0RxK2l4S1VjVmhDMDZtL3JBTnRwK3hleXFQdWFzU08rVUtYTUVmd0NveWl5WlNwTURTMTJ3QjJ0VTlUdHA0TmsvOVRhNHRNLzc1dHRCOUxiVG5GREpkdHkxcFRxaDZ5RlV0N00rNGZCZTZET1RzWXJHRE1xK1p2OFZBckJjbzZPUDIrYjUyb2dIdXkvaGJHOTM0eWM4Tkt5djhFMS9FUXA2RGV1ZmNRMFIxQm5mVkhSQWt4R3BDdkhPSXhDTWFYemU5VlpHTzJXUkJzY1V0V01Yb3p2MXBTNGs1TUJidHNBNDBYVGpPa2pEY3ZCRElvRlA5T2RqM2M3aDRaNjFvaXFzM25YemNHU1hheCt4NWZmV2tMQnZ2NXdOTjlDdm0vQ3pJTG1VNGZTcnE3a0ZUV1V3YW5qZzNYa2FQWkN6V0pjQ0N4NXZ2WldYdmdJeFJyeXR1RVArTzk5L1FjUC84OFh5MXMxc25lUUJPdWpYYU14SkFEcjdvQ3NoMTRaWVg3SS8yQnBWZkNzclRKOEIvRHp2bkQ0NDI0TE55Nm02KzErQVlYZUV3L216cHNnb0lyZ2lGcUN5R2dmT3I3UTZzdmxzMjluNlZ3MFBDTW5pNDVMT2t2YndqelhMcEFuaWVGVDIyMzcvbzBvVGhzN2JlYmRtNDdPcFRheWxHeHhwY0N2STVNTEtlNHlJa2lMVTUrY1psWTZLM1lla3V3STVyZ01MaUhxN1VOVXo5czg0YlZsb3ZRZ0RVNXhmM3FRQWc5MlZtdXRaTENra1FzRmwrSVQvekZZREtVR0RCRU1GOWxLOGJBaVdocjJwUXBlWHlCeGxNQlJLRzBZZDloQ0hFK0JGanp6N2pucXJ2UUppcHNXZy9sZ1IzNkZ1bTdzcWN4dzVuRlpaeURLbEYyajRuR1N3ZE1lMXl4bDBmOEdXZ21lM3hRbWR4QUlBUVRHazd4U0NZUUF5RXhmL2FRcnRWT09sK1VkTlU2eFJWdjZ3SlpBOG42M2RwclpJYnduRi83bUM1ZkhFUm1pdGFzTm1jOXFmcUl2UnlWT3VPUU5aK1dUbkUvcUF6Z0xFb1dKOEFtQXJOSU1iSk9RTEtpNndXWFBUK24wRkZHZFZxUmQwY0ZjN2NrNjlGYy9Fb3hQRzF6SFlRRUdIelN2eTZ1S0xlYWRmUXQ2NmxqVGRwTjlYSGExVlhlYzdUcmNpazMzcTRadFdjVHhzN3pEMTlWUExCQjRqbDJMV1UxMXBFREdKaUQzcC9jRnp2akErblNqMU5raFFwbTF3c0RFQ2VTZUtZcUdEcUV4cTRLR2dqWGpzckJtUGNyNU1vTHBsYjV2Sm9adjVHSEREWGI5dkRyTkRpc3g1MzdQSldGUkY2NUlpdU5xaEV0NnVKT0ovdG44akw5K3BWV0R1MmFhdzVjRjN6S1pNTzJKV2ZIWDI5LzkrWmRNMVFJT01IWVMzTHZRZWtONHRTN0J0bitFN3ptQUc2UUdpUVgydjBLdXFmVCtIZnZVcU1qdjB4ZHRNYjdaR29mOEVzNEhtN2JjWUgyKy93aWx4azJmdkxmNDk3UUlyZHBGL0xNbjN6d0UwRWVrUVZDOFdTYUgvVWZiWFFQdVhSSTVTemRtQzJPam8xdHdMRGJBNEN6ejRBRVVOYnU5cXp0YXZlSGw1cyt2N1lJRWw3L2lwUjlRcTFNVVIrSlFPYUdvWVByTUJFdEhydjR1TUxKVGNFbDMwWmVMUEpId3cvbzNWemVITkU0emhBa2VreTNybUNzRmorOXJxYzZwYnRwZTkySWhaQ3hJTmppOUZETnpQTnIyVGh4STV0OHA0VjZxNG5WbEdPNDFCTStDMlJHck0yMWxWQWtNWHVLMjdId3JVckJWMjY5bkJVNWc4Y3dCcWI4QnlFOXpoL0poKzJpVUIwY1MxbTVZOGNsWEpXMVFWNzJFVDRnYlhtTWpFZ2swU1pnV0VhZ3VuY0R2STNWYnVjQzRNYzZtbXUyS0huVXR1T1FiZEFMVnlUU25Cd016RE84b0F4aVUwbUVEZ0pIYkdRUVhMY2JZalpXS1VaMTAwdkhnQ3J3Vm9Lb0NjSXBoZ2VZbnVhREltdWN4Ny90NEVkQ1A3UmJpQmtKMnVNNnhUeFdDTUxpTisrSUFCSWlsUWJNalduTUlXT2tMcUQ5TVY0V21SOE5XV2F4Z01ORk1nd2plc2xwTEJFNjY1MDQ4Z1lIeCs1dEZOSFNWQWc5UTk2bmJ3REtQTENnc1NrbDJGSXQ0dEJKb2ZxUWtPQllaTFgrSWlpc2thZWlmYk0rNEpTRHJpRlV1M2lFcDZYVDRmd1JJWmlGeGFWd0lYVXJHM004NnpjYWNvR1dFWkFsTi9ocU5TZWNDcGFqYW5saFVtbUd4K0RYaXBNL0YwcUZRZVArSjdUbndiN0xrV1dSVHowSXlIUDRjd1ZUNDNuL0plZWFCTExMYlNka0tWcjVRazdGeXVYUWRQUGdSeWlMdnhwRitldDE4cXAwM3Zna1hCclo3RzhaYVNpQTI0WldXNm1Ncll6VkpnaU1lUWJvdjJQRlpQWUp6Ti93SFFBUWFTSkRxL010SnBhZmpRM211WitrWHNOWnk4OUY5YUhOanBDRjJ3eTJSTTNOTENyZFgvc05wMHhFTkdvcndRTTlreFQ2QjZiTjlUUGtOZTNZU0dET2doaUhxM1p1eE5Va094MTV2MW9PRmhXRzYrZlplN3dxbjMrcjJxY2doQWZ2UlZ1RzBHMUhXcE5zY0JkWUxOMm5zS21GeVhEdGhXRFh4YURNL3ltNDdGSE9iSjh6U3RWbUhCbmZPWGduNzFTMkxnejhGSTRrcE5VZGVDeUZiaXdjaDdONWE5dWM2eldMUzJqTm9Qc1hEZGQwUlZjRFpGYWREd3VGN2lzVTNTSVlUZGVzVFRMRTVvY1BCcWIxbVgzb1pLRUNUYmlWUGs0cGg0VnhLWTVSQVNXUmNwcjFDZ0NpVGJrTHJlam94cVBQYVNrQ09rNkx5SE1rVWVKK0h6MTdveFBwTVBjWW9ZWkJRa3RUZ2NJSkZXUHR2Rm9CMEQ1VkROWlc5bnlManlKNGhLeFZsclVBVm16R3NMMlM3T0dJWVNrL2V3U2lGayt2bEVrNERXNFJNdGJMZVJoclpya1ZyZHl4ek9qa3VtOCs2WnhKWGU5K2RmSzh5cllzN0VweWg4Y2drRmdYZXhpTUhrUXJUNEJUS1pyOHFBYTRGYVBTeEZDaUxFSm5NdDJYQzlCSlVOUWUxTkUvb1ZLbFNFV2pLNGs5eXQwRFJBa2o2M3l3N0RzZ0cyTDM2eTNFL3ZhRUplUGIvZDg2ZmlkL3VPcmhhVGJUVlFRUmNQbWR1R1JoV3p5SmVsSFdQOUNaTkVNWTQwS0tUZExGWDNGUDYvbjJKVUluaXl1WnJZV3R4MmQyOXNSVTBYQ3ZLMjkyd3FTL2gzV3Q4cXFqSXJ6T3ZGQkQzRC90WXh2UFBhSFFRVGN5WWZCT0VHbDNDQnB1UzRYSnByWUwxelpsL3dldlVudHhwaWh2MDdsVk1qUGdJSERwQ25TVHJPaWxnRU01UjJoY3UvcjNQQXBsU2FXRXY2cUl4UDQwRXoxcXJUS0p6ZllTSlhldVppRTRMSkMrcGs4K2cwdGxMR3pJTldFU3l2RnRhUmZ6RlJ2eWJrUjNKZGZuZXpram51TE8wYnFjSUVncUlrVTVaUEg2SnFKRHV4Um9KMUkvMlBJT0VHdkVpcFVGcE9hTW5wanFmWER0TnZ4a2NUVEJRcDZ5ak1GVEh0TWFFelZZUGdkdEc2NEVFT3VSU280YkpWbUFQb01nU2NLQ0llZFYzdjR2WUYrS2VrTjZVSVJYUm9JNDc1bVh0VnpYQzc4a002Tys5VnFTOXJ5V2NWWVVmV2lPb3FvSUxDNHZibnFzcGVTOG1EUlJ1bW1rQ1RMUGY5cDRmcE81T0dRYTQyamFsdjRpYmxtbDh5d0dFUkE0Y3BjanFvSnp3aVZtODY0VFR5MGZSa2ZSVmNhTDNVM0JUN3ZjNlhWUE1zaW1jelJaVDNUY0pEZGxGN3YzMUZYcW15TTRVQ1dmcjFMcTFBRkdjdjFQNXhJdGdQbWpQeEcxSzZZeTF2TnQ4cy8vcWE2UmsxNWh1MmZRakhYcmZTSTRYeEtMbGtuNk1Yc3UyNVY5VkcxemM4cWErVGxzU05saWE4Yk4rYnNOV0dKeExHUWFiVHBLR0xvQzhHU0ZNbXQrZitVemVBTW5kYjMrTlpMdEZXayt3NzdtRnRvZHE3SWxGSnNtZXkrd0s4Nnk2bHcvYlBKMGxpSUZpODRnaDFtdFBPdFphY2M1WVZTSitkNHpNRmhEYmFlZWlmcFNDNlc2REdvSzI2VXdhd0pCcjcxTHk4b3V4cXV2MnhUdHFrczBnNUtvRnFBSXllTnorN2d3SlcxNGdXeVVPTXZHeFg2T0RvRlhoOFAzSkR3emw5ZmRNUjYxRWxWSmhtL2g5OFkwR05mUnNHUlFLQlhtdTA5WVpBS05qaDV1SWpWT0ZvalZ6QU9YcVdTbnlkSG92bXZEVXVRZkVWN1hwSmRTY1VKZFQrRlJsaW5HOVVOZTB6VGdoSFE2RSt2bDBPQjU3K3lVaHRiRVN1RSsxUG10c29hMGxUaXJ0YXdtZVkxM3FITjVhUXVoQWNMNGtNdzh0MGFYdUl1OWtWQ2pXYU05Q2RsNlZOdk1IeGpqU3FHVzNCSDlTYkJVWERWUkdmb2pqa2ZLOTdXY3pqNnpBU21VTlJRUTYvQzFHTjh5VWc5OU0rYXJxSzA2WGFsRm1OWlBxakIvc0VhS2RCck5BWWV5eC9KajR4MkdqME9iYnJBUzJqeVpXWkEwZnRWNXRmVlpMdHR3YldVeXdrTDdsTG9WbzBibWl2Z0NCK0Z2eTFyTmc0MTFIb3UyblU3U2FZQmxNNlZ3djRDQXlxQWcxd3ByNU50Y1BKL2VuQ3NYTE1OYTAyN2hlVEpCbzk0azYvMDZSeFB2TkVPWWRnQTBiK2VqaDQrbG1jWTdlSVBDaUhDejk1RWppaUtTWVpMMGN6VDFkUG10WHlEamk0dDFubUtVOCtaUjdtVmVjQ09hY1hXd2M0c1NXQ2NWaUJTL3VuVVgwQXpHd2x1ZHRjT20zSGhncFJ3bVE4RmR3NzRmUDVtV1lsZUdKdlk3ZzdZY05Pc2V0Si9ZcGhLK0ZKYjAzdzhZOFZQQnlLSW5PMjFUQ0xrSnhCR2YvY1BSL2VDTnduVVdGWjU0UXlMOENpb2JONDRwL1h6UTJyeTdaa3hmK2oyejNaUlQrNlhjKzJSb25yQ0RHMEF0OXJDTUNvdmxoWksrRnNpUzNXNWhNaHdFdWorSTJtSWhDRUE4L0NWcjNjRzUrbjNrVkF6S1ZtOHR4Sjg3eWxWYVZyenlvVVNlY2g4a2lXZ0NaVmhvVkhKb0loRXBOcmFZQk1TdGd6aFdSRkhFSzg5ZWN6Zis4V0dOZ3R6TGp6M0UvVUFObERScGJFRFhNNkNxL3Z4SHVDdFhqYzBMbHVIdTY1bFBnaTVJQTNhQ0hOZWJHY3IvbFVieEE4QjcvOU9mZmVRQzB6Ky9HSTR2ZUdHV0t6UW5mbDNBQURZY2J2UWJPdGlHQVRvTFNVNXZncEtsTHZwUmlsMXNMRW4wOXlCdFJPcjdRQXRsTnYweHNvSlFDanVoL2JSejZsaUw3Y0ZNdW9sUTUvSjFDTTJSU0ZWRUd3UmdBTE9za2lvNzFEOVRMc3V2SDJXZCs1MFV2OVllV2cxZ1M0OFVheWVWdWR4ZFV5UTRzYnBQeUlBNklOaGlhN1pqOEM3RWVGU1lvOWhaekpZT0IvNGw5VHl4bDVxYVNaWmtGbFR5YlpsMTJ1WVlxdFhqQW5qMUo1SWZ5RURtdnJwbXpQai9abjhPUmgwTjd6dUQvUHgwTUVqUEFKdk9MY3pEWFl2TzZjRXJYcFB3TThDRG9GQ1dmTC85WnNTZ1Q3RXQ2S3JEUjF1MmJkYWo4dDV0eHh5Vi8zWDBMT040aVJPMi9lK1l0bE5IQy9xWGxreXM3K1lqTzh6eHRqbjBmbXltMHBoWE4xSUFrNDlrMU40L1BVSFBZcGFCS3FUbjRZZERHYTFXVml5VGVGQXkrQkkwY2V5Ky9NUnVqTFJaZE1qeHVRbXE4c0VlUytQZFpmQ2p4aXYzdUFDYVN3U01hRjZOc1IxYW1sdUlxbGU1MjRhK0g0UWtFekJDU0NrQzJybUZrK0M0WUxhQWNXZG0xTGtoOC9MVEh0MFFRdlNRZGtJUlFSd3RjOXJKNytXb2FCaHpxUUVuZ2ZPYnh2SVRzMjZhd0ovaHF2aXRTR25DcUE4RnVwenNjd1BaR2pvcTBWaGRCZXIxOEdFcTRKczVRRW9SaWEycGZyV0YxUlhxa1oxeWVlcmRQRkRRajNJR2pVQmpWTG5MN29mSXBXZVkxZ2RpdTF1NVE3YVl5SnNkQWFBVW1aR3lseUxHYzZ0Y0lSVGJpVkwrTkJSRHBYQ24zRWEvdVR2NHRzWmFrRVpybitCc1VIYWxUWVc2Q3VWYkFRUlV6aVhTaVk5b0FHbFNZdUhyQnhYM3JrRktjYS92VmtubVNuM2laWnhGeFVweTNSYWtnYVF2N21QNTFpSkdWUGZKanhaU29vU3VlU3k5N05VWGxUL21sdFpDc0xkYjlNaCtOUGx0dE02bXlBeFBPK2NsU1NTeUttZXo5ZFJNM3lVWWMvT241OGtMRFpTcmpqanB4bXpSY0gxMHlKZEliVTRkOEVQYVAzMHhlQ2VrN3UvY015Tm8yRmpJR2dBdERMZjYzSjhrczFkT21IUDlqbityQmw0WGdWMDZuVXBVam5yNWtDMnVuVVBRVDBVeGY2cVlmSlkwK1c2c2tTc3JRVWZibm9sUTZzV00yZ2FlRFd3ZTNpS05KZDM3OXo4VWgya09XSVFhK1hQRGVVSUxUZ3lydmlkcWFWOGFxQVQvZVU1Yjd4K2VMTzRJR2kxeWd6WUl4Z1dLQmYzbFdaZkN1dVF4eGpBTmlPaStEOWRzTUlUVnZydTNDTm1OSUI1N3NJQWJRUUw5dkpiUEluOGhnOGg4S29WS2x3ay9UdnlpS3IxTXlNWTBoTzhHZEZtbnpYSGw3UmVUdFUwdEVxVERnN3ZBRFB5UkViS2NZQmJhR3lEZTh3V1luZExiTXdtYTNFeUdpV3AraXJLQXA3cWxnY1RzWHNPempXMDdjMGxyS1NzUXFqZlEzMnpzcjNERlBuTVBlTFRjclpqb2QvWC90Y2c3S0orbzZ1QUZUK2tiTFpna29ETHNUMnpLOEdMQkRhVEhaU1EwbVFOSkRBMkpEczZXdUExVlNCTVlJTlpEL2FtN2NBWWtuQ2VYRFRXV3hHRmVHR0kvVzhITVRYREZxZExlMUhqWlh4RSsxQVA4NHdadnNTVlJRckNWNVRtMHJTdlN2ams4NFlaUDI2ajVTMGtUZzJSQnNlTlptUmJxci9KSXlhaW96N3ozSjBpMnkxNnc3citKQXN3Y3ZuUTNmV3NLNXhycjI0WVlFRDN4Z3FFMnN5VUNZRndHa2lEbkttSE1NcUpaQWxTVmwvOWR2ZUsyQ2hVZWZFN1F1SnVLUDlTSjNlbzRwaE1SV0VVejVXZGFMenZJZkEyRDRtSTJWZ3lWZFZrV2ZSbDZLWHhpZCsvWG83eGVYSkZuYWRTOFpiUHJYYkdvSHZ0OHR4SGNlS3QwWjdTYWxTL2xpYk1NbWVtVTJLTWVWV3JzK2l3cUQ3OXVHWTMvbGhJUkVpMG5nYlN5RWMxK1FQSWZibzJocU9BdHc1UlpyR2Q3aVJ6Y1dJSmtoVlB5djU4R1JQQ0RqaGJzYlFaUktFcm00a0h2dks2NTdqR1psWEV0cVFkQnVTUTZROXUvbDhVMVJDbit5dDZSNENzNUcyRzRjekJpdGZFMG5ISmY1TFNsOTdiYkJPZGR5Q2pPUlQrYWIrd2ozMkJXMmlzVS9lK0VpLzdTZWRSaFNzOHJtS2luUm9rZjYxSE5hK1VySEdJV3pwMndkeXZMeS84a3hQdkJNeGZ3bEIrNkFOdG91SVRzVVhpdnVTL3lyRWhWbFd6K210QnRvVURiNFpjNzVQTFZmV29QYUZEbDh6UHVWdWtpUWRZa1NlRkZySDZ6cHhCQmFyc0t1OWtRVTBVUEJYKy9vTjBlSi96VC9DTFY5OGRwL212NWVmQUNVWWx6V1dXL1R6MjdyWlI5ZElmN1FSeVEyN2poK09DSVB5bjRXM3J4b05POWNIbTZUSHpmYVZUY3ZMU0RZVG5MOXNmQ3I5ZWl1TmltZnVRYlU5eU5zWGpGeGFMZkk5b3BGaHZmRW92bDdKVm1LVGd3MWxlK2dBZG84Rkh1dUxPZm0xcWFDR3d4Qmo3eXZ5Wi9ZL2hWUHpBWllsaTJ1N1k2eDNTdVBtZVYzSHpHQVNNcEpSeHBaYXZCNU5rWk5XeStoOStoanpIQ2REbThPWFVWWjVXWll5aHRSNitnbm95SW41RlQ3bEN6cTFleXlrNGx3UE5ZL005OHRjS2N1dG0wWWJPTUh4R0tLTHV3M24vTXhqaDF0VjlzcURxWHh3TDh1bS9taC9GS1YyNXNvc2M1azNnL0hBcFNFWWJ6bjZ4Z0JPeTllc0xXdGhpSzBmL2FzUmsrNmNKWWF5OFhMQ0JXTXl0UHp5S0E5ZS9GZjRWQ042emxKb1hIcGhvR1FhSEtqcFJUMUllb25RM3ZvUWgxbGg2dzlRYWo0Y2FYay9QdEkzK2tTUDl4Tmo4cjlwdGtBb1I0bURUODJuaUlidU9NZTlYSytXdVZycTFOR0NXSVZOeDNjTHhtNkFEekFJZnZZOXJ3MDZTVzlKalBzMGFmZnQzOUdHSzRLNVp6R3pWUVV3R3JrWDU0SEx2MVJJYklGSlVIb0M0WTVmL01DQ0t5NU0zY3l4VXVLN0xtWHZVNGJkWXFZeFVoaFVmRlZtTzVKQ1JlZGQ4N0o1dXpUdzI1MHhYUXVaLzhMK3pXaW8yL0ZmUm1BeTFqZEdWS29XdndUYk9WQXZkYWg1ZkZEYWl3UHdEOCtDWjZCMkRzOVdSK2tCQ0FhL3hYNHIxZ2c1VHVsaktNSkNkSVJpZ0pNT3QzbHkxSnJZZnZHNWRyTStwZ2JYTjNZVGlaY29sbGFFdmw1eENSZVhCYzAwTGhrTWhya2JlbCtKZStNL2xibE03dDR2SzFnNWpaaHQ5NDl5VHF1dmpoU3lVanQ0VlRTRm41aDg2R3NkRDY0SVQ4cVFNSWNGdGlUdWxLVFJ1aURJenpjY1VHclE0VE0xVVI1RUNJd1oyd2tkeW9lM1NhcGorbGtvV01MQlJOakxiUzgvZ0taZXplMTRWbVBHZ3FYQ0haQmVGMWxVODUzMHZDUHZEN28vZFdzRXVrNEQ0S3RieFFpbUU4MXBuQmxmSXV5ZlhXbUVOdTE1b3dXT0VabENPRnd1eGd5VlBaODZQekdvUDVnQnY1N09LK25HaHZ0VFI4aXc4WkVBQlNVWGRVWmU2Q2pEcjZObytpN2FnempydmRvSFI2YWNBQXd2Uk1tdWYyamFnazFUcHJnN3pPQTltanhWdGhFZlEvSGVpRGxuU0RmQTZhSHVlR2xJbEM0eEQvR0FjcUtJZUVGZFRXVG5nRVNXejZKR3dQMWQ1UCtpL1Q0endNTVRFczR5a0ZyK21USmdVeDJhWUFUY0tWY21saGpZb2tuT3c4dksrYVBWbFYza0JpbmRFTThzdWFiL2FKUEZON3Y2dm5XYWRyc003eFBXOG1zM0tsdGd1QkNleG9FZ1FBTWpxd1V2UFd2UUJldW14clgvajhMYVcxK0VJS0p4SlUyNUpZRklTU3gveEllSU5uLzVSMGErUHh5YkNSU3pRdmc0N0Q3NUtkQWRGR2NZd25kUHdZTEtoSldFYlo4Nyszak13YnJEVmdXcXlaNHR1ZWplNjZzUlo3YVk1ZkZGQjQ1V2JkK1FWWDBRYnpKQUdDcmJyQ05QcGRYaDNmWXc3ckxkc29JdDJ2d3BpNE5wb2VzYmd4VVNjSllCSWkzdkRObFhkbFV0OTRHNmltMjRTdTd6RjExZ1pRMVN0cFNuY3Y2VDhFeTdBbUZGTzloTi9oTXJqOVU4YVVTSDFtUmtTZE5VZW5ZeFNPYXprOUpmTU5ybnhZMVdQZGhtY0RHbFV4VkptTFR4cjRVM1ltUGpiaXFwSEZjUzZKUlNhK3dSak9Cekl4ZEJDSFphdmFoTDJpRTBMTzlrRHBkV29ITGpzMWlaMmdLUkt2TDRoOEZReTJwNXJEVHFJcDRDMFpvS2JrM0Nscm9mQ0hsVzVKZVQrbTJwVDJ0T1BuSk9rVGkvakd2ejhSRkh6N0VmaVgvcUtndFhHZlBOL2x2UmZ0U3hUMGRMQTh3OW1FdDRibzV6KzFNRmVhNTkwbUE5SEJ3WWVXdGpuZEh2R2IxR2prYklZTWQyWWVYMWtxSU14Wjg5bTYrVUVCOFNUMVdPRmRWWkYyYk9OSklYSEVNRW83VE4xV2JhRDdxSGlIWFhpYk1OcTlTemhURjlJYUNOUmMyTWI0aVYrT0VMYlVWNFByWC9XdmM2cGpxdG9uanFTOG5VS1M0OFlqSTJ4dUdlQVhlamZFUHdZZHFqcVMxaVpjbFRDS0ZobTRpL0FPZE1pUGMreDE1VmxaQVV0WmpSR2VNeGN6WVVHRHk0TUtUZkFCZ2hWWFpJVE9kc1NVd09ENWYya01DNzVZbkFSeDhCQzQ4dnhCeGZKRk5nREo1RFdMNVNjTzI5T1Qxbkk2R2ZBOGkrYkZNckJrb2orQ3Z4M2Y1UjJhaG1sS09xd2UyWXhCWWEyQ0p5TVNlWGo1ZHlxL096QmMyMVRZNjZBQTloeGFuQXJRTzBYWVlYSk9IQ1c4WHlZYUlITEVaWG9iTld4TVhqaUM4SmlYLzNVRVV1cGNPeEpJdXBSR3hHQ1FHczduU2dJc1ZrNmE3eGZtV2RjMFFIbG1IRVpyYit2OVA5MC9uWk1yNEZjKzdHaDdYK1d3YWdvbDMwQUJPa2lnaGU3OEtxNStTZVZaOEVCZXFaSTBrSHRrazBFRkprOHQ4MVYxdW9XRFViRHp6cTllMFhEajB2WHNyQlV3cVVuakNRSW9wTU8vUUhNRXMvL3VUTDZpWFg0WWlxVWw2MlBBOUp6Q2QzNlNFZVJVaU8rSFFMZmFNazkrZURiRVlCaXdTd29xallCVG9KRXhjUnNIVXJ2cTZybnlZTmI1TkVSNFBpbjJaOTZZNjVLVmw1Yld0YWsxQ3BqSGZMMGZoN09sMm8zZFBsT0dFQ2g5WFA3ZXA3N2ZYaUh4RGpTZG92N25kdVdhdDlBVG1kbmh4SnllS2gyOHJaT0Mram5hK2V1ZW9DdUFrQkwrTjlweW5ENkxPWUo2RWNEdStzTFdaRzc3am5xODBwbVpkYjZKRlZBQVdzMVlmMTRwK1VMZGkrVklLejZOT1I0bk9qNjdHTlNqWEQvMlo4ZDArLzBxWWhKd0k3N0k1OGhDV3JzQ2RKNmR6YlliWHZ0RjBCalp1U2RxMUJWMkIvVjhJNncvSXBFZnBSajF4OG9jSzI5Q1B5c2FzdVEzSzdHWU9LU1RSZ2lrMW1TcEt6ZHRaVHhiKzROTzZ3VHJYYURwQjVtRXpoazIxdWlveG1RU0U3bXdBWTdCeXA2VjFXL2xMbGdqUTk1aU1DYnVMeE4vMGZzQnRIdDlJQ3BhbFFvbUNHRjRLcUVyY3FvbGtkRWcyWFZvSkxxQjBmOTdRT3htSWp4YTFyUmpkSlZuTDd5cDJvR0ZydTFmNWNWYjFVQ2p2TmFEcWtoK00ycWZNVjlLRUg2TWx5eG9jS2RLOHNSNUtRQkNyYzdUQlJHK1VBNHhhVkJtbXFuVytiUFhpU2hGcE1jdlpZTXhzNlNIYWFXSXVFWndwSDVlMUtieDRqZ0hXM29JV0ozTWZNM3d2NXlXSVo4QjVqRjVNcXdqbzVNVDgyODRacjhMbXQ0aHQ0dndJakpiTUdwMW5uWmxiekhxaVJRUzJXUUtVRWxUZUY3b3BqL3d5MGNjY0dQRG1Memt4c2wzN0VpdHZaRlQ3ckdCQ1hTL0d6N0RscW1WVEdhM3hjSmxjdVMvRHhlMlQwQW5rT1hEQkNCZ1hoVXRzUjhRRVFYS05EUkZma2hNSVA3bGg1SW5wZmlDc0wyaWZHMDdLcDZSeFFCcmlRaTh5c1VmZW1RbHc2c01xNXFiN1R5aGZ5T0lnYjZsczRvK2JJbEJzcmV4YXIwUTNuZlJBQlN6SnBtS2UvS1JOOSsySEpFTVFGZ0E0Y3pwdFc3VGFHWS85NDc3RmxYZlBBTFMwQ2EyenBmdUluM3lIUmI3c1ZpeG81aTdjZ2tUTWNoWG1iQWp3OEd3d3NvVFhDdk1odks1QVRsYzQ5WS9ZSDdrcnZOYkdZSHhidUF3bUdqb1dSUFdScE14UFc5b3ZuU0NJaFRZRlBIbzVpdDMyeXI5UW0yTWx3Q1dsbkF6b3oxSHkrSDUvVUJCMTg2QUZFOG9uVk04QlhwZnJSR09zclBOM285TDY0bFhQaXpuOTdpcUVJMVp4a1c0bFlmUzRFZWxidVM2YWx5dVJnWnc0R1A5ZzF4V3ptbTZnMXFoWWZDUUFzeGxXbmR0cFhuYVhKL2E2UGYzUlk0L0x2QWlYemlCcWNmZm5SNkJuSWlYT1BiTFFQMG5kNzhvZDJCSVdOS0t4c1hMblFSeGpXSitKZXFPVTNmMFhwbDVnSW1UeDVmQnZDdGJpOWVRK0J6VU9nU3A2MmtBSGJUdTVTWjJwQ2FRYWpmblB4YTREbzhsNjRNaEZDZ0pTWHNoYnJiaUcxWHRma3A0ZWIwUlJUZjl2MytKaGg1TW10dTlzc1NZYWV0N0NzZ1M1ME02aFFTbkllL3BIU3hrNmp0WVNoNExVdHNsd3ZLYXAzOVdOb3QxQ0lpM0tOOXJSM2lCMytQc3Avc1JReTZ4Q0l4bnpnZktNeEJGazlLRG43d3NIaFZyVzFBaFQwSUYwTVlVMzA0UkZlc3RhRnE4WW9IcG5FNGs4SXRaRk5hSkRsS21ETkJBYnlEdk1RUnRSeGFtbkxScXRoN0lsbEwxQUppVGVFTVNnd2xYcVh0WEtnRFBoV3dlK0tCOEJtcFhOSjVjWHhJb3lKYmp5Nk85YnNJZUp5VUY3Mm5FUWY4NHdMMTA3Qm5KZnJIRDlFdkRsNXVOV0hkdFE4bGR6UzVUUEJQRnl1TlNKbzNva3hDV2lyS2JsWUxaa3E0ZE0zZzFXeTZvOEo1TWpJZEhuUVc1Ly8ycnIvODFzZy9sN1g5aUMxQzNIZWJIYk1wRjRQOEd4VFBwZy82Q3pNdEY5Y2ZkQmNBVDE2enFBT0lydDV2aElQamdYRVU5enBSdGFiRm1RMTFhbEFPQ2tYeXBIcFk2TzVmczJwVm5FUFRwbXdsTVA0bXVGUmU5M3I0bHkrUTJzR0VFK1E3VzhvZEo3bnFUdk05NFVOSXJOd0lsWktrVmozVjRNUFRLblZaS1M3ZHg0L0FkRUx5U2grVmVINFQybk1Rc0VEZTQyQlFOTEpSZytFdUw4WFFPQkJrVXdwaWI4a0ZTaEFZdWRoVUpRcXY2SHNqN2MxZit2Z1p3NDI3Z0hVNkdQczBGRDMvRCtkeFl3amJQZnl1OGZXSnpPYkl6L2ljOEZjQklmN281MzI3TkwrOHU2MjJiYWZFc3VpN3p6c1FIa0FaTk4xemFGMFNFa1V1dTVSdHd1RmYrQkYyYzM0N0FWVUtMaDdmWEtsQnZKaFFqSjl2citXbmJ1MEoyemxXQ05zZDBKcHlyM2RtcWdHMW9oTVdWUWdkUGpiOHRjSnIrMFpwQmliZml3NjE4ZmVSZDdGNXREb1VyczlSZHBQeThYZ1Mya25Hc3lzSGdwRzlPb0VaQ1ROcVNsNHNFM3N2STlncVJnck1jRHpUQUNHdFJubnlQd2F3aXJPQ3ZUNlF1SFI2NVBpakREL0NQc24vSnVwYVAxMFlKM3FwWktoQktuSFo2NVNHd1p2ekdVZDYvYlQya2hTK1V3VlBEVVJhbGlLZXY3Q2xjZHRtNGpXK1duQzU5YTNLOHpqMDVyTE1oTHhoZEd6djdyTUVzbU85QnJUaTJ2UE5LYmhueENhZGlSWWE1eTN3aEwyV2k2cmc5YmtvSkhiT1loMm9naFQvQjBFYmNBWmNEd2lwbG45RTd3cmdHZUlSM0MzUDNhdWZldXJ0b2Fkc01lTXRJSkczK25HT2VGQTNZdEtyUU9YOUJuVVZ6WjlxZjVDUmhlMFRvR0gyT2JBU0Y0bjVldzRwU0NrNkJ5a3o1OWM2LzBhR09nRVVHVWFJaDNJRWhvdEtBRlliQlZ0NDF6ditoUFIrRm40eW9kMmljK2NyaFdsWVM0c1BIMWc3djFHUnpKWVBtN3ZtVTZucEFlb0pneXlzcFBQeWhJZ0Q5K0tpbjBNY0NLTlYyMVFMakRvWHhMcVRvMHdIUjJXUm95QWtXQXd2aHhROTVuYlROVmtKbGlQS2pnemJqV005MEU3V2hVc2RMR21uMlpEZGRlQlYwbEloT09NK3NlaDJ3WnM2bnRkdkFubkdPRFA0SlhhZGx6QTZjbEdDRThIbGxGUGYxaWZUdyt1K09pdW9PaTBkUlFXSDYveUJNa216Y0JNVlhBOFZMaTgvd2NaYnoyd1J6VlRpMExqME41WkFQOFkwRjNSc0R5RzVXa3lBajRSOU1sM1ByUjU5eTBOa1dGdW9lUlpWT3daTmZncEM3enh1bGhVeXNiQ1QvVkFGcW53TWhlL3RVeHloNXdJMzdLSHhab01zRWdDclJqKzVMQ1U3MFVQRHg3L0xXRVF6REZvV0I0U3Q0UjVQa0ozbjl4WWV1VEZTS29nREdVT1hjV0I0Ky9rcm1XbnhWK29RQW03ZUZiemFqS0ptQ0FNeGRJQmxKcE1nR2FGZ1pJQXJZV2MyY2FwbFpxRGJpbVdNTms0eE02Y1plcnR1dUFQU2E1a3k2M01xY2poMjdwUjhhamswZEZZUVkyVkJKS0pkZC9HKzExL1ZxWVpuOTNPczZLU1ZjSkNIMnhEQ21naTFRZXhoakRDUkRkZEJ2OXZIdzZ0alhVNEFmN0k2dG5oQkFJWkJJUFZ0amNVcWd3YnBvZEg2LzJkRTZoT3ZFcjF0QVpNV055TkFIV1hMVy90b3FSblkyUGxzbmwrakptYjJWMTc5b3oxc1owOHdnVHVsdGovMDhaaXRzQ0IrQUxQWFhSZmlyUXlGSjY3c1V2NXlWODRDY2Z2UzZSU2g1eE5henhaaDc1YnB6UnJVVzduRVliZGM4cUxpMnhNWS9ZZGlIMTBLamhtaWFmWXpidDdGdnNISWFIYUc4eU5QS0xDWnVwaXQ4Yi9aZTgxNUNhankvYjVIK29nMWtkQWd6ZEM4ajRWK29LbXFxdk05T2dtMzVWd1g1a0ZDaHNMNnpiTGRZVldnZmsvTHMrTi9VbFBTMUVIdmFqQ0FSTmNEYjJISTc0ejk0V0Q5eGtoREVWSnBBekJ5dW1uV1Zqc0JqWGxSdS8ySFhqM3I5VVpsSkhjd2puUFNSaHptcDZDYVVSeVVtU0gwc0xpR25ocUUrMWdQK3BXaC9aUWJuM2Q0QmNRVFdwcGlYZVdlazVMUHNoVlBuY0hleVdXVnp1MFpUc3phNmJyaDhibDk0K0VCeUUwT2RXZ3FONm8zV3hIUVVOWVhpRVBRVFo4SkQwMzBlUVB0ZHBjYThkMmpRbm15NDEwb2p6ZkR4eGpOOE43blhmd2EvUTNjSWdtSW5yL3d4Sk1nYTNITmM2RkFRcnBTbS9yWm0xSDZKano2N0ZPVDZKdUxheDBxbUI4OW1kR1VYQUpDUEdmTjRmZjFhZGdUcUJqSmJUSVE4ZnZ1cnR3MXBEelp5OG41emxTa21RY2pUSFFEbWVhS1JvMVkwTTRLMklSK2JXbmxibWJCcXR0RS9vU1R2bjZRV1NZd0tHcStXWU5UTGVCaTJudlVFNVR2dGtFbWE3MGhDNjAvczBKS0Z4aU5keGtUeXpOOERrSVc0L3NOTmxycHZJWVBGVWoyRjBrZ0pTSXllWGU4QlZBTVZvcklzMGhubWVOdUJsUXh2Umg3aVczeUJrUUhWb3gyY2NLbzdkVVBFTlNIM3VpOGI5RHBtS2pHci9remc5cHlaK2U4YVVOU1VrMFRnMndUcVkvNVJHcldLNnZYekNEd3hRWDdMaDVlUkRzeGJIVE05MGpoa0FMRGR0SEhTYUVVTTZOM3VvQkttbHVBbGdoQXJvNDlPU1Q4VE9USDFMOHpHQ0lmNmRpeStMamR4Wjc2dW5QaWk3NCt0d0pEVjBocWRpYXZ4V3lNV1FoU1B2RVNFaDdYN0FPaERUSjJPV3RObU9RK0ZuVHE1ZWxUMGIwMmJYMldzMFo2MHJoUUNxUG02Nm9KbGh0SzVrTDBtRmFuc1A3YXhpclJMZkRoZlJCZVVkTWsyM1pXcVZUejAwQTE1RlNyTnVWanNEQjlLR2FPcWZYOHBtUmV1ZW43SDNCQjZSdmUzWk1DLzBHcXJ6MkdtQ240RUErNk9DbThlNVo1OGdrR0E0eWwraVRHOUhvd3R5QVFJcFZhR3BVTTFLOUZDcUhRUlFvUHM4bWFhZWxtZjk0ekt2SW1xcTJYYXZ6UmUwWTBCdk44L0lkSTU2UzBwODZIVitraVY0UmJKOEg3aStRWGpRNGhpT2I5OHM1eTNBelRpeWRzRTdvdkd2b293ZG85bGk3SS9ZcG9Ldi9GTVh4M1o1UkljRFF3TVZJQkhMaEVMUnQ5MjU2TzJ6VEgzLzRjQkgwYVpNNDRUUW1SbDM4eUhST2V4SjBnWmYxdGtxQ1haQ2drZ1dIem9DdThaRi9EMUhzQUx3STlReEZzRVJUenZmTjBSV1dyZ2pja0hzZ2Y1TStnVXFhOGMvOVE5Y3RYenpXOFo3SjExQ0szNkVvSVljZWM4NTdYekd3ZXpGemkyT1R0eFc5SVJoTGZUK3V5Ti85NEZOSXp0S0s3Tmptd2I5QlZzV3dKZ0gwVmNTN1BNaG91bWZFS1FMWjdqYUxCNVc0c2x2OEUyQUlmbk1qcFpBY2xrYlBJNytYb1FiazVBVEQ1UE1vMTNwWkxKaVVGa1NuYldTWTlmcjcrTmNCUnJGbk9Bb3NWbU00Y0pTOGJ3QnhiNE1kSjRuNUZrQXFRSUNGV2NKck9EUStVbzF4V21GQzQ5VG15MkVrSDRJbWJQNnJwKzA4MEFxOEN3ejcxL2F4c3NzdXJYWEl4MlRydUxzVzB0VUwvQTQ3SjBDUVBReVV5Z2YvK2o1Umo0ekZMbDJKS1Z3cHo0RnFSMG9QaDNVcCtsMXFrRnJlYldiaHhHSkg3aHBwYm9kRXZyN2o0Y1lDM1RyMWh0Ny9MZGtCQXJmWjlQUFNERGxCYU9SMXpUeVorejZTWTd2R1h4Q2NQT1o1SE9WL3NnTUo0ZTgyRlZwMm1FS1BOUmpRUEtIeGwvWGhoNmUrdEI2TWtCSm5lRzJFS2laVUVBS2hsU0xabGd0eWlOWWsxYklXZWpVa0ZERmFqR3cvUXR1RVFxTnRHaEx2TDcxUXpyaUJQT1pLd0FTQkR2ZDJkZ0JsZWpFVE96K0FUNHZBbjk5MS9XcHRFNXRacGMxVnZud0dPUHd1ZmJsNUtPMnIrMnFJMlFXbXhLS0lpMEM4TUEvRHNvVGpmUFFGUXpVY3hpWHo4WHBLUFBTcUdvMTZuV3NHQnBGbStKdnRMeTlmRDgya3Q1VlhFMXMzNnlvNGNkT0Y4Rml2WXFUZHJ4cVhtR25LR3U1KzJxNXB1UDAwbnNnZVpaVHJyYyt6Y3hOWThMQ1o5MEdIR1krOVBHNUt3Sk5UUVJNZmRiMEpFSzZWWmF3OXNEcW1paU9uSXNYRndhUDM4QkVSWHJzWGFiZ052NXQ1UXBzeCtRT3RjTVpOUC9pZnJ2cVNnY0ZDQi9ETGJjUS82c1ZibG1RUXRWa25QOUJoNjdjS3FwV1JvVDEvYVc0V2lncUVNKzVQRXNSU1hMY2srYU9lUW1GTk1FQjZzYlNiRS8wSUI2VGl4RlQrLzlSQ3BGQ3VQVExHamFTOVViWkQvQ0xMR1luVTF2YWswNkJHRFQyTzM4ME1JUjE1a3ZLamtyRy8veDFWK0RqK1M1WjF0NjJvd21PcG14MzhlK0xVdlNUMlRwa3ROR242NzFCZXcwSjdlNnJ1VkVSc3pKRzRXRkdQY0ZVRUtnb25MNXpFUTk5bUdTcE9VVGNnU0NnSGwzSlpkUmJDM3JvbjdtUzhlVHlzRmFCTVBPdGJBTHRmOVNqWVd2Z0RJWDFmdEhSOWFTaTBVSXRzNm02N3N6OXlkTk0rSlhlaXZtcm8rS28vZC91cC9rL3ZqMVZpV3BiTVJXMVZ6bWk0SFRhQzYrQlFVaU9QTHNXa2FsaCtFZ3dCQzg1S3VvZ1hZMDNiS0k1TDhWWGFzTVVGVXJHMm1Nbkc4VjI4blhoY3c2S2h3WVdMcGIyVjVUdjc5dlFrdkdCenF0UFZpNlpHOXdjcTlybjdJSEpvNndrdHNOZDMxbzJTU1ZHZ20vdUc4Uk9KZTVzdGxLKy9pQUdGTlgvTU03WFNveVluU2lnbWxYa2p5MHpRa0dpQUxpQkh0UjJoeVRWZUp6bHBscXVWR2I3NHB0bmJlOTArL1NBL0RyNGc5eEhrTE1IejVRc3dYMER0MGdNcUNGYkxycE50WFg0WXhjeHVsalBkdkNmakZNODY4dFlpZnFxTVhBVlpOWlNydEdwZnY1RW9EQzFwbmp4ZUtzT1hBWHlZZ1pIRllmcDNMUXA2ekp2ZVQzdTh4L3RhQmlMMlFBQWtlQWZ0NVNGOWE0SC9YL01vaSswcWMrMVk5eElFMnZEbkdzakRpYkU4dWxMSTRpY3FISU84NWJ5blZKc0ZkcXZsb2IwZm1PZ1E4SElXSzNKNkdPOVFiOHVaK045TGdteVp0ODRvSHJnRUM0TXBneVo3bUpLZGdxSjl3Y2pUMFBVa1hNeDBmNkhMVGRNTEhwd3J6TWQyZXF3RGtzZHhiaUQ4blN3UEpUclJUdEkxZnAvdkROV0pvZEpsbUJCdG1PbUt5QVNudzJDZnZhZ1ZCZks0ZTVuMTdNcGltV3JuaS9LUVpsUTZBOUdsOGZGVUhheWNYTDRkYlpodjdzMzZaQjllLzBRVU1nbElvZnlXZzlIbGFnUXBlT2RZYStlVm9BUHgzS3FmWDhNY0dJM0g3NnJTSzF0Zm5YVkRUcVVIOHkzSk1QRENuVHA4MXdndE1UVGRjakRTOWdVU2RsSnJiSi8rTDRvVW9xM1U0bVAzbFNkYUV2YURTanV1YnV5L1AvcW1EY0dBVTJ4UUFpRGpvYlFzaStZd2RveGEzbFFRMzZwWGs0M21rTVFXTU5XRE03dW5PaTkvZXlwZ1FjR0JvRXVYSjZHeElyU21KOTZkRUUwbnZZblRCb3RIVzVaM2RRTFcrVytTNXd6ZktGQW15WXVjUXd4ZkJIRDBZZFBJYjB1ckdPY2hrdkJ0TGJkbmJsYUp3R3R4L1VGblljQXZjVXFMNEh5WkFFWmVPNWxjOFFFbkEyMVpPQzIwdS9OWEdmK2hEeWt1SlBUMmZhOEd5Z0xadXFGSlNxOWIvQmlmYUpPRWE5a1dBQU1SS2h0QUJheVBrYlpEZ1E5K0JoUzR0SEY5MjJ1RDFxQUtzb3E1eG12WFJZUERMTjNiQ1JjT0FWbi9URWJ2alFsN3M4MFFFS3dUZkZFNzhSL2xhRlB1U3pBY2E2UVpkUE00aEpBWjUrQ1hZUzY2NHB2YVNUTTlNcUJ5cHVUZG9nZ2Y0TEdRZUhSTEtDVFNFL1pqTU16T1BkdENIREZvLzN1d3g1NkZsK3RGeVI5SHZLMnQ5ZTNEbEx6bnpRbmVTc25xNGtFU2s1WXAzNFAzZVhSS2J4TG0yNTlwSlBNUzJOeGlTRWkxUHRJNXZ0TzJUU2Jpb01hS3daU0cySnExY091TWNoajZYeURjMkk5RDZ0MDVHUFNEcmlZNWt2T3dKcXBZVXpzM1l5MTlJZ25mMVBTMnBPME43Ykh0TUdCcG8zdGk0WjV2cWhtdDY2c2tWdXFDSWgwdmdMUytxZGdHRnJ3K2JPVFc2dHNwRElnN0pibkhoZzNja3B6S0lCTGhOcThYQkFIT29uc1VSZHM4a3o4aEZKQ09TOStPRm4xd2k1N3djWGdUL0xSTmRjNVNaSDd6WU9YbFgwOU9udUJDM1g5RWIxMW9LeUVVWEFZTG9rVHBwWE1hcktkcGQ0UkRlMG4ybERmZER1OTBwSmF3YnJhdHo2WTFNdmtMbFJDc0tJc05oYzZyU2lkOVc4L2lPb20yU2twS0h0ODV6Z2FsQ2h3NHhRVU1xbXgxdVNNTDhIQTgxcHdVN3BWL1lyb1J4UTJKKzRvTkxabDloMHJrNFFWZEsxNlkzMnl1UUpIVTdYbkRIRE9LN2RmUGlFOEIwVkR2djh5dE9IUWtKTUFWOXhnTWl3OEk0T1NoNXNlaGdBRDc3M3hESTd0OEJNYW9OUXZoMlBPVkIwbklZVThxWjUvOTZoN25SWnYrQ2RmcUlPM2hwN2JMMnZNbFRCQ0FkUFF5bGx1eVBMRDg4WjNoNjJUbzI1MW9CejZydy91ODJNRWtlb0pkMkJtb3ZlRXJvR2tINm1QcldKKzNua2dvd1d0L0Ird3libmtwYWh2UlJhOXFidlF5RGJrY0FxN0V3ekNzQVd6MnNCUlBRWjVhZ2tyVVZZVnhNOC9RQyswZFY2V2h5ZDN1Q0VaNVBxSXhLd0p5R1l6TXhWSmk5dmJRUmxyMGRJRGhwaEd5dXRZSFNsamVQbzFBa0M0ZE4rdzBRVmJFN1JidjZTRkVOYzVxK1BGV01VdUVCR055OG5FTzdtaUFRdEdFdXljYitnaHZlZUNnbldLR056Nm5ONGtYZERHZWZoOWZFYmJmYjY3cGdlNU53SDAxcmdodnFxSXg5NjU1SnpnTytLNjE0akNORU9sUFN0NkhYMXNvcnJzTFE3MERuV0x0MTZkZzkzZVpHaFpPVVhlaVErYTM4dldnNjFibzVpSTRFaWxtREhuMzZBZ1R4YnVhYWVpNlF4MTBpMW91N0VUL3BhNmRUVTNEUTNZMnRxRGxGOEZ4Nm4xOFZ2STZEQUNHU0RyOThiN2EzVTBNcEJqeXlIcGJZY2hMb2NGUW5ZUlh2K083RXZpUXZTUmJtT2ZXVUJlN2lVdkNsdGxRWmRkWEloZjlmUzNGelg2bXhpRHZqQnUweG0rTVB0NXluM1ZJOTQ2NTEzNzZnMWRPK3lZMTlNdU1Ua0hBMjJOVUd4T1htNk11cmZjZFZ2d1BaZzBFZU9mZUpPTy9FdERlbktOR0NWU09EcVBMT1hsbzFZOW8vSkUvVHpDaFpUcW14THRjREJPeFYrbWt2Z25vYklmeDUvUDRKMG43V01VTTFPVlRqNG5HbTNZVnYzTEJaL0JGVU8rSCs0SjRIK1QwMXZGY0pRYTdqckhWNmUzZm9EUG5LT3MwVGNkU1dja0lESlFXWW8zTlJUY3VEY2lJT3NpUlQydTNhcWR5MU5GMmh1dUM4U1FkYjZ2SGE5MWlpWmJWV1pGQjl0Tkg0ZFFycE5IUDlraUhQOWxZWlBnaW14ZmlUNFJTSFBCTURodUtPZGhvM2FUeUFUSmtsWDUrQ3l6VW5tcjl3aFRhSUtodzA5aitQQVM5akI3d05jcE91eXhud1FKMUxNd05zbmxPZVJ4NElCZXYvZGpqRW5YN1BtWGFxQmY1Q1Y5Ty9jbnIxb2xJNzNITnBrMVY1WDJvTk5hcjhacFdaL1Jqdnd0SWFKZXhFTTlXSUpvYWtmUkJ1SnYvSG01VW5zWnlJTEZhRm5WTGtyNlFWU2JEclpNMnBEdk5pUzM3NmlacWhsRit0NnJHYmdPNW9EbnJlZ0NhdE4yWW9lanhKWlhRTUgraFZ6RldDUFZHZXZyRXI4eHdqTzRJdzJnQ3M5b3lhZDYvMGVnelhLSTBiUXBBYTFqTk5aUXhrNFd1TkV4VzVwRytRclBmNG0rSXZkYTZnbzlLaVQyK1l6WWVpQ2NkaDFSSER2SStjeGZuYVd3Smk3QkwyNHozbFd0YjVmUTZlU0UycXhEOXg0MWVPb2s4T005RGNubGNzTnZWLzl0VkdySVZTV0dwWG83R1VyMHREZTlkbWp2Wm1PZDluWEQwSTdaWTduM1AybVBSanQ0K0Fab3EyMUR2cU92UXdHdHI3eVNXUDJxMHp0N2Y1ZllNU2YrZkI2bi9tOUk4aDExZUgxa01YRlNob1dhaHpoUXJPdm1TQnBjdFpPc2FLQ2VOZU5qN1JwM0lOeldTc0F2TDFVTGFvRnVINU12bEg2aU9RR2ZMNUlwNmhxY3VCY3RUOXc3eVFkYUtMV09IY0JNQjdkUFhlUHgyQTF3QXFsZEgxeUpTYXlocHFjNDJueEgzNWc3TjRSQ0w3cGdDakZOcGdkcVdBVUc4dklOL3h2N1VyckdZY3hscVlIMnRxVDBlclBuYnlUbU1xc0djMG95K2hEdnVqdGVvM2k0aDFIS0Rib2Z0OEhsRTFkOUFtd0p5RjNkTTVrbmxjSkR6aTJqckpDMmY4bnIyb2pLTWVsMEJ0VXhpdTJtWVZWUzNGclNmL29wcDIxa09FOGo5VUFqR2lkblRvRElRaWVMU2E5R3dKRUFRV1VwTWlJNFBac2lESWdMTlAwVXl4dElKTzdpblh6dFdCMFhPZkswWEJWVHZ1NTNrK2JOWU5tVUJ1eFYxdzhWMDM1STIwVWJqTkdzS0NveEtURHNWMjBvNnJJWEN6WGlSZldnTnNOR0IyTktLYXFmWnl6ZS9VWWx1aTBScE55WWZFMVRoYzdNQXd2OWJBRFFnT09Fd09hQmZ0RFUrcWxPbUgvL0NVOFkreFNrZXdyS0EzejVrOGcwYkt6Z2lmMThPZUxsT2NyZEZXK25PUmpZblM5dkFhUG9NTjFJNFMwSXd0dXVZNE1NRFBlQkpJMmhkUUlwdlpIRnZ0Mm8xMFE1ckhnV2FCQnNUaUNET0xZdmUrbmJjQUtrV3ZtWXFKS3BYVFlBL1M3cTVXZzg4UjNPY3B3RmVVYjhuS1FqRUtIQmNNcmx6Ny9ZY2VvQ0xVOGVsNWNKZHNVK3VMSEFXSDluK3dNNU1yV1hFc2h0bEo5U0lKLzgxbHJybHd1Nk56bTBPNkRNWHB5YzFEMkNsSXZjSWlaZURkWmRSQUJtZytmdWp4WlEwM2g5YkhtNFZuTkwxUWp1MCtoOTR2a3o2bGpReTlIZUcwblpRaXVlY3pHUVViUDVib3hjd1k2SHlIT2pKVm42K3dvRFRwWC9HeGJNSzN0dnR0a1Z2U0JlVnBuK09MditiNURuZlZxRndlNmF6SGRtb2lUV00rdnRQNjAyclRPU0NzNXR0elcxNWtkb0lTYU5KTkJXY2hUVmJ4ejI5Y25aelNHcmhzQXkyRTUxUDd6amN4cTF6Q1o2anR2WjMxM3lUTGtkbXE4OFNUYldJNlVucXo2bzJ1b25POGw4TnVnVXZNZG0yK3NheDYxY3drRVFpS0xSSExEZ2g2TVNFWXAwTkllaW4xbDhxQWJqRE0vTUNYeFpRaU9odUZwOHE5bmxEbVFabFZHdHYyaUVyb0ZkTW5tTWpTcXBZendjYXM0azRHZzhCVE1GQ0pMalhqczdITVNMaDllaE53L0ZPQjM2aFNaRmlBSmc1Q3JuZStKWEluUGFKbjRkMjh6S3Q4UEV2ZFVUb3hnSlNtWXlyMkR5RU1jSmhSZ1RzSXMwZ0RRTWhJQk8rMWVlOHh0YTdUOE5ZN05lYzV2WDhNY3FiSEhZaG52Zm50cXEzRG5NRzRWU0tXQlhidXNXVWI3NTFSR0d5cmdrZnlXQ1BPN0dTcDlhZEl4Uy9OUWVUQjdzUmRyYklZZWF2eFliOGtOZWo2VDdqRzU1OUM3NEVUSDFHd0ZHQno4aFVjZzZVQ09VdjVVVVp0cTFMZmFSVW90bEpDbEZXZUFGNW9Pc1dHNkNkMFpmaFVhbWpsS2UzZDgxNklYemZEaVR1QzlDaS9ma3Z1ZTRhaEJUZkdSSndCaWRIWktoU2NiSG4vTWthQkR6em9RczN5YitTOWN4a0swTTV0RHFoamJ3RkhIcUJxRXlUc1FTSEIrdlAxUDhUK3NFenl5UEVNVWwxODVUUXdaUEh6TEo0VWxFZGN0K3BBWS9hc2R0TE8vQ3VsbzJtclQwYkxia2I3YzIvN3A1MEd0eFA1Ri9qTmRhRlp5WkY4djdPbDF1b0JCM1RnNktoM2xKYTVMTlI2MnVxQ1pjK1VTQ1R1ZVRIdy9IdGRLNHFFSEorMEZVNXMzK2dnTUhqaUZRTHptUFlkWnkvYzJ4MTBpZVMwdDl4MWhGK3JGWGR4aDkvU2FEQm5SWVNOZTVnemxHMkVJYjVLN2I3WmZhTTRadEs0Z1JpZWlZckkxbjB1RTZ5SlpRbXhNYmYxWGlQM2Rad2RHT3REaVhKQjVvTUNzZUd6cjVrQzBORGNEQ3ByMjJ4YzJ1NDV2Y1VsZmtJY0RzSTJmVng2SkJHSHFkTlhTNGpTWGl1VWwrVXNjcGhBTUUzYWFNdVplWDJrKzV1eExwclN3cHF4Nk1SeGVhVkhCSU9KUW1ZVElmeSsvS2kvTXFHRG90ZDZXOVdXS2ZmYmZRblpkRnlaVWY5Z2Q5SzhSOGt0NmFMemZ5UHFuaGtnOGt1Q3NJRXJselN0c1lwUldPeXhLMlZpSlVvajJvbTliYWtjbHhwam0xYWcxMXVMZE4vTTcyOURBR3JLK1UwMkN1dkpHampYWUk5WFRnYTFEWjRhUVlPdnBCWnUyTnYwVkZYVFZlRHVMMy80UHc0Unh5Uy9RbHJRMnNrZ2pUUWN5ckVKU3RXc0NkZTdQM01DSnFMcXlrcGZXblZXdmQ4WDFUZGU4VTU4bmludHJOa2lQQTFtVzN2aFZzWTZpNTlPbHVJTzhxWU8vNjQ4cDN6MVJLallOdkk0aEZCM2JoZ0VkMVhxYjBDSXF5a1lmaVhkOEx6RGRMRWZsZVRYd3lwWVhvaUNwWGhmNmJLMHFnZTloM0xZK0VhUGtzcUdtTGNuOHgyOXFRNjA2NXczMWNaWmVZaDRhMVZ4azEzSjdGQ0hjVHFva2dXY0hwTWYyeTh0SmdDWXhadkwxQnh0VmlpMFZucUxhbTZSL2kwRElUUXdudDVhZm43WWEySzFPejZDdlVxalNPMytTVnNmUTZaRWdnYUtyVTVhSTI5MXFPNWpSYWpNOEUzaWFDSWpOOVl0N1NNOTBac3NOVUphWTFMUlZpRWhJaFB4eUtwV3M1UWRTNFZSOStyZjBKOEJJYmpkajRRamlIaGNhNGxIVmJTYjEwUHBMY2J6OWhkcWFPOXRSenNuNWN0eEJMSEV2d0VPYzRFaUdTZnBBdUQyVjlIbi94WGdQOTRPU1JVTzhXSTY4c25paFhVSmxzTWxCWXFQem4rNHppWjBEdXNTTXZ6dERUcWNGaUt4bVhjajhlQ3puQlZ1MkNtdUp4YXNseXVuZ2t2bnFlaDA5MW9Ea05PaThtbDR3V0JFTEVNdDN0cTFUSzF5VDBFSmMra1R1K0FqZk1jWWQyd2Fod1c5Qzl2WEIxRkdpN3d0cHZMVEdlZ2MvVEJ0RHZpK3BDRGpzN0laZ05KNFBCQlgreE1BKzI4cXB2cWtUMXN1R3ltMTFuYWpFQzBha1VKLzdVT0xQRjVaQW43ck1JZkZWMXpWUU1MWk1FOTNLWGJvUDg1QUd6SlR2cE5BYnpDUGY1Z05qQ2VuMS82NU9hWWJ2N3RyNm01Z0o1czgxSEkxN3drT3hQMWxaSjR3bnZaK0ZySmtpUmlncE0rRXVQeDBlNVNGYUkvZ3RxWENiNmdvLytkTHNDZHAxUEpRUU56M1puSDcreWo3TjBJNUJRejdTcnMxdHgzZU54RlRFWDlid3hQQkFtdXhUSHB1c3VNV01YZGx5LzIxOXJDSzg2S3lHbFcvbFp1WTRJVEVySDlTUEFxRVhJclE3SW9pTFZqUHhQVDZkS0hGQ2tZOXFjejI4VnZFampVc1RPa2FjNXVyRzRhUFhjNTcyL3N2eXp1NmE1aVlmb1hmRmRLOUFQTWx2QkNud0J0QTNNc0UzVThxUGt1WmNCbmtuMVQ5Y1l6SlhnZzdraXJDczJmUktKWTZlY2g3cXdyM0RRdTd2b05IVEJZZzkwQStIWjhFV3RydEswT1hNTTZlTVZpMkJ0NXJTN1lvL0NJOVE3Tm1Jd3J0Zi93OXVxbUFvUytnN1NScXM1Y0dhODQyZmxDTnpCd3pvWDFVQk5zZ0R4dEcxTlRxTDA1WGVudkxvMXZUNHlocGVGbnh2T2VlVEt1WVZ4NE9WcEZzTmk3dm5oN3NqanBvaHNVVm1MS3QraWxJeStUaWR2bk01VHJnR0l2V3g1SFJ4VVdEalcrcnJlYW0rRzZvZjhwc3F4dXpMTFFhanZTY2dnaytHY3FzeDltT1ZrV2F6djlCeUFZcXBSMDhPOFg1WGV4TVFudkJNYVJhM20yR1A0d0xlMXBQUFo5NDFxMW1vZC9BQjNoSEJ5SFZJQWF6M2ZVK3FFZzBxWFQ4YlF6ZzJKUHg1ak9oSUJCbkVpYmg1QS9pRzFmcEY1dUxrZWpMemlvUUpnVkRjZk9zaVpkVTJHMTBxMFdzNlJ2RkhiNkVmSFlkeUlWbGVPODdDaWNLNStoUjJiaWh5STF2S3BCVU93d3h6WW5ndTlNdDhlWHR2bzlBY2JFVVBkVlhJcnBvM1E3TGVuelhiakp1U3BMOUlUNE9udnFmS2E4K3dNTUtPY2lqVWp2TmtFSDBxQzVkaFF3aHlVK28zRkoycTM2dTVDSTVnamg0NGpWa2FaU29DRzlNZjQwUE95a2x4ek5ud2VBdDZTU1R1Z0xHWGFvQmNvQUFrWjRxVkgrZ2J0Q24zU2pXaTZKdjl5bkNsajNwSjlxZUtybzR1U2hlRlJHb1pIR0N6MEs2Z0VVWWFmSmRVVmFKSDN0OUx1S1ovcVpuNkZyTThycWJVOHFQMHN0VWowK2xaT3RURkZtQzh3NStNeERwa0FPMm1LWHVuR3l6bFJqWEYrNUw0YVRvZmxmUDVCcXdxalZHSllQTGthU0F0TXFtT1VaZEZHaWJjR1hoRzROK0krR1pVOXhRbUZ4ZXBHTXNwaGNYaDRCRDVJc1phbWdiazZFWk8yMlQ5OXNNNHVDbzQ1bUNKUE9Ca3lsMzM3Qlcxdk9UaVNub0xFalZKM0ttcnEyMzJqcGVHbERxd1F6VW4zajBjN3FET0Z3VXRhVzkxcDJBamxIa214ZmdVNkNFQzlvclA5VHpSZEJDb1RnWE93UkJmRi9HYnRzeDdoOFdnV1prSzlac1NEVlhjSElXNi9RcVU1RTZUVHFTQThOeTV2U0ViQUQyVEYwSjJyOUpQWld3ZlVIZUJwSVZYeW43TmtpdnBwNTlibWFSMXBZMUttUFlFZHVrQzFLQVAvaDhRbnZXNW5WMU0wYUZPdzhGYlhSSlJkOTIzZ1g3RC9HN0lVRVlZWnZFZ1R2emtFMmVTNEhZclVEV1VPTmpGWnJIWjlFQjZiRHBFMVNZZ1FLUFhUb2dqckFHSEY5cXFGOC8wSXpqeWp0Tk1FaitsZTBQYlNLWG1HcTI2Nlp3RGZUOHp0RVlHaEpWaU02U1pNNUV1Y1JPZi9YdzQ2aHFWTnNkZk5qb2t2bVQ0eWhPb25WbW5iUHVYV3V5ZDQ1V2NUQ1MyV0tZaHRpRGVaYkJzR2NTakp1SHpVbGxoRDFIMHpmMnRmK2VyQnA4andseWtHRXFJYVI3VjI4dHUwdFpNS3pGUmlQSEh2Nm5Tekl3Y2hzS3FXZkJFY0FSZnRKU1c5TE93VG40NDZMekVUQXVXNE51RFd1eXVTSTNsRkU2cjdJbksxSHFmckhNVzlUdnhhNlBEK2d2Snl1YndBZjZQOC9ZSndENGNoRERVc2h5Ykw5WlZOZmxzNkcyZFlqbmkrUTJ2UzlHN3Q4R0Y0UWxwZEdmSUNaaVFpVlFJbmFaMHRlaGpENDRlWDlPTjVMeHd5QWo2QnN0dlplZm1PQUpTMzhVSURkbG9tTzNjc1FFRzBKVm1aS3VOVUtMNHIyL3RURlRUTnpZK3FiN1Y2dDBQOCtYN2RJalgza1o3VWFwWUNWUmpKMkY0ayt5UEp0UlRrRFVBUUZXWWpxWDl5amlMWGI4N003b0tvWTZkZTdNdUttM0ZMcFFoS210bnA3WVdQVDZmcFZUZ2gwUWh1WnlyaGxWbUFHYmhzNjBwRnQ4ZW52Qis3dnNwcmE2c2FrUlZEMmFIOGZERlJXU0tLdWVCK3BOS3BnY0tMRXNVUEorVFE3Z0ZENmVyRDNiY0V5dHVXeXR3UW1uZlhvQzRTdG1ZZmppU3E5Vkg1NmtDQ0JlZFQwSS84SHF6dEZ5a2t4T0FnblFoYnhqM0orT2VmN01IcVFsWTVISG9zd1hiTWZIblN1UFJkN3ptVVIxZ2VZdWxhYjJEeXVkNjN4UEM0THFpRkRYc2xNanVCTU9BaW5kVnRaZTZHL3hRRmdYNTAwbjNpdHpPVnEvaC9iZkdhYTYzVzhPMGMzTFJudUczRDBlbTdKOGNsbVd4eHJONWpiNkRzSlYrcElmRHVkM3d3Mk83WGZLSW0zK1ljcFM2dk9qdWgycStkRWkyYWUrcWhPMHNvRyszT2lucnBOR3FMU0pFUXRvZ2cvMWE5R0d6cXpYZloxQ2RVU1JjMzlTYjVsRWNpUUdQam1qVkVZNzZJYjdUMldqeW1kVWZuTVRYVUFBQkJ1Y2ZidEt5RjJieFZQc0tFSTV3VlRWZVl4N0EwUzc0Z2h1UzFKZStNRTBGY25XN0pUSUllOEIwcjlKNHJMWWIvY0dhQi9QQ3Y1NUV1N25wRVFYU0wwQVNQSXYzdExrUzVuaFpwVUNnN1d6Y2NTcnEwdXE4ZTlLaEhmbHo1eDJOTjVteHdQOTZnZTRJRWZSZGwveEhDQVJwU3c2MlIyTEU3eEFadTN2bmlqZDZBYW1HVWtVOXprWFY1TXNWWTM0WVZJaWt2bGNnMEVZU2U4VkJYZ3RmZ0J0VngrQi9SQW1QbHhwYVlEaTlQMFRPbFd1bW5kNDY3VGV4cnREcUdaYWZTWkdOSWNiamFLMXk1QVNQTUQ1SC9CQ05TTWFpYzZURm1NU2t6aE9rK3Nkc1BobXpPVWFrdUVtajc3MlFSTnJQVmhwMEVwVWV0L1JGcTFGQm5TZjNjblBoNTV2cmFBdk1vL3AzR1VKT3l2Yms4SlpGYjB4YWNVM0RKT2luem1QQTVpaXZYWjR5aHdhWE1EdUVnRlNZZDN3OHNHc0xLRmZqaEhNcWhnRFk2ZHBlVWJCbGswbU9sQXJnTVVYd081d0dTTTJlUld6VHJZUzg2ZU5vdVFUenV5ZUdMZlBWcDVKaG1ycEZtWFd1MkFIVFU2ZU0vdm0raUp2d25rNjhiM3AzUTlZN1lNbVlqVlVldG5nV1M4eENRZC9oNmtSbS9rajc0R3NuMXBHZnNENzZZaG1FcEYxVDdKcVFhRXl6a2NzZUVNZVlLYlNoaU4rVEdBckNwdkNsV0NPUTlVQ20xc09lVEhCQldpa240KzZORGxPaEJNOWsxT08zL3BpaXFiM2FrYjZDeW1nQ1RxeGhveXllTmlScE1UbDFacjJYM29QS3JTNXJaNWI1K0FPd3VYN0NtMS83N1JTZjcwSHhUNFdyWkVsa2xVZFcvWFFFdXFtSkxoK3NTZGZVcnEvenRwUlBlOXYwUXZPT3Z1eWpVSkllNTlSb0pBVXkvZFkyVWRhcVptQjhpQ1QrZHBxb3NDeSt6bW9UeDFuem5XVHdGNUNLUC9lQjQwTW4veXRtVnVpZXhpRm5lM3QwUmxMdGx1VHJJNEdwV2FjeXNMUW5OREg0cGNjdXduRlZBejNIdndTQnNpWnNEQkphTnVnMDVYZEwzSnhiUlB0d3NVRk9qaVlHV1hwUWdDZDZKcmlPSlo0a3Y5VUY1WHBtOW5pb2RLbGRWLzE2TjFTMlB1MkJlU3BPbDQ4UEl6QWg2TEJiT1Q5RUs2Yy9xMkFMd1Jua1FVUWpEdEN4S3FyR1Zrc09EbFNjbHF5RHFac3c3Zy9oKzl0NWlBSDRDUDR1aDA4M3pscHRLem4yM1lVVkdoQTZFZXBDRUwxMjBybFk2Z1l0aEYrVUxOUkx1OUhDSldNTWgzQ3FSSDg2ajBuNERwQnhkYnMzMi93cW8zaTkrUjg5LzRyeWN1dW0va2JTK0ZnODhjUzVpdUV4eHU5dE9ZaDIvWktBL3BnTENjYXN5VGc4U3ZrMjh6UGc1WHROdk8rOFMxcEdtMDkzNUFpNnpaUERPazUybi94UEplVnZkWUtSK3ZPMmF5Mk5wcTJJWWVPdUd5dzhnYUNDOTZLSVRRdXJ3WFFGWUhDU0N5QkJZdElpQ0drREp0VWNSZUJLYzdtbVFmbjZGdml3UDVnbTA4UTdFaCtEYjF1U0RMakJvaGFycE4zWXdDTnZEYWpwRG5uZVR3b0M0VW80cEhNeE91elZCNEpHdVh0TnpkL2d3T0pDNFpQbEdEQVlCdUt0WVE0Mm4yMEtaYjdoVkdFL2FScWJITUFhYTJMeGJKNnc4UmRFbWdQWnJubVFDOW81K3BlUnE0NUtIQjc5Sk81ckk0R2R5SUFqZ1FWdFlNMTR5VStUT0xkalBMV2gzY21ZaERKalRYUTROKzBuVk9UR2dUVzZNU0Qrd0dXdmFtbk9vbXl4TWx3eDVDZUNsWXJia3h5bGxidWlHV1hRamVIOWJVbzlsdVNvZG9XbGFPUFFFQnZsSHhxR1kxc3U5L1FtQlcvVzFSNkxGNDRUT09ZOVRXSGdQTjJ0VDd2cHE5TnAyejRCODhYeUtzL3hKbzREQ0EzTDVhWURvWDBzMFdzOUx5MUhRZUQzajYxenZ2ZGl3QXd4RDBKVXV5WnNBN3ZYd1hOZHVwV1E3ODJFMCtQSTRUdkdjbUdmK2s4Ym1GRGtLVkhicHROVnhlQmhvalJaRlN0WGU5a0xsMjA3SXY1SXZ2RktCSWF5RG1URTRiRnQyaFo3b3A1TC9pMm5oNnNicEg0RXBvbTZLN0VvQ1NuOUtvS2dxeSt4L2M5NVFvamNxRTFLaVRnbGZHSE1aZXFqTjVLR0gzdVB3OVI0ZmFBUlExYTIrakE3TmpCU1MrY0p2OUVNUjBtWTFCTHcxTnhkc2lCRXJsQVRpY2IrRkpDU2Q1VDVodHNhTVdweHJhTW1jVGJaK05OMnZFelM5emNzUTN6RlA4K0JKdjlnQXdMaGVmeHdZa2tma2hTbi9aUmxPbzg3T3dqUzRVNERmMTliNTNmYVZuaHZIV00zV3AvTVNWaXpTUjdBNTZMMk5wSHRyUCsrczZueXEwYWxpekFCSERVSlJFYkhyNHJZejNweU1LbjlQMDhHbEZHQnRWdkNIYVhIRnBnVnZieHJUYXFsYnc1VGdOcnV6Z2k2bVkwc28yT0RJVUh1RnlaMlMxSWlxMHFlYWxyTVJDa1dVZzdBQUpMSVdvZHVBQUlVYklDTEcrY096UFU3ZnJjaTZCbnV5UmVaTGpVV1FEL3Nvd1pDbWNNRGJ2NTg2b0x3NVRKYUpXV3V0cjVPOE9GZ2lzV2ZWY0ViQS9mcTR1UkhIaHdyeHdGVmlCNHBnV3JuQ3BDbGt4bnliak1sck4xMTMzT3hiM0dvOU1jMEErS3llWnRUa2pWbEdJYUNVNkhoQkl5NWNzaU13RzNtUjUyNzVheWhFVWxXYjIzTmRkZWVDN3BoU1gyS3cvOFhMVStTblFwdGtwWGJsVEtMUXloSzRIZnFzREpvTXZmRUp6NkJHaWRkK1REeXp1dUoyZFZJaitmTnozOVNyVzQxMGZ5bzdjSmcyN2JtSlZHZUljY1VuMzc5dStIcDFaVEtMR1JEWitZTVhrV3RqR25SYldWQk5yelFZeXBGeDU0emtHYlh2U0w2dzhUcE1pak1nNkJlS3oxeWFiU2dmM09pSVhxYzZGTWdoZ3hUZkxxQjhoVTE5Wi9WN2c1SFpFSzAydElZUUxlS2pqY0RwaGJQekcyUFdieCs4eWIxZG40NGViQjBpNlFaZ3FyVWNlUy9mQ2FMamMzNXlVOHlJd2Y3VEc5b0lDVWxtK0F2eEdYNDdDNVo0UXVKaW9XT0JYRTFRbHVFRGRxNjhPd0p1aWhpSmVNeG5vY3lvYTQvZnVuVnRQUFZReURkcStpcVBNZWU1NEtHRmR2d1Q2dnJIcG5iWFVXZEgxcXQ4Q09JNlZoSUc2M1RPeURLRE5pUm9nWEg0ejlJbnB4ZmNJUzI1WnpuajVMeHM1MER5SExhSDlCbjgxT2MvQnVtSmtUdW5TbEV0QWFoNGlWK3FxNkE2NGdHQ0EwY2x6SzlPemZtSkRxMlZUei9rdXNLbU1tL0NjK1ZZd3VyZmMvdTlqa2xic3hnZHVvM3M2UFMvZ1JKSWJMdThVaE1TL2N4R1JlaTJobUFsVlEwdjZLTHBaUFF6RzFobFI2RHNrSXFkb0ZvdnVCQWRTeVlEc1dhOGxHbGZ4Y0RGVDhpRU5nTk5wejRJVWVCbVl5WUxlOHd5TTM0cDBOOGw4WExaT2tKRStVUnlvdTdRNnJTSzhDNEprelVSUTFSaUE3RG5ybXFnb1crakVYQ3pWeEkreUdIRFV4dGZneDF5bVN4QmZsNWJzNzRCK1VhL2hwRSs5REJHYWs2eXdVcXZKaXlGRndkR3VRMDJKKy90d1NHZ3NlT1RjejZuVTBSUkZubXZKSVRrQTNXbjFJR3ZiWWVTOWh4VWNwSFFnMitKRUVDVEtqWFcyR285elZvZXcxdWluazNUTlZQQ2Nxd3dTZjJ3ZFcwT2F3YndYZWNKSjdIUXZVRWlyTHA0VmZYTG02cGdOY1JzQ0xCTkV3bUlJazVMZW1pU2dKQTZwWVdtcDVidjVKY0ZDMHFlWGU3YUNhT2xXQkEvanFpRXh2UDJPTEhKS3dDM3Y4S3lKeFc2ZE1RM0xCSzgya3NFNDFMb1lpVVl5dGtzQWxmK2kwZmp3QUdCb3VEYjJlM3padW03TjhYVTZ4V1ZrQ01HR2YvV1cwaWJTdURPZTNldlMvNGp0V0JZTHhiMG9LRG5JOHhBQXp2d2FRd0dldWhLL2p4U2pTT1lJUHRwc3RhK1ZFWXFNdFp1WVErQXkzOXU1TU9CR0lJVWlKNVczMjlKWGV0WWQvM1pEelQyMzlJd3FJaVRjQS9mQTgwYmd4OWVWT080dndYMjVnb2d3blQvOCsrTHNuSFMzOHVnenlFSThnMTMvdnk2TXZCQ3U3MEVCR1RRTHp4dUtBLzMrcURqandEYi8vS1BGRko0YSs0di9MbmFPTmxHK2wrcmEvU0lDbkJsVlA1ZE5lUE1NT0d1c1NtNjBkMDFkZ2YyU0NLczk5NUQxeE5qdmlqK2llZ3N4b2VwOTJBTVZ2SUNYeTR0MzJLeDFGeURuMDZMTmI1MzBEU3oxVmcydGNHZ28xMlUyclNwVlNQMmVuSnQwb1ZJeTE4TFVnaFFVdEd4SzhHTUJpeWJaTDIrQUZhNGN5R015RjdXRy93M2w3Q0lOUEdoaEtvQ3JDT21yRlFGeE5UTEFSYlRkaitya0RPYXdHQmhzaUhwNnVPTzIrMmZKUFY1QVgyaEpnSTNpYXMwbjA0Y05XbWN4SmZ2UVphblkwYVZjeFArYk03UHJSYWVGcTZvcE1VVHgzMUcyUytTMm8vR09yNHp0M1BYTUYrY3pRVXF3MWx5VGFMVEd1S3RCZVpEODZwTmthYnBxTDJCSjhWRXNLWGZSY09CS0FkVVoxTjN0L0pJUDRjZGFKT010WEsrdzRRaFVBTkRKaUR2dWRaRmtxZklKOUlGeGxGWjllTURYbkE3ZUxRUWM1SkJBN2RsakVpT1FZTEtFckRlTUNWeGxLY2owYlcvMlFzcVh6aDBKbi9mRVg0SFRVNEVLUTZYZ01EMnRTMWJKUU9WazNsN2NCVVkxM2JuRjJUWDNueUxLV29oMFQ1eHJlQU5NVjNudUdJa2oxV1BFcGFtRlRqZkVuQlNFc2paTTE1VCtXREVDMStaRm0ySXYrY2VCTWdQckkydDR4NGJYRGpFTnF4OG44VTFRaDdXcm1RNDQ0b2w4REx0dUxvck1nZTYramVPU25WRlNyTnlmY2MzRUlJWEV2bnlTdjZvUXJDZ1p0KzV1SFpHQjJCY0pKTFd1MXQzTkZPbFc2aFI4SEZrSUdleXV2K3BSYnRoNlV0c2tPK1lLWjQ3akhGdGRyU21pUUg2NHRXNnJjbGNISkFJN2d3ZVdEbHp1eUUrOGRmdE1CN3J2QlZqRDZ3K1plZkR5dU9GQW1xak5ZSUpxMVdMUGd1NzBOYUFBT09ySzRsWWdHUjlvcDY4My9VcTJTY0pPQS9pVXBlK2NrekZsN3VkcmNjektEdTh5bStOV05TUG41TGVBNWl1U3JjUjdPZHQvT0FmNkxWSGxVNllzQmJPWGdtWnVCNnVkVU5vakNseEZKbWJLVzhEUTZFQy9lM0krOVdIY2xOWUJmZjlRQXRqNktqOFBIYVJ3TlIrR0NUc1hIbm5zODF5L1hQbDQ4cUF3SEt4cGgzbnRmRStmQTE4TnA0TnhCSXNVYklEdEdBYTQ2Rmw4alI0WlQ0bVRxTmdPV05wTWs1Q0hMTTFWUGVNVmJMU09OUHhFYnF1TEVsbFdlMVhMNzk2REVTN01XLzBVNVVWM0h3ZDZkV3hqN3VSM1JDdU5KWU1lTDVFSDUzdVJJMkc0SGlHcEcvUWxYQjJSS1RRaEZzSnZ0VThzeXRTdSszelMySkhLb05vN1A1c1JxdnN6Y2cxVHVsQmsrV1JnaGZ6T3I3QU5JeVREKzU1dmpaRGFaOG0ybktjWmtId1dKUUhpMkovU2ExNStvTnpyLytmakwxY0JsLzVNSENwRXUyS3BWUE94VHN6Q0k5VlVRczhLQm84cW9qVnpjb3F0ZTRKOUUwMFJWSkZNaWVRcGpLc1V3SVlORnpqRFpNMlJJVE04YU95RXA4ekQ1TjM2WGtreWh3dnRXa1BtLzZSVlhrcllPUXRvcVljRTBoRTlBL05Ya2J0MXZtaXNWekorZDYwTzF1MmIrSDR4YkVtL000cSsvMEozNjIyMlJONkl4dVBZdk9ZVFN0V0c2eElRUmZBYUhGemxBQmxQZHhLRGhOeS82SkM0TVd0WlVOeFN2TVhOTnJlR0orYVNsR3REVjQ1dG5UYUFiQVNTanBOVkFyazhxK2c3b3pNRWFjeDNXemtJbG82aU02dTJXTEIyMDNMb2NsU1Z2MUdBdEtna0Q5OExmK1ZLcWdlUW1SUktzQ1ZMVExJc2pDdG9zWURqSk5tb3JCbmJZcWtsZkRqSXhCdHZwOXpaRVlsZUJMaUZKclFPUklNMER6L0dVbWd5S0t4aFVqSERlc09TeERvRlNwR0xPcDRlYWxQNXBRZjZDaDdKVktBak9KME91aEN3M0FsY1ZnL2svTWJGKzhmc1p0ckxkd3JBRHl1RWNCWTRISkl5WitxdEhBRVdjNURrWlMxekJQbG4zTlZZK0VSTnlXTHZ1SU5ZS251b0xHMFpwcFkrR0krSlVUZUxMWG5ieGJKWWJzanhEc2xjb0QxSS96NklGb2pnRFI4R1UvRFFRZGxYYS9BL1FxRU1yY0JKQ3hYdE5mdzhYcStCaFdDK0dldFZJYm9ldGdnVFJVbkRuUGZUN3BmT2ZIeVFGcFBQdzVvdFo5dDJLcHVtaS8zVTZ2V2VlSndwTWRRRzRwY1pVTU9FWnZtTG9VUVNYWmJPT1FlKzc0cnk2UEhUOGpaTjlveERhRTFLcGx6R2VLcVFaenZCd2RDcVY1aTFVOUhicTBteUwzV3JpcDVHTlVRblVob3VYeER6dWQwMmtCeXBMT1pVNjdhSExQakowMUhzelN0bFhQZU1KY1E1U3dyNTRuUm1GSkRKN3Z1elZoUnMyVE1GNTVhVlVka0RTRFpHeDdRREQyTFNDSnhCbnMyeDd5a0pzaWx2YTJPYUZkdHVtaGR4L0p3RDNtV0dna0d0NDBWMWpoTHdMUUFvLzRyZmcyK2xSeUsvb2ZZVFp4cDdmbUVmQ3FjS04vempuSHRTS1ZYZ2JjYzA3ODlIVUxqVVFOcE1NUmV2TkphNy9kaWloa1d2eUVISnpHUTdnZE1vZ1ZYZEZSak5waHMwTkpQV0NRUGVWc3B6TFVyODVES3lPK2pwMUdCNzlMcGxiM084dGc4TzBORnBqb3pTZHo0T0xSTTBvdUU5b1ZTdmVSWG5Wb1RSUitwRWY3U3RXbUhmZ3Y3aE9NS0Y3L2JENlN5RldRbG9zclFEUTVjbXFlMVc2N1QrQU51d2h4RVN5UU5DQU1mVXBMZXNyVmJVd2IzRVJBRXlsejd3VzVtSEpQZ2x0WE95NXBVWVJtajVYdzQwUGxrNVhlRURITndhVE5WUWhOMS9TTHp3Q1ZBaEhkYnhJTEVTM01FRTByMHIyVFhHeGFyRkl0OU5nQjZSNUZxTnVyUUY0NE5BdDNrV2NuSFlGSittWXowK2dNNUQ3VXA1b2JXbHd6b1YvZ0NCekRZdWJrSktBVCtaRit1VlpXRlJ5ZDFSaGdTYlRrakRIak96VC9qZ2E2dGJBNncvSDY0aWhhclRJRG1KT2pnVlhCQjB2RDIrOFhRclBTU09ZN21CZnFxMHZtRWp2bFpVWTBrSWFhYVU2WENhTzRxaXhLV2J5cUZRTEgrTkVnYmdPdUw5eVVOSHc0MFlEZWVNS0xVTFhnbnl4TEFNaitzQ1d4bEpKYnB1ZzN1MHVBYVFKYjd2TVJGK0lJcTFkVkdadUZFWHRmL25Mdm1ja1E5L29XNXFNb1BUbkx3cDFWVElrU1VvdVNERzIyMjJmenNhOFZGVFdqS1QzUXh0R0pxNWNSSnA1MUdMbEJnSHk5OGxybjExb1FraTJQdS9wZmhUSkpDeTBGR1JqWC9EYmRLbEpmd25XNXplYUFjcmNuMm9ZYVJ6dkE0MytiNDArWUVTSk93YUZDNjFsb2VlWHBKU0tzcFZQRkNDOXlGUTJxMTNZUFhWTVVYcGtqZEdaK0JaQUFRSklJa0hpclN2WWZBUGI0SmllQjRtQ2FPTjE4a1pXZkM2dzQ1OGNUMkxSUFJjaVRubDRsVkMwWmRKb21lZFNpbFNoRTcwaVhCZ3cvd0tJdFZ2MERoZkltQWMraEFhWk1qSnlsb0Jqay9hNURjZVVwZG8vZmw4S0FybjFIaE8va2pQdHVDRWk1RmVTekdYQU01VmU1SWxRTjBoY1NGUHAxYnROS1dnZ0NzU2I1emlGTk85SG1aQlQ3WTZzVDF0TWNvTzk2cHVuaFNFdk9EWVdCSXVKR2JhYW9lOWV0cjYrcXoyZjVIOHJBcml5VFlMalE5UXcrSFRsQVNJY2pYS3JmTGZ2TTgxeWN5cENkaGRpbUVSYWJTaU40V01UNWh0bDFucjNjYUxQNkZ5RUsyaVhQL0ZSUm1PRVd6TU9LdUxPVUFOcGY3RVdYZnNpMUR1SC9lTS84Wm4wUEhkOW9qS2UwSTVXUlRSQVlzL3pYbjd3TUIrckhaYm4vb0xBSVVGc0hwdFdsY1lITUxqVjhUd0N1R0RBSWdxTm0vVDZyWlMvejlHNHM5NzZxYmJ5V1VQQWlDNDFuMDZVZlA3bHBiNWtHRUxqVEY4ZmhlYjkxdHZpSU5jOWR6MXVpMzJsN05ZR3JBOURPSXBSTzJKTWNjRkUrR3ZUb2RveE51NFVlcDBHb1M4RCtqQVhzODRua0VMdzFyczlIRjhETVNoSHV6RWdIcUZPQXk4WFF4d1pTVm1uN0RSWXhsT1B6RWxxMGUxTlNnQ1B4ZVh6VFdCc1poaTRlcUM1YWxDcUl3UE5FUW1TbldqQit6UVhXbEJUeWRLcVJTSkMvREdRdi82ZGFaTkphdi9iSUFVWTRIVUVYVnNBTFpFYW9abnQydHRKY2ZHUUEzcHpya1JXUkNyKzdueG5udzd4dTZaNU1ibWI0dHdsUzZ2TUVnSjdEVWRXUSt0RU0wb2RpeXdmWU9kZ1VZdHhZWkdmR3BMb05mYTJDNHBObEErTkVrWlFlbFFYem42emltcGFmSFJhUi85cVozTm5uaWlUbWhYYlFaNDI0bWRhcG12RGhncWpGRUs3TmtJNHNhYWtRdFRwbXhLNHJ3YVZ1cmpTemJCbTQ4V2ZSSE5yM3poVTlGdUYwRitNRmFDMmMxbDVuQVB6bm1tNWJyUGMvZXJmVXFidVg4RDUxbXY4MDdjbVJHSWNybWdNcDNhbXgwQ2YyRXVOSFplcUpyOXg4QWxLZ2JQRUhycnBaNzlCSUxiaUIrRGR5QTJzVmpnODVIQXgvRGxnVG4yQk9QWGVxcFpGdmgvaDcxOXY1M2tyaHlZcXYzNkpiMENKK21nQU4rbEduWTFDT3dyYzVqenRWZEM2SG5UUEVaNDBJc1ExS0JybUJZcUMrMi9pMmsvS1hITjlXZGppL2ZGaGRTcjdCaGdZSktZeUlFSjVXNU5lUVZSSUg3YStRUXBRamh6N0FVdzlYdEhzSGVNb1NISDh4YTkvZHNET2VFWjBvMWpxSWx6Y1B0eExzODFEOXNkZWJLSzdaNFJ0RzhUZjRkNDZocjJtazZOc1dySC9sOFAwQUlZOW0yQnhOek5BdTJXTHI5WjNsUERaWFdiRkhEZFgyY0JpbExaSHV1TEFBWDZlc2dCYUFNK1JQYytOODBnVVlCY2JOVmJ0bUxmYVUxdnBSTXV0UVZ6VzM0Zmk5ekJIaGIyV0hqNGpESUVqRE1rbFp5RUFkRnVudVNaSXgzYTdpYUs5NllDeEZIV1RMNDQ3MVl6T25HdEREZ2wvbzZnd0lXVG9ZamVJZ0V2Kzk3SEJBWHZ0YURKOUFmZHJWNUhna043OUR4blhCRjg2MmVXUlVOcTRGeDYwNU95N1B5WEViSFZXcmNJT29RTFVQQTZqdml3cExJVFNzZVMyakJTc0w1RitKMjllMVNhbTh2V3hyN0xoV2NNVndsaTZGUGd5dUdlQThFTy9FV0pnTzZiWUw4MGVwTm5HZ2V0Y3RrZmdjaU9kekg1ODBKQld1Uk1COXhJMSttNEp6SksrM0poV1MwUmRXUFRVc1BxcUxjVmd0b2Z2RUwwaHBlcXpJYVN4aUZ0NG45RFREUnVuNUdhbWdOanVHeGlXUTV3REdkR0pyRGtpdkQzWWFJN3R3anh4ZWNEdjMrYWx6K0NoRE1iYXluS1p3TUwzNitTekFzMGkrTnJ6ZXJqamZyV3JXdE00YU1RMnM5Z2M3enJmL3EyU0RReGxlVjNkYUt0WHI1TkJsZUhGRmdtbHV0SmsyeG52RUIvelA4OHdDZWpxZnVDUDRHbjFhTDNzMHhPWDhPcmx2UmZSYjNkdlp6M1JMYk9LNDRqb3lHNnZDRTdWTXFZM05vOGw0ZHlTNVhUUWp4eWQycjdYZ2hwS0pab2lObDk0M2NZYUUzc2dGdUtKZUhHa3h5WHA4b0QyZHlYZ3dFRjlDcFhhYis1MlZoRVZ1VCtGL2pkdHBmS09ZWGMyNDg3RWVKcGl6SFV6SEY4ZG1zM0h1NVNxeFVtU1I1Z0w5MEZZM2RiNVovS3FQbHozNSs4NWRRZW9Kc2hPaXVDaVBkVWoxVDRDRDA0eXd0TUpGNktkZU51TEZacWplbE1IcU5XeUxlU3pEaXl6eFRBS3JtdzhjM21VZkFnam9zK0dxOVlqZGxibzhJTnppZWRtVmNtbDBEWmN5aTNBK0kwKzNnQmhvRXNrNm1RQ0VVRU81eDBBeENQY2xMdHk5S09ZcHA2Uko4TUpaUUEyUTAzT0RCNlc2NFMrUHFZR3g4UEs5dHZwNi9xbjBJQ3ZtNFBleHcxeGI2OXhQRlN3NUFzZmVaTFJscE92My9wVWFPVk04RVVaZlpRampLZlNoblZXYWhrSXAyTk5yL3RwamFwNmExUnFUK3pqZ0VybENSUUs5NmkrMmRpck1mb2o2ZXV2NnFiWDNscmFHRDIzVmRJVG1oRHIyTFFlNFBKUDRRYUx0eC96cVN5SExzZklsaDYxYWdwLytQRnpjR3k1bHVhekpGWXN1d3lQYm00NzVzQTZqNHV3SUdIU2N4VS9KUzdqRmg4SkRCd2VkTkYyQVFzY3dKaGN5Qkc5eXgvT1QyTzQ1dXExUVdvTXcwMitTd2VsL1lFallLeVZnSHVvOFVlakQzdHFGdUU4cDZJcVBrR3VwL2FzQmJmOGRqVnBXMDNGTTlSeDV0dFJSSlpIa2E5VktJUHRIc3pQUzJnYmwvWkEvc01PakNLVkZmTlkwTVNmV0paYzVEV2ZYVzNUblNjVGxEQWI1MG9sOVZ1c0p2V2xFakdaK0ZzUXNvSW5mSjkvdTNNZzRlMEhiWm0xZCtaTXE5N1o5WUt4OWNFZWo2NWdBeTZIR3l3RjZBbHQyTWdXM21pWUs0OEZzNjFrYzNvWnhoYlJsdlRKZDZTTVpoVkFjdXVCWXEwYXdCWHpzdnBZZ05QcDN2ckkxcTFrdlhvcGxMVnNJeDE4VmxDTDUxeFNsWEdabncyUThvRldNZndPTEtSM1lZWGcwbURBaDBKcmFXb2NsRFkxa2gyUWZ5eXpLN1dFZnd5Z1RKUUtlZDN4RFg1dytYNmNRZ3ZVU3JXYUUyQ1NWN3dyTmJTbm90UVRqQlFXRFZtL2krWitqZlhEWGRHQ1dyVHdhMUhwY3MxN2VpdDJrc3NuM0hCVHRWcTJORGxLTkVjWE1kZENTaWd5SFpHanlDaEswcDl6TnRKdWluMW1LUUJrR0tnbnFpWndkU05GekJQbTJ0eHVVSXg4YklMeUREK0dmV2hJTFpRV0dNRmtBT1V3VVoxL3kveTlmTFZzc2pQT2U2SDVZMjFMZlZudjdVa1RYTGR2T0FXVWxqK3FWa3FSTktENzZXS0dCcWlQQUNvb3BVOE5qWWY5elFZSU1OeDFjRTNNOTZZbWtMeUMvemhGc0RUc1J6ZXJIWWV3eFVtTlZ2TnFNR3hiaURDOXZ2SnZZaGlVWmN0Sy92d2FsZTNiK3Q4Vm9oSitsQTBOYjJWb0o4Z3pHWkF0L1hTQjZFY1FnT3lSRjhsVDhKTjlkNEgxWXlMOU5hNFdxYUhZL0FqREVVUTM2bnZZKytqSzlFK29sYVk5L3VmRGZOaGpFaFBJN29oYTNoV3VpWUcvVVpwZThpRVNvcUs1ZmsxWjZHd3FtT29NV0pKNkpzd25PUTNBcUdPdkhqVk1FWHpwOTJDYmE2ZHpJRTQvdTI1Z0MyQ2ZsbDhJSXFONmpTeEx2SnpLZG1aT08xWnNhSVRkRkFlakNPVnZBbWh0OXNpZHZ3M0lmU1V3S3hIU0lQaWd6Y2laSSt2eUpwdDUydjFLNmtjSThVZWxGNUhlZFR2eENCSmxxdSt3aDZNY2xUYm1lY0ZxSHFCMmxzM1pZUXNYUm01aFhtamRnNnhJV2J1QUhzNDMrNXZ6V0RwTzFnb1NNemFPMFZCRjNsM1JZNzlXMWdMUWk5VUpENjIremcwSVAyYVdtVzZ5Z202Z1Jrd0lZT1A3WUxWZEVqdDkyT3JxNmdzNGNDa0NNQTdnQ3krSHNHK0cwSXR4NFFNaHpoZDhRb09JKzZ1eDZGL2JRTGxPRnhFLzRtZ1hDQUdCNEQxVTJpYWMwQ2dFV3p1ZG5IRnFBU045Tm40Q2t3QVZTYThsZmk1aFUxM2VJNXNuVjhKdGhqa3dGRWpaZWdqM1NhWlBISzVjUjlnN3dLOXpiby9jWUJpQitxeVJHUi9zd1cvNDI2QTZ0QmpLcHk4eTBjVEJFNzZKU0lMcnpKTU9NYlNZVTdVRVhOUDdtNWlCQ0xhcEVrcmJGVjR2VG95ZU5WMHBqQlp5bHM0eW8yeU1meUsrTW52K0YvQ016UUh4dGpBLzlucHZ4U1JJclhpd1FuNVJtWUVrUFEvYzBJbk1oTDNFMmxOUU5RT3FCZ2RMUDg4eDBBQ253WktZUkxwK2hINmppSWtwRHp5aE8yVzl5cDBZVFNsUUpGMlUxaFp3TU0rWXUwQlo4SkVCYkdvOXVBaWFDcG5mbzVDTG42U2RxcHUwUmozL0h3UUIwV3ZuSEVZMGI4OWQ4NkhXR3BjcEVJbVBSZytSTnhxZUxjaGt1a1Y3Z3pUUnM3clNGQmdzWWx0dGhxUUI1WGJjVGpGL00zQTBxVjQrNWFDN2h6VkRxQ25RRlVOYjVremhiM1N2a2Nmbjl0dmdVNVNhTnJvY0tIZ1lKd3BrbHY4WDZybC9CYUNJK05TNWdoVmxCZnhlR0lUOFE0TENwUUh4SXdWWnVPR3daSEpFZkRNclRUeS9qNWtkZzVrdDBveHQ3NitDTFNjTHAveFYxNVRKdG9QazNTa3ZEUWJGdkNPSWNQdVhZcEY5OGJSWlcrdGhEV0hCNzVwamhEckMyaXI1VkhjYTduNUt4RFo5dWhQanlVd1VOV0JqV2Y3a3dEVVhOa0FlMmRHTmFZQlpHTUFxREgyclBYZENZTGt3WVpBdTgrZFdkL2E4N2E3YkNocENqTEhDTGQ5RnhPSlJhS3daZ0hzN2N2cTF2encyMG8zTnN3aHdoNXZheXc2Z1FxTmVhK3dPdDRCVTkwNmpCRU9tQlVObEhqOXZ6MGdFM3ByTmFwdzZPSnp4TkM0WWtzcVhFQnI2c0lra1UvTEprbTVJelZHV1Y1YTNvVnF4b2FoWGNzNlExU2lTSHRKb1NweW9Gd0JLcnJBVlNCd05RS29oYXc3NWxIclcrc2tZZG5ENUVyM1RENHIyQXFjQTBUeEw3MFJ0Z00xTi9BWEM4T2o1SlhxdDBFWUZBTkJDTVVXVnNTdDVXV05sR0VWQmNrM1Y3ZHFWTVMrUlYxZlNDejk3dk4vYzNDNDAxSVJnOU9FZEZiK1N0dGVIWWRYSXgzK0xvQnJBeGpxM2k0Y2hNczNBTEE4SVIzQkY3dEt5QkZTZEIvdjA3S3RUWXRIckFUSzdSZVk2anFmVlMxelVFTTJvazBuVTl2SjJraXZZVS9Wd082TEJiQVRsOC9mQlk1by83Zk9RZVNHTG4vRXB2N1ByTzhhRUJOZ1NreVBJZ1NIMHJpSEFURnFSOGVvUCtyaU5ZZjJSaEF4cFFjQ3dYRWxoZ2o4Y2JXOHBxS2czUVhmN3gzS2xEeHNkVU9MeEdWakZ1eEFyTld1eHdXRHd6L0h4N3FkVmdVQ1FPR1NEK3Vzb1U2OUZBSlc0b3ZXNHQrOUU4ZDJnZVFYc3BTOGkydHdRdzZuWkZYSDJKSXNYM0lHeTlSbGlhQ1FLT0dsM2JySzBwTE1Jd2hTdk1nUDRHWXp1bUJCYnBnUjFOSi90R1NQNGlUclNaeStOeElHd1B4K1ozREJjNHlrYVo2dExTUjlDNHQ4Rk1qNWNLVnRiUE9yaXhaeGRnVEMyU1dudUlMb1IvSHRDcjBtNEI2VHFhMFoxNlVBd21lUGdZdzBLQjArRHZvSWZYQW5oWDJpdkc5a1ZBUHdhdUNjTG1QU0QvRWpSeVRRSmtGZEY5SFhTUWtPeUk4ckNXbi9rZy9LY0RCUnV3SjJlb0xUbUlzL29JakwrR2FJRGF3cDlRSXpianQ1cnpBWU0vbk01UTlwR2dlMjEwQmFZRldEZjFpTmNMV3A3c1l0a0hvYWQ0OUtCeVNCS3pPQ2pVZmQzWEFBUnZRdzJQN0grSzZhSFFXbWR5VzZBdEFCdEdsK0FzdzZFVHVqdkp1ZnNoMy8rblVlWjcvZ3VIcFhTcTZmV2UvUnJ2THVuTXJuU2x0SE1hYytKZHBiUkx0N2UxdDYvL2Q2QmNIN0l5RjRBNnhZcHpDcEg3MGY2UURIdDBRMDk4M0Z0K1l4Qy9henVCNkJZdWZId01vYm1MWGdUV3V3R2t5RnIxa2pkT1dtV0o4dk5iSmpkbTNzZUVJRUo0U1A4MDQ1L3Q3Y1JvWVdkcnRFY3NWKzh5TjZvZURIeTVMakxBMDVSbzVBZWVLNEdrcnp1aXBFbUFHSlFvY2p0bG5JRGoxWTVuSjVMUSt0djByNGdXZ1dsMXhGaFNndkIvbGdPb3RMdGFvV3FxajVIdGNXcFJ3U1JVN21vamF1cVMyWExFZVQwZXZBQno3bFIzbFF3R2t2Y1lUWGNIT3poWHhNelhKM1U4M3ZvTy8xaHgrNldJS2pMT20vTkZNMTdEYjNPQUZGQlJXYmU1TjNHZzF1dDU0dWt6d2ZXZXBNYnlWbVQ4WklPSkVTRG9maDExSGx5SVdqQ2kvU2x3VVZDZWZwUmpGdllFSUEvb2pYNHdiZnY5NVdtU1hRZWNnYzdmZjlhY2JRcndnWUZFMUlCWFZ1aHRIUitNYTZSc2JwZ3U0eWwvcU9uQjRpbnkwbHE3NWtqTlBmWVlyVjI1Vit3TXpEMTkzUUYyeHMwemdXelNERjNubDdZVEZFVUVPZ2xsNlpMYkkrT0EvbGIreTJEQnhIUmlVYzhpVUgyVXM2MEZyVm9vMVA5YnFxcjU1ZWxWYmhDd1NSSmNwR0VGNjJrd0lvZnFGc0lXZWNQdlRGWGxMWGx3am5kRDBZenRjZWVYOFJIMFVkTjRGTkVia0xwK0pMWkJ0OERCQXhnZVh0bGFEZGs2VjlzWWNGem9iN0RNbU01L2EzVWRVY1RCaHlSVk5Fc0RSTGlqSzZYKzE0Y0htK1lrVm1NY1NoZ1c5M3RvRUZidUNhWWhwSS9McjdQSHhmUFptYzBKOTFrbit5VHJxeTl6c3lGZEt6NXlha0kvdEVMZG1QODhhTXEvRXBDZjJXUHpONG1oZjZ4V01Md3JPWTBWRHk4SXdTQjNaY2ErZHpaNUNQWkkzUlMvcmVvM3NvNDcxK0NqdDB4SnYyQVdhNUVnaUhibnZWTTJnWHVUOS9DbmM5M0ovRlVFdXdtdXJqdkNMS0g3QTFmT3NmSExiS2U0K2ZyMENDOVM4MW9kMG9Lb3p3YWhzZTlFeThNK3JjM3E3Z3hvcUlGN01xdkFBRGdpMFBBSkxpaTNXZzlqZWg1Z2FBVWNuMmtRMEVKeGwyaGxUcDdLdlBYR3FnUW1YNDRxdGI0RzdwRmZFY2t0KytqcjI2Y2JvMHgyR2JobDJ2V1N6c1QzL3hIdTUyUjlnYjlKZThvY01rdyswNzgxUC9BeGpnd2FCeGZ6NlphWVJKRWVKWVM3OFRLaVhsMjJlaGROakhSRXZKL3RHQytxcWZOdXdYbE11eHIrOVMxaDJaNkhuQVhWTndNcS82MVFlVmhmazNHOGJiUHZpK3k3ajJPMzZjemNxdXM3SUlFVWE1dUJZWlBTenExdzRMN1NDUzRlUngrOW95R2dXaHQwN0hxLzRJb1dwclFPbkJ0NUZZSC9Yd2lEUERUczV4bUxCQ1JnZHg1djNKZTRtUUd2NEZvQ1lQOGU2eS9oTU5wNjY4SnNlNEI4Sk1FRFR3NVVpVFBjcGs4SHFtL0JUMVpiSVdLcGk3MUtaZDZYUWtrVTRpUnZVdHJsLys0azlDdFNwTktkZ1lxTUNqbnc3aVFQdVI2N2syZkdscjFqNDlvazJEZUpOTWpSMXFNWU1pL3lFWEgyM3dnaHRkczJ0RmVQZWNKeDZsalRnbXpzL1BhYXJsY3FlYjk4NjJLUTd3dWFwRVNXUnZaakdKbno1a29GZTVxeGlvQW9NMFhBaVRPaG03MUZFRzZEUGE0b0VYWVdkRFhmRENOcnZSb0JEK3g3dTF5RThIdHR1allhdVgzK2JUUTRuNjcyQUM4UnNVV2l2K3kyWkpjMGI3aGh4YTkzZU0zRlllVVZHMlJtL3AwazJmVWQwRW9XWmtFMDJyQ0lIc1ZHcXFhdzlOcTJVdElTSUdJYXJrc01NUW5HU1hPWnlLRVJUS2xWN3lSaFJWa09vRmZFTWc2TDdhYThTSzdheUdqRWtZNkdCWHp3ZkxHMkhKK2xoeE5rK0dDajdZcGh1QWZVWEk4Q2ZqaTd3bmU4d3p3ckVWRldrNVVxTkpyMXg1Q3RWVFQxWUpsK1VtT0VHTmRZcFp4Vm9hb21NZW9Ja29lczF5V0k2V1N3QTIzVFZIU1hGbU9YZVZJWklEZXJHNDF6R1BkSnh6WFVIaDRYZXhZR0dvT0plRHNtRW13cHd4UVpOK0RkQ09RT09CTlRhTEFNWjBFRmh5cVFBbGNsd0FGRXQ1OWVzdmt1aFNOL0NMVGg3aTQraFM4anVHN0U5SW52dC83cmVQTEFQOFNldFQzYmdyUVk2dTR0aERWVHVmUDZ4N1BaeGRhSTcxTG9kYkJJSUUxZFg1bWkwODVyeHdRamw4bmtpSys5c0tMc2tvRmFNUytFb0I4REZZSzRtUWxaMUtKS3BwZ21aT0tjUnNha2hFTFZYdG13dWI5RUJBUkd0cXZWQlV4Z1dGQWthcVo4NkxBUjhoY0YxellwM3lHZmNPN0swcHNWYVRXQkdXZy9QMGltdUErWEpWbFN4SFRoUHUyVVBRdWFCUVRkQUttL0svRW5YU0N2b0RPNUxQUERuaUx4MVpiVEZsRW9LTGZ2Y0NGa3dnNjI0S0JWcGNVOXRjTkFJN0tzUnBiU0Z6SXJNRUZwMVpGT3R2Ym5oSnVoVHZtdVRoUEYzc1Qwb1MrNnlQdFV5QnhhOEgrWjBlWUFwajJseDlkMVV1ODdTanVVaDl2UlNBa291QTNWeFREcHVQUjVsNEtwZGFWYSs1dU5qczRLSU1sV3VOYjltczBucmJ3YWxvaS8yc2Q3M3dsYUZIMVU1S09WM0x1ZndJTmpJdjB3aTJLbnMyMzhtN0ZQcmVYZDZTQy9INm5FeFpQY3ZYaGIvUitCYUtnbjFBU0Z3bGorQ2gzS3RGQlVFY1c1dUwvN24wc3NTUXdZVmdRclMzZTEzMkVuaG9EMjkyL2RkOGNzOUJLa3lhRCtLK1czY1YwMEMvdVl6TjY2RkU5ZS9IYUU2SDdnOXZ5UWw3VEdFbklaeVVFQTFmSUtRazZFQlF3SGE5VFRuV1pKZGVFRm4rTnkvLzI2eVE1SGFET1UrOG8zT1luVWNXTTMrNXREQ21wclRRLzQ5Y3U0ZVZiVCt3MlVTcG1adXdodmRWaFMyOWs1ZHQ3dkFScnJtOXRsd1BTRFNVUzhuZTZzOGp1dGh3Wi9UdmVNS3FJN3MrOUU0cFl1dmR2N2pWUlQ2NUtSZFBNT01tNC9Cb2xJTlE1b3p6Tno5TGE4TFVyMXpwUDVsYS9jbzlmREhKd09QbXpwRmFkMEdER0R1SzlKTGVsSGUvcVoyM2wvak0wT0xVSjgzSXJUbzZtb1h0a2lGY0RwMU1jcjJaTjRJNzRCT24rblJ0REgrVTZITDAvb1JSSDF0bDl1WFFTazNRcmsrcnhQcG5iTjJBZjhzOURFZGZ4Z2llWnpYemxETDlyT1RKNUFBVldkdDQ5cmZyVHBGSlBuODRybGM4amR6ZTRuSGpuQnB4YU9zR1RQOHFBMlBqRSt1SElpTzBBRyt3dDF3T2QraEQ4WkV6bXVWV1ZkdVNROWNhYTVzckV3am54c0FLQjZGMGpDdHFQWWRWWlhWUVBKZUJyN3RxWjRBK2NGQm52RDY3Yy9jRTludXNHVVpvMTZnVkt3eE5ha1lVeGZTZVAxTDZzaWtYTW10cXBZZVZDWWRSY3B6S2cranpNTHFSb3JaSitQVnY0Sm12MXhJb1h0U3RacUFxaHh5SVo0Y2ZKaEJZMkprUjlxSW1jMFh2cW8yUWJ0VzJvcFBiWElLaDF0V2h1R20wMHFYMTNVSGYyM2lXaW9ocjF3R2VjR0FGUGdYWWpUd0NQVTNGcUNtdjZBcmtIYTlQcHpkTkdqakF1U3V1TkxadkZDM1VhSFZDNVVVM0QwejkrK3hNT0E3RkR0d3BkR1NxOWRDaEJFcFNvODNzUm15d1RWMGtYcktOY3dKZlo4MkxiMkJuQmFzQWl3cjhvNVRqTnFjZEdhK3d5ZndaeUFxRnVwVmRVUFR4U2pyRUJnQ1pRUjNDYkRiem0ydTlWeTJUakRHZUNtV3BPUElGdGdYYWZKMmpBREw2Wm00eDdrNXdLRHFzcmVQcnVpdUFoVzlVbElmbUxhcXlwdWE1QlBSWVp5eVYzK2VRUkVZMWQ0K3k5eGtvL2ttaWxXai93UjluS1g0ZUFodmZuckVqdW9IeTRRdytFTTdHUXNCTjVzaytpT3dnYStadFBOaVJNUU1BczF0amQ5bEgzSGJSVXY3d1RNT3B3cVJ2eHdrLzVzWU41NlRhZHVhc0k5MUVrdWFHdEY4UTZvVWJ6cm9oeDJUamhCcUVubjlhalVYc0JjZSt4ZFBTZSs4cENTN3dwaXFTUjRVaTR5U0kwNlMvam5nSzJ3UHNpQk42R0xnNWlBN1NQY00wandMYlJ3Y0tGZ20wYkM5ZG9OSkx4NXN6RlZ3SnFXM0JvYW9FQjUzd2RsOFRnOXdqNGk5bm1GSjYwaHVlMUkxSC85dHpGYjhTcnFRaTRBOVpJRzRJaXg5UVMyNmFPVmxsUm4zTytzckVmRVYrU0ovdVllZ0RqZmF0OUNFQWltZ2NxNG5NQmw3N1IrUGRxdkNwOFptdjFkL3JidkNhZVVmazFnb2F4WUFxQWxHTGUxcHIzNlhZaXVBazE3NHk4OGZBM1NWT3dnWWx4WERaUXMwREltaUxpL3cwS2FEck1pSGlaSjhIaXpHb01PcGJxTHlUUzRSQXdLVlVhVGNMOGw0OVRMZEN6eHE2MCtCeDhJc0V1OEVLNkpsbHIwdHVnbWhlZE9NNnR1bmY3OXhPM2w0M0tlQmpJaFdrZ1FqV2t6VmFyakJjZnd3bjVRUWJVd21wY29SbnUyVzE1eGtPRXdDZDlZTVJmdnZTZmdZRTZvZGc2NGZxZU9oOEZFMUNyeTgrV0Y5TDZXUGtwSHRqTGIwSy90Yi82YVd4WDJyYVBDNWVnWEx2dnpCTGtRNmhVNGE1V2FtbjRTR2tENHd5QndoV0tyZ3lJd252UFI0bHVHVXpyY25qM2tSYkNXZlRhOERMRU1qdkkrZm5qbDNmaUdrTEVaU2RFUlluV0h4aXQvcE05WnJNa0lvYTc4RTdsVGlJdmJ3ZHlzWmdPbzRTK2VLaHdIWnJodEpLTU9xb0FRdmhKNnRDRHI5MnpmVkZiaDFjRC85Z2IyZ3cxWi83ajRXSkFrVGQ1a292VTJWMlBYbTdML3YzbWxLMVhMMTAycDZkOUdFTUF0Ny9XNGdBQURyZEtZK3BadWE5U0h2YVBRNUlOTkpqUFVrQ3ovZTkxWkdKa2E2QUxteHRzOVl1YjlBSHJER01zODJkcHo5TFJSQVlRdWpybEtuWStJbHpQakNLRG5sN0I4SnJEWE5tMnFaWmNuRWF2UHZraGpQNjR4Ui9UczVxeUxEc1NUWG9qTG1tc0dXTGl3VHVFVlRjblRVQllDZnpGbjgzdHdFazZFVGdsWVJOQ1pMME5MSGRFZDkrcnRyMkt0L3kxQVcrc2lzVVAyTlhLaDFTU1RXTXFSU3VzREZxSXR2SE9HdzlNaVFwYkp1alB0bjVQOHh1cVRtNzc2VmpuN2t6aWh2SDNTdXNQNmZRT0RjdUZ6NEczTnhDUjhHK09xZFlZN1VSN0pqbEFQcTlRRjlXdW5tY0ZUajVhb0pWWU9ZcWF2UGxseFE1ZjNDNDhUUXVGYlE4dS9ZTTZSZkllNld6UEs0dWFpemU2ZXdlVGcvRHVnZzgwc1JwdFRSUDhWWkxmOEk0PVwifSI=\n",
    "description": "",
    "tags": null,
    "title": "Solutions",
    "uri": "/2-configuration-management/12-solutions/index.html"
  },
  {
    "content": " YouTube Video Resources Slides VMware/Tools from Ubuntu Ubuntu Features from Ubuntu Ubuntu Desktop Guide from Ubuntu Ubuntu Server Guide from Ubuntu Switching to Ubuntu from Ubuntu Know Thy Ubuntu from Ubuntu Ubuntu 22.04 LTS (Jammy Jellyfish) Download from CS Ubuntu Mirror Linux Distribution Timeline from Wikipedia Desmond Tutu on Ubuntu from YouTube Video Script For the second part of this module, we’ll be looking at one other operating system, Ubuntu.\nUbuntu is built using the Linux kernel, which is a completely free and open source operating system kernel developed by Linus Torvalds in 1991. It was his attempt to make a free operating system kernel which would be completely compatible with the Unix operating system via the POSIX standard. There were some previous attempts to do so, most notably the GNU project from the 1980s.\nWhile Linux is usually thought of as an operating system, in actuality it is best thought of as an operating system kernel. Linux is typically bundled with other software and tools, such as the GNU project, and those bundles are referred to as Linux distributions. There are hundreds of Linux distributions available, each one customized to fit a particular need or design.\nHere is a quick family tree for the Linux kernel. You can see that it all started with the Unix operating system developed by Bell Labs in the 1970s. It was forked into the BSD family, which still exists today. The underlying kernel for the Mac OS X operating system, Darwin, is still based on the original BSD family of Unix. In the 1980s, Richard Stallman created the GNU project, with an ultimate goal of creating a free version of Unix that was completely open source. That project stalled, but around the same time Linux was introduced. For most users, the Linux kernel coupled with the GNU tools is typically what they think of when they think of Linux as an operating system.\nLinux itself is a very versatile kernel, and is used everywhere from the Android mobile operating system all the way to the world’s largest super computers, and nearly everything in between. Basically, if it needs an operating system kernel, there is a good chance that someone has tried to run Linux on it at some point.\nFor this class, we’ll be looking at the Ubuntu software distribution. Ubuntu is a term from the Nguni Bantu language, meaning humanity, and comes from a philosophy of the same name popularized by Nelson Mandela and Desmond Tutu, among others. Ubuntu itself is based on an older Linux distribution called Debian, which is very popular among many server administrators even today. Since its inception, Ubuntu strives for a new release every 6 month, with every 4th release (every 2 years) being listed as a Long Term Support or LTS release, which is supported with software and security updates for 5 years from release. Depending on the metric used, Ubuntu Linux is generally seen as the most common and most popular Linux distribution in the world for desktops and servers.\nAs a quick aside, here is the current Linux distribution family tree. The full version of it is linked in the resources section below the video. Ubuntu is shown here as a major branch of the Debian family.\nThe first major release of Ubuntu was known as Ubuntu 4.10 - Warty Warthog. Each Ubuntu release is given a version number based on the month and year of its release, in this case October of 2004, as well as a code name typically consisting of an alliterative adjective and animal combination. The first LTS release was in 2006 as Dapper Drake. Since that time, there have been 7 other LTS releases:\n8.04 - Hardy Heron 10.04 - Lucid Lynx 12.04 - Precise Pangolin 14.04 - Trusty Tahr 16.04 - Xenial Xerus 18.04 - Bionic Beaver 20.04 - Focal Fossa The most recent version, Focal Fossa, is the current LTS version and the one we’ll be using in this class.\nNote Of course, many new Ubuntu versions may have been released since this video was recorded. However, we’ll stick with the 20.04 LTS version, Focal Fossa, since it will be maintained for many years. –Russ\nNow, let’s go through the process of installing Ubuntu in a virtual machine. At this point, I’m assuming you have already installed VMware Workstation or another virtualization software on your computer. If not, I recommend doing so before continuing.\nFirst, you’ll need to download the Ubuntu 20.04 installation file. You can find that file on the Ubuntu download page, and also on the K-State CS mirror of the Ubuntu downloads page. For most uses, I recommend using the CS mirror, as it is generally much faster for students on campus. The link is available in the Resources section below the video. On that page, you’ll need to download the “64-bit PC (AMD64) desktop image” from the link near the top. It should download a file to your computer in the .ISO format.\nNext, let’s open VMware Workstation and create a new virtual machine. For this course, I recommend choosing “I will install the operating system later” to bypass the Easy Install feature of VMware. This will allow you to directly observe the installation process as it would be performed on a real computer.\nNext, we will select the type and version of the guest operating system. In this case, it will be “Ubuntu 64-bit.” We can then give it a helpful name, and choose where it is stored on the computer. If you’d like to store it on a secondary hard disk, you can do so here. I recommend storing your virtual machines on the largest, fastest storage device you have available, preferably an SSD with at least 60 GB of free space.\nOn the following pages, you can choose the desired disk size and format. You should consult the Lab 1 assignment materials to make sure you choose the correct options here.\nFinally, you can click Finish to create the virtual machine. We’ll have to customize the hardware anyway, so we can come back to that once it is finished.\nOnce your virtual machine is created, you can click on the edit virtual machine settings button to customize the hardware. Again, make sure the hardware specification matches what is defined in the assignment for Lab 1. To install the operating system, we’ll have to tell the virtual machine where to find the .ISO file downloaded earlier. Click the CD/DVD option, then select the file and make sure it is enabled.\nWhen you are ready to begin the installation process, click the button to power on the virtual machine.\nOnce it boots, you’ll be given the option to install Ubuntu. Follow the prompts to install Ubuntu using the Lab 1 assignment as needed for configuration information. In general, you can accept the defaults in the installer unless otherwise noted. For the timezone, you can choose Chicago as the nearest city.\nOnce the installation is complete, you’ll be prompted to reboot your computer. In rare instances, the virtual machine may not reboot correctly. If that happens, you can use the VM menu in VMware to restart the virtual machine. It should not cause any issues as long as the installation process completed.\nAt this point, you are ready to begin Lab 1, Task 4 - Install Ubuntu 20.04. Feel free to get started!\nOnce that is complete, you’ll be ready to move on to configuring Ubuntu. The next several videos will discuss that process.\nHowever, before going too far, I recommend installing either “open-vm-tools-desktop” or VMware Tools in the Ubuntu virtual machine. This will allow you to have better control over the virtual machine. You can find instructions for doing that in the resources section below the video.\nAlso, I recommend taking a minute to familiarize yourself with the filesystem structure in Ubuntu. You’ll learn more about how to navigate this structure in the following videos. This diagram shows the common root-level folders you’ll find and the typical usage of each.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Overview \u0026 Installation",
    "uri": "/1-secure-workstations/12-ubuntu-overview-installation/index.html"
  },
  {
    "content": " YouTube Video Resources phpBB Installation from phpBB How To Set Up A Remote Database to Optimize Site Performance with MySQL on Ubuntu 18.04 from DigitalOcean (works for 20.04) Video Transcript In this video, I’ll discuss the steps you’ll need to follow in order to install a new web application that uses PHP and MySQL on the environment we created in the last video. As with the previous videos discussing web applications on a Windows server, you’ll have to adapt these steps to match the application you are installing and the particular configuration of your environment.\nFirst, we’ll need to create a new database and user for this application on our database server. So, first you’ll need to log on to the MySQL server using one of the methods we discussed in the previous video to get to a mysql\u003e prompt. I recommend using either the root or admin user for this step, as they both should have the permissions needed to add another user and database.\nOnce you are at the mysql prompt, you can create a database and user for your application:\nCREATE DATABASE \u003cdatabase\u003e CREATE USER '\u003cuser\u003e'@'\u003cip_address\u003e' IDENTIFIED BY '\u003cpassword\u003e'; GRANT ALL PRIVILEGES ON \u003cdatabase\u003e.* TO '\u003cuser\u003e'@'\u003cip_address\u003e'; FLUSH PRIVILEGES; This command is a bit complex, so let’s walk through it. In the first line, you’ll create a database, replacing \u003cdatabase\u003e with the name you’d like to use for that database. In general, it is a good idea to use the name of your application for the database name. On the second line, you are creating a user to access that database, so replace \u003cuser\u003e with the username you’d like. Again, I recommend using the application name here as well. The \u003cip_address\u003e entry should be the IP address that the user will be accessing this database from. For the lab assignment, you’ll be using the private network IP address of FRONTEND here. Since I’m working on a single server, I’ll just use localhost here. Of course, replace \u003cpassword\u003e with the password you’d like to use. The following line grants that user all privileges on that database, so replace the entries on that line to match what you used above. Finally, the last line will flush the permissions cache, so the new ones will take effect. Once you are done, you can type exit to exit the MySQL console.\nIf you’d like to test this connection, on your FRONTEND server, you can use the mysql command in a similar way:\nmysql -u \u003cuser\u003e -p -h \u003cip_address\u003e \u003cdatabase\u003e In this command, the \u003cip_address\u003e is the private network IP address of your BACKEND server. If it works correctly, it should allow you to log in to the MySQL console using that command. If this doesn’t work, you should diagnose the problems with your MySQL configuration before continuing.\nNow that we have our database configured, it’s time to install our application. For this example, I’m going to use phpBB, a bulletin board software built in PHP. It is a very simple example of a PHP web application. First, I’ll need to download the software onto my cloud server. The simplest way to do this is to navigate to the Downloads page on the phpBB website and copy the download URL. Then, in my SSH session connected to my server, I can type the following commands:\ncd ~ wget \u003curl\u003e where \u003curl\u003e is the download URL I copied from the phpBB site. I made sure I was at my user’s home folder first, so I knew where the file would be downloaded to. Next, I’ll need to install the unzip program, and use it to extract the downloaded file:\nsudo apt update sudo apt install unzip unzip \u003cfile\u003e where \u003cfile\u003e is the name of the file that was just downloaded. On my system, it extracted all of its files to the ~\\phpBB3 directory.\nAt this point, we should begin following the instructions for installing and configuring phpBB from their website. I’ll generally follow their recommended steps, so feel free to refer to their documentation as well if you are following along.\nNow, we need to copy all of those files to the appropriate virtual host root directory. Since I’m reusing an existing virtual host, I’ll need to make sure that it is empty before I do so. Remember that the lab assignment directs you to create a new virtual host for this application, so you won’t have to worry about that. For my example, I’m going to use the foo virtual host:\nsudo rm -rv /var/www/foo/html/ sudo cp -r ~/phpBB3/* /var/www/foo/html/ Next, while the instructions don’t have you do this, I’m going to change the ownership of all of these files to the Apache user, which is www-data. This will make assigning permissions in the next step a bit simpler:\nsudo chown -R www-data:www-data /var/www/foo/html Next, the instructions direct you to access your site’s URL via a web browser. So, I’ll need to navigate to http://foo.russfeld.me to continue this process.\nOn the first page of the installation process, it will check to see if the server meets the necessary requirements for this software. Unfortunately, that isn’t the case yet. If you receive any error messages about directories that aren’t writable, you may need to adjust your permissions for those directories. By setting the owner and group to www-data earlier, I have bypassed most of those errors.\nHowever, it complains that I don’t have XML/DOM support, and I don’t have a PHP database module installed. So, I’ll need to install those items. A quick Google search should help you locate them if you aren’t sure what packages you need to install. In my case, I’ll do the following:\nsudo apt update sudo apt install php-xml php-mysql sudo systemctl restart apache2 Warning Update 2019-07-30: You may need to install the version of php-mysql that matches your PHP version. So, for PHP 7.2, you can install php7.2-mysql. -Russ\nOnce that is done, I should be able to retest the requirements and get to the next step. Here, it will ask for the information to create an Administrator account. I’ll enter some default information here.\nNext, you’ll need to configure the database for this system. I’ll enter the information for the database and user I created earlier. I’ll also enter the IP address 127.0.0.1 for my server hostname. For the lab assignment, you’ll need to use the private network IP address of BACKEND here. On the next screens, I’ll configure some additional options unique to phpBB, selecting the appropriate options for my environment and how I’d like to use the application. Finally, it will go through the installation procedure. If everything works properly, it should be able to connect to your database and install the application.\nNow, I can go to the control panel for phpBB and create a new forum, just to make sure that it is working properly.\nFinally, you may have to do additional configuration to set up your Virtual Host to use a security certificate and redirect to HTTPS using Certbot, if you haven’t already.\nThere you go! You’ve now successfully installed and configured a web application using PHP on Apache, and connected it to a MySQL database running on a different system. In addition, everything is properly secured using TLS, firewalls, and other state-of-the-art security settings. Feels pretty good, doesn’t it?\nThat should give you everything you need to complete this lab assignment. This lab is quite a bit more open-ended than previous assignments, since you’ll have the ability to work with a web application of your choice. If you have any questions or run into any issues getting this lab assignment completed, please post in the course discussion forums as always to get help. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Web Application",
    "uri": "/6-application-servers/12-ubuntu-web-application/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Hypertext Transfer Protocol (HTTP) on Wikipedia Video Transcript Next, let’s review a couple of application layer protocols. One of the most common of those is the Hypertext Transfer Protocol, or HTTP.\nHTTP was developed by Tim Berners-Lee while he worked at CERN in the late 1980s as part of his project to build the World Wide Web. HTTP itself is an application-layer protocol that is built on top of the TCP transport-layer protocol. As with many early application-layer protocols, it is actually a text-based protocol, making it very easy to read and work with manually if desired. HTTP is the protocol used to access webpages on the World Wide Web. In fact, if you look at the address bar of most web browsers, you’ll still see http:// in front of web addresses, indicating that it is using the HTTP protocol to access that site.\nSince HTTP is a text-based protocol, it defines a set of commands and responses to make it easy for the system to understand each packet. The two most common HTTP commands are GET and POST. GET is used to request a webpage from a server, and POST is used to submit information back to the server, usually as part of a form on the website. Other commands are defined, but they are generally not used very often.\nWhen the server responds to a command from a client, it sends along a numerical status code for the response. Some of the common status codes are listed here. For example, 200 means that the request was accepted properly, whereas 404 indicates that the requested resource was not fond on the server. You’ve probably seen some of these error codes in your web browser from time to time.\nAs I mentioned earlier, HTTP is a text-based protocol. That means, if you are able to type quickly enough, you can use a text-based program such as Telnet to actually send queries directly to a web server. This image shows an HTTP request and response sent using Telnet to the main Wikipedia server.\nLet’s see if we can recreate this connection on our own system. Once again, I’m going to use my Ubuntu server configured as directed for Lab 3. I’m also going to start Wireshark so we can capture these packets. I’ll add a filter for http to make sure we only see the HTTP packets.\nTo initiate an HTTP connection, we’ll use the telnet command to connect to a server on port 80:\ntelnet cs.ksu.edu 80 That should connect you to the server. Once connected, you can request the homepage using this HTTP command:\nGET /index.html HTTP/1.0 Host: cs.ksu.edu Once you leave a blank line, the server should respond. In this case, you should get the following headers in your response:\nHTTP/1.1 301 Moved Permanently Date: Wed, 05 Sep 2018 19:47:12 GMT Server: Apache/2.4.25 (Debian) Location: http://www.cs.ksu.edu/index.html Content-Length: 316 Connection: close Content-Type: text/html; charset=iso-8859-1 Looking at the response, you can see that the status code is 301 Moved Permanently, letting me know that the page is available at a different location. A little bit later, it gives the new location as http://www.cs.ksu.edu/index.html. So, I’ll have to query that server instead.\ntelnet www.cs.ksu.edu 80 GET /index.html HTTP/1.0 Host: www.cs.ksu.edu Once I do that, I should now get a proper web page as the response:\nHTTP/1.1 200 OK Date: Wed, 05 Sep 2018 19:49:59 GMT Server: Apache/2.4.25 (Debian) Accept-Ranges: bytes Vary: Accept-Encoding Connection: close Content-Type: text/html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en-US\"\u003e\u003chead\u003e ... Going back to Wireshark, we should clearly see those HTTP packets. You can see each request packet, followed by the response including the status code. If you examine the contents of the packets, you’ll see that it exactly matches what we were able to receive using telnet. It’s really that simple.\nNow, let’s do one more quick example, just to see one of the weaknesses of the HTTP protocol. Some early websites required authentication using the HTTP protocol. However, unless you use a security layer such as TLS, those packets would not be encrypted whatsoever, and anyone able to capture the packet could decode the username and password. Let’s see what HTTP authentication would look like using telnet.\nFirst, we’ll have to create a base64 encoding of our username and password. These are the same credentials used in the lab assignment:\necho -n cis527:cis527_apache | base64 - The output should be Y2lzNTI3OmNpczUyN19hcGFjaGU=, which is what we’ll use for the next command.\nNow, let’s use telnet to connect to our secured page:\ntelnet people.cs.ksu.edu 80 GET /~russfeld/test/ HTTP/1.0 Host: people.cs.ksu.edu Authorization: Basic Y2lzNTI3OmNpczUyN19hcGFjaGU= If it works correctly, you should receive a response like the following:\nHTTP/1.1 200 OK Date: Wed, 05 Sep 2018 20:06:10 GMT Server: Apache/2.4.10 (Debian) Last-Modified: Wed, 17 Feb 2016 16:48:27 GMT ETag: \"5e-52bfa04e3af09\" Accept-Ranges: bytes Content-Length: 94 Vary: Accept-Encoding Connection: close Content-Type: text/html \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eCongrats!\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e Congrats! You did it! \u003c/body\u003e \u003c/html\u003e So, even though it looks like the username and password are encrypted, they are easily deciphered using any base64 decoding program. As part of the lab assignment, you’ll capture an authentication packet just like this one in Wireshark.\nThis is just a quick introduction to HTTP. There are many interesting features in the protocol, and they are easy to explore by simply capturing packets with Wireshark while you use a web browser to surf the World Wide Web. I encourage you to do just that to get a bit more experience with HTTP.\n",
    "description": "",
    "tags": null,
    "title": "HTTP",
    "uri": "/3-core-networking-services/13-http/index.html"
  },
  {
    "content": " Windows Backup \u0026 Restore (15) Screenshot of login Screenshot of fail Screenshot of restore Screenshot of login Ubuntu Backup (15) Website Data Apache Config MySQL Backup README Ubuntu Monitoring (10) Munin or Ganglia Installed Backend added as client DevOps (10) Webhook config GitLab Webhook Push to Git, see change on server ",
    "description": "",
    "tags": null,
    "title": "Lab 7 Grading Checklist",
    "uri": "/z-instructor-resources/13-lab-7-grading-checklist/index.html"
  },
  {
    "content": " YouTube Video Resources Using the Terminal from Ubuntu Advanced Command Line How To from Ubuntu Beginners/Bash Scripting from Ubuntu Environment Variables from Ubuntu Cron How To from Ubuntu An Introduction to the Linux Terminal from DigitalOcean The Ultimate A to Z List of Linux Commands from Fossbytes The Linux Command Line - A Complete Introduction by William E. Schotts Jr. on Amazon LinuxCommand.org Companion Website Linux Tutorial from Ryans Tutorials Video Script One of the most important features of the Linux operating system is the Terminal command line interface. One major design feature of Linux is that nearly every operation can be controlled via the command line, and in fact some operations can either only be performed or are much simpler to perform there.\nTo open the Terminal, click the Activities button and search for Terminal. I recommend adding it to the favorites panel by clicking and dragging the icon to the left side of the screen. You’ll be accessing it very often throughout this class, so having quick access to the terminal is very helpful.\nBy default, Ubuntu uses Bash, the Bourne Again SHell, as the default shell in the terminal. There are many other shells available, so feel free to check them out and see if you find one you prefer. For this course, I will use Bash since it is the default.\nLooking at the Bash terminal, there are a few bits of important information presented immediately in the command prompt, which is the text shown on the screen before the blinking cursor. First, shown here in the green text, is the current username, followed by the at @ symbol, then the current computer name. This helps you keep track of which computer you are using, and which username you are currently logged in as. This becomes very important later, as you’ll be controlling several servers using a remote terminal program.\nFollowing the colon, you’ll see in blue the present working directory. In this case, it shows the tilde ~ symbol, which represents the users home folder. You can use the pwd command to see the full path.\nFinally, this prompt ends in a dollar sign $. That represents the fact that this terminal is not a root terminal, and doesn’t have root permissions by default. A root terminal, such as one you can get by using the sudo su command, will have a pound or hash symbol # at the end of the prompt. You’ll also notice that the colors are disabled in the root terminal, but that can easily be changed in the Bash configuration file.\nLet’s review some simple Linux terminal commands:\npwd - shows the present working directory cd - change directory ls - list files ls -al - show all file details and permissions mkdir - make new directory rmdir - remove empty directory rm - remove file rm -r - recursively remove a folder cp - copy file or folder mv - move file or folder touch - create a file (by “touching” its entry in the file tree); existing files are unchanged except for updating the last accessed timestamp cat - concatenate (print) files apropos - find commands for a keyword whatis - find command descriptions whereis - find location of a command on the filesystem man - find command help As we learned in PowerShell, the vertical pipe | character can be used to take the output of a command and use it as input to another command. Here are a few examples:\nsort grep - search for text wc - get word count To edit files using the Terminal, I recommend using the nano command, as it is generally regarded as the simplest text editor available by default on most Linux systems. If you prefer to use vim, emacs, or another editor, you are encouraged to do so. Also, on a Linux system with a GUI installed, you can use gedit to open the graphical editor as well.\nTo edit a file, simply type nano followed by the name or path to the file to be edited. If it doesn’t exist, it’ll simply open a blank file. Once there, you can enter your information. Remember that Nano is a terminal program, so you cannot use the mouse to move the cursor, only the arrow keys on the keypad. At the bottom you’ll see several commands you can use. The carat ^ character represents the CTRL key. So, ^O would mean CTRL+O for the “Write Out” command.\nTo save a file, you can use CTRL+O to write it to the disk. It will ask you to confirm the filename before saving, and you can just press ENTER to accept the one presented. To exit Nano, use CTRL+X. If the file is unsaved, you will be asked to save your changes. Press Y to save the changes, then ENTER to confirm the filename. In practice, you’ll become very used to the process of using CTRL+X, Y, ENTER to save and close a file in Nano.\nBy default, the Linux Terminal does not give you administrator, or “root” permissions, even though you may be using an administrator account. To use those permissions, a special command called sudo, short for “super user do,” is used. By prefacing any command with the sudo command, it will run that command as the root user, provided you have permissions to do so, and can enter the correct password for the account. In essence, it is a way to “elevate” your account to the root account for a single command.\nFor example, to install a program on Linux, you must have root or sudo permissions. I can try to use the apt-get update command without it, but it will not work. By prefacing that command with sudo I can, but first I must enter the password to my account.\nOne quick trick - you can use sudo !! to run the previous command as root, without having to retype it. It is one of the single biggest timesaving tricks on the Linux terminal, and well worth remembering.\nIn addition, as I showed above, it is possible to switch to a root shell by using the sudo su command. Su is short for “switch user” and is used to change the user account on the terminal. You can use it to log in to any other account on the system, provided you know the password. However, I DO NOT recommend logging in directly as the root user in this way, as any command you accidentally type or copy/paste into this terminal will be run without any warning. It is very easy to irreversibly damage your system by doing so.\nThat is just a short introduction to the Linux terminal. We’ll learn a few other terminal commands as we continue to explore Ubuntu. I also encourage you to read some of the resources linked below the video if you are new to Linux, as they give you much more information about the terminal and how it can be used.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Terminal",
    "uri": "/1-secure-workstations/13-ubuntu-terminal/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Email on Wikipedia Simple Mail Transfer Protocol (SMTP) on Wikipedia Post Office Protocol (POP3) on Wikipedia Internet Message Access Protocol (IMAP) on Wikipedia Why You May Not Want to Run Your Own Mail Server from DigitalOcean Video Transcript In this video, we’ll look at one more set of application layer protocols, the ones for sending and receiving electronic mail, or email.\nEmail was developed in the early days of the ARPANET, and was originally built as an extension to the existing File Transfer Protocol or FTP. Using these email protocols, a user on one system could send a message to a user on another system. Email was originally designed to be a text-only format, but the later introduction of the Multipurpose Internet Mail Extensions, or MIME, allowed emails to include additional content such as HTML markup, images, and more.\nTo send and receive email across the internet, there are a number of protocols involved. First, the Simple Mail Transfer Protocol (SMTP) is used to send mail from an email client to a server, and then between the servers until it reaches its desired recipient. Once there, the recipient can use either the Post Office Protocol (POP3), the Internet Message Access Protocol (IMAP), Microsoft’s Exchange ActiveSync, or any number of webmail clients to actually view the email.\nOn an email server itself, there are several pieces of software involved in sending and receiving email. First, the Mail Transfer Agent (MTA) is responsible for SMTP connections, and is primarily used to send and receive email from other email servers. Next, the Mail Delivery Agent (MDA) will receive email from the MTA destined for users on this server, and will route it to the appropriate mailbox. Finally, there are any number of ways to get the email from the mailbox to the user, using the POP3, IMAP, or any number of other protocols. Or, as is often the case today, the user can simply view the email in a web browser, using a webmail client.\nTo see how email is routed through the internet, here is a nice diagram from Wikipedia showing the process. When Alice wants to send an email to Bob, she must first compose the email and send it to her local MTA using the SMTP protocol. Then, the MTA will use a DNS lookup to find the MX entry for Bob’s email domain, which is b.org in this example. Then, the MTA can send that email to Bob’s MTA, which will then pass it along to the MDA on that system, placing it in Bob’s mailbox. Finally, Bob can then use a protocol such as POP3 to read the email from his mailbox onto his computer.\nAs with many other application layer protocols, several of the core email protocols are text-based as well, and can easily be done by hand using telnet. This slide shows what it would look like to use telnet to receive email using the POP3 protocol. However, most email servers today require the use of a secure protocol when accessing an email account, so it is difficult to perform this activity today.\ncis527@cis527russfeldpuppet:~$ telnet mail.servergrove.com 110 Trying 69.195.222.232... Connected to mail.servergrove.com. Escape character is '^]'. +OK POP3 ready USER test@beattieunioncemetery.org +OK PASS uns3cur3 +OK logged in. STAT +OK 2 5580 RETR 1 +OK 4363 octets follow. Received: (qmail 11929 invoked from network); 26 Feb 2016 16:16:14 +0000 Received-SPF: none (no valid SPF record) Received: from mx-mia-1.servergrove.com (69.195.198.246) by sg111.servergrove.com with SMTP; 26 Feb 2016 16:15:43 +0000 ... ... ... DELE 2 +OK message 2 deleted QUIT +OK Bye-bye. However, many SMTP servers still support sending email via an unsecured connection. This is mainly because there are several older devices such as printers, copiers, network devices, and security devices that are all designed to send alerts via email. Those older devices are difficult if not impossible to upgrade, so many system administrators are forced to use an unsecured SMTP server to accept those notification emails. So, we can use one of those to perform this quick test and see how it works.\nFor this example, I will connect to the K-State CS Linux servers, as the server we are using will only accept emails from a limited number of systems, including these servers. In addition, it will only accept email sent to a limited number of domains. Both of these protections are in place to limit the amount of unsolicited SPAM email that could be sent using this server.\nFirst, I’ll use telnet to connect to the email server on port 25, the well-known port for SMTP:\ntelnet smtp.cs.ksu.edu 25 Then, I’ll need to establish a connection with it, send my email, and close the connection. To make it easier to see below, I’ve prefixed the messages from the server and my telnet client with ###:\n### Trying 129.130.10.29... ### Connected to daytona.cs.ksu.edu. ### Escape character is '^]'. ### 220 daytona.cis.ksu.edu ESMTP Postfix (Debian/GNU) HELO daytona.cs.ksu.edu ### 250 daytona.cis.ksu.edu MAIL FROM: testuser@ksu.edu ### 250 2.1.0 Ok RCPT TO: russfeld@ksu.edu ### 250 2.1.5 Ok DATA ### 354 End data with \u003cCR\u003e\u003cLF\u003e.\u003cCR\u003e\u003cLF\u003e From: \"Russell Feldhausen\" To: \"Test\" Date: Fri, 6 September 2018 10:00:00 -0700 Subject: Test Message This is a test! Hope it Works! . ### 250 2.0.0 Ok: queued as 827D23FDD1 QUIT ### 221 2.0.0 Bye ### Connection closed by foreign host. Once I have completed that process, I should receive the email in my inbox shortly. You’ll notice that there are many things interesting about this email. First, my email client reports the correct date and time that it was received, but at the top of the email itself it gives a different timestamp for when it was sent. Additionally, even though I used my own email address as the recipient, in the email header I listed the name “Test” as the recipient, which is what appears in the email client.\nI hope that this example clearly demonstrates how easy it is to spoof any part of an email. The email servers and protocols themselves don’t enforce many rules at all, so it is easy to abuse the system, leading to the large amount of SPAM email we receive on a daily basis. However, I still find it fascinating to see behind the curtains a little at how these protocols are actually structured and how easy it is to work with them.\n",
    "description": "",
    "tags": null,
    "title": "Email Protocols",
    "uri": "/3-core-networking-services/14-email-protocols/index.html"
  },
  {
    "content": "Greetings! Here is the information you’ll need to know to get your lab graded.\nGrading Date/Time: \u003ctime\u003e Contact me ASAP if that time will not work for you for any reason and I will work with you to reschedule. Zoom Link: \u003clink\u003e If you haven’t used Zoom before, I recommend going to https://ksu.zoom.us/ first to download the Zoom client software. Then, when the time comes, click the link above to join the Zoom session. Your Setup: Before you join the Zoom session for grading, please have the following steps completed on your end. This helps things go as smoothly as possible: Use the fastest internet connection available to you. If you are using a laptop with a wireless connection, you may want to plug in to a hard-wired connection if available. If you can work from campus, that may be best, depending on your home internet connection. Make sure your audio input device (microphone) is working in Zoom. You are not required to have a webcam, but you are welcome to use one if you like. I will be using a webcam and microphone on my end. Confirm that all the VMs for this lab are booted and running on your system. We’ll be using the desktop sharing features of Zoom so I can see your VMs. So, make sure you close any windows or programs that are running in the background that shouldn’t be shared with me. Operations: Here are a few operations you may be asked to demonstrate: Windows Backup/Restore Screenshots Ubuntu Backup README Ubuntu Monitoring System DevOps - push to Git, see changes on Server Finality Once you begin the grading process, no changes may be made to your system to fix any errors encountered. The grade you receive will reflect the state of your VMs at the start of the grading process. This is to ensure that grades are fairly determined and that no special consideration is given. That should cover everything. Please let me know if you have any questions prior to your scheduled grading time.\nGood luck!\nRuss\n",
    "description": "",
    "tags": null,
    "title": "Lab 7 Grading Email",
    "uri": "/z-instructor-resources/14-lab-7-grading-email/index.html"
  },
  {
    "content": " YouTube Video Resources Slides User Accounts from Ubuntu User Management from Ubuntu Server Guide How to Add and Delete Users on Ubuntu 18.04 from DigitalOcean (Works for 20.04) Gnome System Tools Package from Launchpad Install this package to get the GUI to manage Users and Groups The Beginner’s Guide to Managing Users and Groups in Linux from How-To Geek Linux Users and Groups from Linode Video Script In this video, we’ll explore how to work with users and groups in Ubuntu Linux. Before we do that, however, let’s take a minute to discuss some important information about user accounts and how they are managed in Linux.\nEvery Linux system has a special user account called “root.” This account is the default system administrator account, and it has complete control over the system. It is easily identifiable by its UID, which is always 0. In fact, when many programs see that the current UID is 0, they will skip any and all permissions checks without looking at anything else.\nOn Ubuntu, the root account is disabled by default. It can easily be enabled by setting a password on the account using the passwd command on the Terminal. However, that is not recommended, as it makes your system more vulnerable to some kinds of attacks. Instead, the system provides the sudo command to allow regular users with administrative permissions to perform commands as the root user, without actually using that account.\nThe sudo command stands for “Super User Do” and is by far one of the most important commands to learn as a system administrator. By prefacing any command with sudo while using an account with the appropriate permissions and providing the account’s password, you can become the root user while that command is running. To give users the ability to use sudo they simply must be added to the “sudo” group or have their user type set to Administrator.\nYou can also customize how the permissions for sudo are configured by using the visudo command to edit the /etc/sudoers file. For example, you can limit which commands certain accounts are able to perform. Be very careful, as that file has a very specific format and any invalid changes could render the entire system unusable. I generally don’t recommend editing this file unless necessary.\nAs you configure Ubuntu accounts, you’ll notice that there are three major account types. The first type, Regular User, consists of two sub-types - Standard and Administrator. These accounts are the ones that will be used by real people logging-in to the system interactively. Standard users do not have access to the sudo command, while Administrator users do. The regular users typically have a UID starting at 1000 and above, and each one has a group of the same name created with that user as the only initial member. This is very important for assigning file permissions, as we’ll see in a later video. They also receive a skeleton of the default directory in the /home folder. As you learn about directory services in Module 4, you’ll find this information to be helpful once again.\nThe other type of user, System User, is a user account created for a program or service. These are similar to the pseudo accounts on Windows 10, but on Ubuntu there are typically several of them, with a unique account assigned to each service. For example, the Apache web server which will be installed as part of this module creates its own user account, which can be used to determine what files can be accessed or served by the web server.\nNow, let’s look at the different ways user accounts can be configured on Ubuntu. First, we can go to the Settings application. From there, click the Users button on the left. Here you can see information about your current user account, and are able to change the password. You cannot change the user account type of the current account. To add a new account, you must click the “Unlock” button at the top of the screen and enter your current password. This is the graphical equivalent of using a sudo command. From there, you can create a new user.\nYou can also install the gnome-system-tools package to get access to a different interface for managing user accounts. Once it is installed, you can search for “Users and Groups” after clicking the activities button to find it. Here, you can add or edit accounts, and you can also manage existing groups. However, you are still unable to add users to groups or do any advanced editing here. To do most of that, you’ll need to use the terminal.\nThere are several terminal commands which are useful. First, the adduser command will walk you through the process of adding a user to the system. This utility is not part of the default Linux system tools, but has been added to Debian and Ubuntu to simplify the process of adding a regular user. To add a system user, user the useradd command. You can also use commands to remove or modify a user. For example, to add a user to an existing group, use usermod -a -G \u003cgroup\u003e \u003cuser\u003e.\nFinally, you can also use the passwd command to change the password for any user account.\nTo work with groups, very similar commands are available. You can add a group using groupadd by providing a name for the group, as shown here. There are also a couple of commands for setting passwords and security for groups, which are shown in italics on the slides. Those commands are not typically used, but if you are interested in them I encourage you to review the resources below the video to learn more.\nOne important aspect of working with the Linux operating system is that most system settings and information are stored in simple text files, and anyone with access to those files can see how the system is configured. Let’s take a look at the 4 most important ones.\nFirst, /etc/passwd is the traditional place to find information about a user account. It includes the username, UID, home folder location, and more. It used to include a hash of the password as well, but that was removed years ago because it was a security threat.\nThose passwords were moved to /etc/shadow, which you’ll notice is a very protected file. In fact, I can only access it by using the sudo command. Here you’ll see the hash of the cis527 user account we are using.\nSimilarly, there are /etc/group and /etc/gshadow files, serving the same purpose for groups.\nOf course, it should go without saying that it is not recommended to edit those files directly. Instead, use the appropriate terminal commands to modify information about user accounts and groups.\nAt this point, you should be ready to start working on Lab 1, Task 5 - Configure Ubuntu 20.04. One of the first tasks in that lab is to create several user accounts, so that will be great practice for using some of these tools and commands.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu User Management",
    "uri": "/1-secure-workstations/14-ubuntu-user-management/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Transport Layer Security (TLS) on Wikipedia Transport Layer Security (TLS) from High Performance Browser Networking on O’Reilly Firewall on Ubuntu Server Guide Uncomplicated Firewall on Ubuntu Wiki Fine-tuning Firewall Rules: 10 Best Practices from eSecurity Planet Video Transcript Lastly, it is very important to take a few minutes to discuss some security concerns related to networking. While this isn’t a security course, it is vital for system administrators to understand the security tradeoffs of any system they configure.\nWhen dealing with networks, there are a few security concepts to keep in mind. First and foremost is your firewall. On any system you work with, you should ensure that the firewall is properly enabled. It should also be configured to only allow incoming connections on the smallest set of ports that will allow the server to function. In other words, you should only allow ports for software you are planning to use on the server, and all other ports should be blocked. This follows the security concept of “Principle of Least Privilege,” where you only allow the smallest number of permissions required for the system to function.\nSimilarly, many larger networks should also employ some network monitoring software to scan for unwanted network traffic. While a firewall helps guard against incoming connections, it cannot detect a malicious outgoing connection from a compromised system. A proper network monitoring system could help you detect that breach and stop it before it becomes a larger issue.\nFor smaller networks and home users, simply installing and using a NAT router offers a significant layer of protection with minimal hassle. A NAT router will block all incoming traffic by default, unless you or one of your systems requests a port to be opened for a specific use. While it isn’t a perfect system, in general this is much better than connecting a computer directly to the public internet itself.\nLastly, for any network connections, you should consider using some form of encryption and authentication on the connection. By doing so, you can be sure that you are connecting to the correct system on the other end, and that no one else can intercept your communication once it is established. Thankfully, there are many ways to accomplish this.\nMany systems today use Transport Layer Security, or TLS, to help secure their network connections. TLS was formerly known as SSL, and while many users may use the terms interchangeably, technically SSL is an obsolete protocol that was replaced by TLS many years ago, so TLS is the proper way to refer to it today. The TLS name can also be a bit misleading, because it doesn’t truly reside in the transport layer. Instead, it is typically above the transport layer, but below the top three application layers of the 7 layer OSI model. Many diagrams show it as part of the session layer, which is a pretty good way to think of it.\nIn essence, what TLS does is perform a handshake protocol after the TCP connection has been established between the two systems, but before any application data is sent or received. This diagram shows what a TLS handshake entails. The blue boxes at the top are the TCP connection being established, followed by the tan boxes giving the steps of the TLS handshake protocol. During that process, the two systems exchange security certificates, agree on an encryption algorithm, and generate a shared encryption key that is known only to those two systems. The sender and recipient can confirm each other’s identity based on a “chain of trust” for the certificates presented; in essence, they look to see if the certificate is signed by someone they trust, in which case they can also trust the certificate itself. Once that is done, it then cedes control to the application, which can then start sending and receiving packets.\nWhen a packet is transmitted, the TLS layer will encrypt the packet so that it cannot be read by others. It will also include a second part of the message, called a message authentication code or MAC, that will help detect if the encrypted packet is altered in any way.\nMany protocols today use TLS by default, including HTTP. You’ve probably seen the https protocol in your web browser many times. That simply means that your browser is using TLS to communicate with the web server. In addition, many browsers will show information about the certificate presented by the web server, and will alert you if the certificate is untrusted.\nThis is a very brief overview of the security concerns involved in a networked world. I encourage you to consider taking additional courses in cyber security if you are interested in learning the details of how these systems are implemented. In any case, you should always be thinking about security as you configure your systems to connect to a network.\n",
    "description": "",
    "tags": null,
    "title": "Security",
    "uri": "/3-core-networking-services/15-security/index.html"
  },
  {
    "content": " YouTube Video Resources Slides An Introduction to Linux File Permissions from DigitalOcean File Permissions from Ubuntu Enable Open as Administrator on Ubuntu 18.04 from Website for Students chmod on Just Enough Linux Linux Users and Groups on Linode Video Script In this video, we will discuss how file permissions work in Ubuntu Linux. Working with file permissions in Linux is sometimes one of the more challenging aspects of the operating system, as it is very limited compared to Windows. It requires a bit of thinking to determine how to configure your permissions, but if done correctly it can be very powerful.\nLinux file permissions are assigned based on three classes of user: the owner of the file, the group assigned to the file, and all other users not included in those first two classes. Then, each class can get any combination of three file permissions: read, write, and execute. So, in essence, each file has 9 bits in its mode, determining which of the three permissions are assigned to each of the three classes.\nTo see these permissions, you can open a terminal and use the ls -l command. It will show a long listing (hence the l) of the files in the folder. You’ll receive output very similar to this diagram, from the great tutorial on Linux file permissions from DigitalOcean, which is linked in the resources below the video. The first column gives the file permissions, known as the “mode” of the file. The next columns give the user and group assigned to the file, respectively. Following that, you’ll find the file size in bytes, the last modified date, and the filename.\nThis graphic shows how the mode of the file works in detail. The first character is a d if the item is a directory, or a hyphen - if it is not. The next three characters give the read, write, and execute permission status for the user or owner of the file. The character is present if the permission is given, or a hyphen if it is not. The next three characters do the same for the group, then the last three for all other users.\nGoing back a bit, we can see that there are several different permission setups here in this graphic, with helpful filenames for each one to help us understand them. First, notice that the top two entries are for directories which are accessible either by everyone, or the group.\nFor users to be able to list the files inside of a directory, they must be given both the read and execute permissions, as we can see here in the eighth entry. If a user is given read permissions but not execute, they would be able to read files inside of the folder, but not able to get a listing of the files names, which isn’t very useful. For files, the execute bit is only needed when the file is either an executable program, or a script written in a scripting language that is directly executable.\nLooking through the rest of the entries, we can see what it would look like to have a file private to a user, or publicly accessible (though not writable) by all users. I encourage you to familiarize yourself with this pattern, as it will be very useful throughout this class.\nFile modes can also be expressed as a set of three octal characters. This is very useful if you have a firm understanding of what each octal mode represents. For each class of users, think of the three permissions, read, write, and execute, as three bits in a binary number. If read and execute are given, then the binary number is 101, which represents 5. That would be the octal value for that set of permissions. So, by using three octal values representing the permissions given to the three classes of user, it is very simple to set permissions using just three digits.\nThis page shows some common octal values you may use. I recommend reviewing it as you need while setting permissions. It is available in the slides linked below this video.\nNow, let’s see how to set these permissions using the tools available in Ubuntu Linux. First and foremost, you can do some work with permissions via the default file browser, called Nautilus. As in Windows, you can right-click on any file or folder, choose Properties, then click on the Permissions tab. Here, you can see several options to choose from. Each one should be pretty self-explanatory based on the information we’ve already covered in this video.\nHowever, you’ll notice that you can only change the permissions on files which you are the owner of. So, if you want to change the permissions of a file outside your home folder, for example, you won’t be able to do so with Nautilus. There are a couple of ways around that, though. One is to install the nautilus-admin package. This will give you an option in Nautilus to right-click a folder or file and open it as an administrator using the sudo command. This will allow you to change the permissions all folders on the system, but it still does not easily allow you to change the owner or group assigned to those files.\nFor full control over file permissions, once again we must turn to the Terminal. There are several commands that are used to modify file permissions and owners. First is the chmod or “change mode” command. It is used to set the permissions on a file. The syntax is the command, followed by some type of descriptor for the permissions to be set, then the path to the file or folder to set those permissions on.\nThere are two ways to use this command. The first involves using the classes, permissions, and a set of operators to state how the permissions should be set. For example, ug+rw will add the read and write permissions to the user or owner of the file, as well as the group. You can also say g=rx to set the group permissions to be exactly read and execute, or o-rwx to remove all permissions from users other than the owner or the group. Finally, there is a special class, a, which represents all classes. So, a+x will give the execute permission to all classes of user.\nIn my opinion, I find that method simpler to understand in theory, but a bit more frustrating to work with in practice, because I have to explicitly spell out my permissions in a long-form manner to get exactly what I want. Thankfully, you can also specify the desired permissions using octal modes as well.\nYou can also use the chown command to change the owner or group assigned to a file. For this command, note that the user and group are separated by a colon, but no spaces are before or after the colon. Also, note that each file must have a group assigned to it, even if you only want the owner to have access. In that case, remember that each user account has a group of the same name, so you can assign that group as the group on the file as well.\nFinally, there is also a chgrp command that allows you to change the group on a file you own. However, most users still just use the chown command for this use. However, if you do not have sudo access, this command is very useful.\nOf course, in many instances you’ll find that these commands require the use of the sudo command, In general, you can only change the permissions or group on a file you own without using sudo. Any other changes, including changing the owner of the file, require sudo access. It should also be noted that anyone with sudo access will in effect be able to access any file on the system. So, there is always a tradeoff between giving users the power to make exhaustive changes to file permissions and them being able to access any file on the system.\nNow that you know a bit more about file permissions in Linux, you are ready to begin the next part of Lab 1 which is Task 6 - Ubuntu Files \u0026 Permissions. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu File Permissions",
    "uri": "/1-secure-workstations/15-ubuntu-file-permissions/index.html"
  },
  {
    "content": "\rDecrypt\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\n",
    "description": "",
    "tags": null,
    "title": "Solutions",
    "uri": "/3-core-networking-services/16-solutions/index.html"
  },
  {
    "content": " YouTube Video Resources Slides How to Use ps, kill, and nice to Manage Processes in Linux from DigitalOcean Daemon (Computing) from Wikipedia How to Manage systemd Services on a Linux System from How-To Geek Systemd for Upstart Users from Ubuntu Video Script In this video we’ll be looking at managing the processes and services running on an Ubuntu Linux system. First, let’s take a look at processes. Recall from the earlier video on Windows Processes and Services that a process is a program running in the foreground for the user, while a service is a program running in the background for the operating system.\nOn Ubuntu, you can see all the running processes using the System Monitor program. It can be found by searching for it after clicking the Activities button. Here you can find information on all running processes, system resources, and file systems. By right-clicking any of the running processes, you’ll be given options to view information about the process, or even stop or kill the process if it isn’t working properly.\nThere are also several command-line tools you can use to view the running processes. Installed by default is a program called top which will show much of the same information as the System Monitor. Using the keyboard, you can interface with this menu and view more information as well. Press the q key to quit. There is also a newer, more user-friendly version of top called htop but it is not installed by default.\nYou can also see the running processes by using the ps command. By default, it will only show the processes running in that terminal window, so you must use the -A option to see all processes on the system. You can also use the -u option, followed by a username, to see all the running processes associated with that user, which is very handy on multi-user systems. The pstree command works similar, but it shows a tree structure demonstrating which processes were spawned from other processes, which can be very helpful in some instances.\nTo stop a process via the Terminal, use the kill command. Technically, the kill command is used to send signals to a process, and it has other uses than just stopping a process, but that is the typical usage. Refer to the documentation linked in the resources section for specific instructions on how to use each of these commands in more detail.\nFinally, there are a couple of other system management tools I’ve found useful over the years. Once of them is glances, which is a Python based system monitor that goes well beyond the tools we’ve seen so far. You can easily install it on your system and get a view like this that shows lots of information about the system. However, since it is based in Python, it installs many additional resources on your system to reach full functionality, which may or may not be desired.\nThe other tool I recommend is called saidar. It is a more traditional terminal program with fewer dependencies, but still gives a good overview of the system. However, it does not display information about individual processes, so it may be of less use in this situation than the others.\nNext, let’s talk a little bit about services on Linux. In many books and online, you’ll see the term daemons used to describe Linux services. That is the more traditional term for these processes that run in the background, and it is still used today by many resources. The term daemon comes from Greek mythology, referring to a being who works tirelessly in the background. It is also a reference to Maxwell’s Demon, a similar concept from a well-known thought exercise in physics. Most daemons in Linux today use a process name ending in d, such as httpd for a web server or sshd for a secure shell server.\nOn a modern Ubuntu system, the services or daemons are managed by a tool called systemd. Formerly, tools such as init or Upstart were used, but most of them have been phased out at this point in favor of systemd. It is still a very contentious point among Linux enthusiasts, and you’ll find lots of discussion online covering the pros and cons of a particular system. I won’t go too deep into it here, but if you find references to either init or Upstart online, that’s what the discussion is about. To interact with systemd, you can use the systemctl command.\nFor example, to start a service, you would use sudo systemctl start followed by the name of the service. Other options such as stop, restart, and status are available. Unfortunately, advanced configuration of systemd services in Ubuntu is a bit outside of the scope of this course, as it is a very involved topic. I encourage you to review the documentation for systemd if you find yourself needing to make any changes to services on your system.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Processes \u0026 Services",
    "uri": "/1-secure-workstations/16-ubuntu-processes-services/index.html"
  },
  {
    "content": "As I was reworking this lab for the Summer 2020 semester, I ran into a few issues getting my own bind9 server to work. In doing so, I ended up taking some great screenshots of common misconfigurations of bind9 and the related output from using the dig command to diagnose the issue. I figured it would be helpful to share those screenshots here to give you a nice guide to some common errors you may encounter when working with bind9.\nIncorrect Screenshots Bad PTR Response In this screenshot, I attempted to do a reverse lookup of 192.168.40.41, which should resolve to ns.cis527russfeld.cs.ksu.edu. This test was done from the machine with that IP address. Unfortunately, my server was misconfigured, and the reverse zone file was not properly included (the path in my named.conf.local file had a typo, to be exact). Since that file was not availble, my system returned the response from it’s local DNS information, showing that the IP address 192.168.40.41 belongs to cis527U-russfeld., which is the name of the actual computer and not its name in the DNS system. This can happen if you try to request information about the local system and the DNS server does not contain the correct information.\nNo PTR Response With the same misconfiguration as described above, when I query a different system, such as 192.168.40.42, I instead get an empty response as shown here. This is the usual situation if the DNS server is misconfigured and the PTR record cannot be found. If the output of dig is missing an answer section, it usually indicates that the lookup failed.\nMissing Period So, I corrected my error and restarted my bind9 server, and once again tried to query one of my reverse lookup addresses. However, in this case, I did get a response from the DNS server, but it was incorrect. The address returned when I queried 192.168.40.42 as windows.cis527russfeld.cs.ksu.edu.40.168.192.in-addr.arpa, which is not what I wanted. This can happen when you forget to add the all important period at the end of an entry in your zone file.\nThat one, right there. If you forget that one, the bind9 DNS server will automatically append a whole bunch of domain information after the address. Sometimes we want that, but in this case, we don’t. So, be very careful and don’t forget the periods.\nDNS Caching In this screenshot, I had just resolved the error above and restarted my bind9 server to load the newly changed files. Then, I once again used dig to query 192.168.40.42, expecting to see the correct output. Instead, I still received the incorrect output as seen in the screenshot above. This is a very common problem!\nThere can be a couple of causes of this issue. First, whenever you update a zone file, don’t forget to change the serial value to a larger number. The DigitalOcean guide just has you start with 2 and go to 3 and so on. I like to use the date, followed by a serial for that day. So, I would use a serial such as 202005061 for the first update made on May 6th, 2020 (2020-05-06).\nSecondly, even if you remember to do that, many modern systems employ some form of DNS caching that stores the responses from previous queries. Both Ubuntu and Windows do this automatically. So, you may also have to clear your DNS cache to make sure it is properly querying the server. On Ubuntu, use sudo resolvectl flush-caches and on Windows use ipconfig /flush-dns.\nImmediately after flushing the DNS cache, I was able to perform the query again and get the correct result.\nCorrect Screenshots Below are screenshots showing correct output for various dig commands. You may use these screenshots to help in your debugging process. To take these screenshots, I updated the static IP address configuration for the Ubuntu Server to use it’s own IP address 192.168.40.41 as the only DNS server.\nCorrect A Records Correct CNAME Records Correct PTR Record Correct Recursive Lookup (Outside our DNS Server) ",
    "description": "",
    "tags": null,
    "title": "Bind Troubleshooting",
    "uri": "/3-core-networking-services/17-bind-troubleshooting/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Install Additional Applications from Ubuntu Use Synaptic for More Advanced Software Management from Ubuntu Apt-Get How To from Ubuntu How Software Installation \u0026 Package Managers Work on Linux from How-To Geek Snaps from Snapcraft.io Introduction to Snaps from Ubuntu Tutorials Video Script Let’s take a look at installing software on Ubuntu Linux. Before we install anything, here is some information about the types of files installed along with a typical application in Ubuntu.\nFirst, the executable file itself is placed somewhere in the path of the system, usually in either /usr/lib or /usr/bin. This is equivalent to the .EXE file in Windows. Each application may also have some shared library or object files, which are typically placed in the /usr/lib folder.\nThe configuration and settings file for the application are usually placed in the /etc folder, with user-specific information placed in a dotfile in the user’s home directory. To see those files, use the ls -al command to show all hidden files. By convention, any file or folder with a name beginning with a period . is hidden by default. These files are typically referred to as “dotfiles” and are used to store user-specific configuration information in the user’s home folder. They are hidden by default just to prevent most users from ever touching them, but by including them directly in the user’s home folder, they are more likely to be included in system backups. On Windows, this is roughly equivalent to the AppData folder in each user’s home directory.\nFinally, every application on Linux typically installs some sort of documentation in the /usr/share directory, which can be accessed using the man command. If required, it may also place a few scripts in the systemd directories so the appropriate service or daemon can be started automatically.\nThere are several methods to install software on Ubuntu, each with their own advantages and disadvantages. First, most users primarily will start with the Ubuntu Software Center. This is the equivalent of an “app store” for Ubuntu, and it gives quick and easy access to the most commonly used applications. However, many system and server packages are not visible here, and must be installed elsewhere.\nThe official Ubuntu documentation recommends installing the Synaptic Package Manager for those tasks. It can be installed via the Ubuntu Software Center and then used on its own. It will show you all packages available for your system, and you can easily install or manage them. One nice feature about Synaptic is the ability to see what files are installed by a package by simply right-clicking on an installed package and choosing Properties, then the Installed Files tab. You can compare this to the output from InstallWatch Pro on Windows.\nUsing Synaptic, you can also configure the software repositories your system is able to install software from. While most software you’ll need is easily installed through the default repositories, you may find that adding or changing those repositories becomes necessary, and you can do so here.\nOf course, you can also install and configure software using the command line. The most common way to do so is via the apt, or “Advanced Packaging Tool” command. In previous versions of Ubuntu this was the apt-get command, but the syntax for the apt command is exactly the same. To use apt, you must first update the list of packages available by using sudo apt update. Then, you can install a package using sudo apt install \u003cpackage\u003e. To remove a package, use the remove option. You can also use apt to upgrade all packages on a system using the upgrade option.\nAnother option for installing packages on Ubuntu is using the snap tool. Snaps are self-contained file system images containing the app and any dependencies, and are designed to be able to be installed on all major Linux systems. Several of the default Ubuntu packages are using snap, and the Ubuntu Software Center will list snaps if they are available. For this course, I will not be using snaps since they are not quite as widely supported yet, but feel free to review the relevant documentation listed in the resources section if you are interested in learning more.\nIn addition, you can always install software on Linux by downloading and installing the software package manually. The packages typically use the .DEB file extension, and are installed using the dpkg -i command. However, in most instances you’ll be able to find a software repository which offers the software for installation via the apt tool, and I recommend using that route if at all possible.\nFinally, if you so choose, you can download the source code for most applications built for Linux and compile it yourself. This is very rarely needed for most mainstream software, but often there is specialized software for a specific use that requires this level of work. In general, consult the documentation for that particular software for instructions on how to properly configure, build, install and run that software from source.\nYou should now be able to continue working on Lab 1, Task 5 - Configure Ubuntu 20.04 by installing the required software. As always, if you have any questions, please post in the course discussion forums to see if others are having the same issue.\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Software Installation",
    "uri": "/1-secure-workstations/17-ubuntu-software-installation/index.html"
  },
  {
    "content": " YouTube Video Video Transcript Okay, so I’m going to try and talk through a model solution version of lab three and give you some advice on how to go about this lab. And also how to verify that you got this lab configured and working properly. So the first thing we’re going to do is we’re going to look at our IP address. So I’m going to go to the Edit menu in VMware and then choose virtual network editor. And in the virtual network editor, we’re going to find our NAT network. And we’ll notice that it has a subnet address of 192.168.40.0, yours will probably be similar, but it may have a different third octet right here. It’s usually randomly assigned by VMware when you install it. Mine is dot 40. Yours might be different, so we need to make note of that IP address. The other thing in here we can see is this checkmark the use local DHCP service; once you get done with task four and have your DHCP server running, you’ll want to go in here and uncheck this box, you have to click this change settings button to give it admin access. But then you can uncheck this box to turn off VMware’s DHCP service, so you can use your own. So remember we have 192.168.40.\nSo up here in Ubuntu, we want to set a static IP address in that same range. So in my settings, if I go in here and look at IPv4, you can notice that I’ve set an IP address 192.168.40.41. And the dot 41 is what the assignment says that we’re going to use for our Ubuntu server. We also need to set the gateway. In VMware, it uses a default gateway of dot two inside of your network. The reason it does this is dot one is usually used by your home routers and some other services. And so to avoid any conflicts, VMware has set it to dot two. We also have the netmask of 255.255.255.0. That’s pretty standard. We talked about the lecture. And then you notice that right now I’ve actually set it to refer to itself as a DNS server. Originally in the lab, you’ll probably set this to dot two, so that you still have working internet. But once you get your DNS server working, you can set it to dot 41. So that actually refers to itself for its DNS. So we’ll go ahead and apply this once you apply it, you can turn it off and turn it back on to make sure it’s applied.\nThis is the only VM in this lab that you need to set a static IP address on. I’ve seen some students try and set static IPs on the Ubuntu and the Windows, you don’t need to do that. For testing of your DNS server, you can set a static DNS entry temporarily. But the idea is by the time you get done with lab three these to your Ubuntu client and your Windows client should both be getting its IP address from the DHCP server on your Ubuntu server.\nOkay, so once we have that set, we can confirm our IP address is correct by doing ip addr show that will show us our IP address. And we’ll notice right here that we see the 40 dot 41 IP address. We also need to keep in mind this ens33. This is the name of the interface that this is connected on. And so in the DHCP setup, you’ll need to go in and set a file that defines which interfaces to listen on. This is the interface we want to listen on. But for the DNS server, we actually tell it the IP address that we want to listen on. And so we’ll need this IP address in our DNS server configuration.\nSo once we have our DNS server up and running, we’ll also need to make sure that we allow it through the firewall, I’ll leave that up to you, you can look up what ports you need and how to allow that through the firewall.\nBut if it’s working, we can use the dig command followed by an at symbol and the IP address of our system. And so what this does is this tells dig to ignore any DNS settings we have on our system and query this particular server, and we’re going to query for ns.cis527russfeld.cs.ksu.edu. And when we do that, we get this response. Here, just to help. Let me put this command back up here again. So I’m doing dig at the IP address of my server. And you can do this either on the server or on the client. And then I’m giving it the name of a server that I want to look up in our DNS configuration. And so if I run this command, in this answer section, I should get back that this name goes to this IP address, which is correct. So that’s how we look up an entry on our forward zone.\nIf we went to look up an entry in our reverse zone, instead of doing the name, we do dash x, and then we would put in an IP address, I’m going to put in the IP address 42 which when we do this, it will show our answer section is we’re looking up 42.40.168.192.in-addr.arpa, which is the reverse lookup IP address. And then we are getting our Windows Server back.\nSo to really check to see if your DNS server is working, those are the two commands you need. You just need to be able to use dig at your server’s IP address, and then dash x for reverse lookup, or for forward lookup, you just put in a domain name. You can also test your forwarding. If you want to make sure your forwarding is working, I can dig that server and just look for www.yahoo.com for example, and I can get responses from that. I can also look up the reverse. If I want to look up a reverse, I can do a reverse for 208.67.222.222 which is one of the OpenDNS resolvers. So this is pretty much confirmed that our DNS server is working on the server.\nIf you want to check it from one of your clients, you can switch over to your client, and here on the client I can run those exact same commands. So if I do dig at my server IP address, and this is without setting any static DNS entries, this is just telling dig to query this IP. And then if I do my address here, then I can run it. And once again, we’re getting that answer. And so this is how you can check, you can test your DNS configuration before you have DHCP working, you don’t have to change anything on your client, you just use the at symbol in your dig command to tell it to query that particular DNS server for this entry.\nSo that’s the DNS parts. If that doesn’t work, there’s a really good guide on DNS troubleshooting that talks you through a lot of different things you might run into, Be especially careful of your periods and your spacing in your DNS configuration files. All of that is really important. There’s also a really good discussion on Piazza right now about the number of octets in your reverse zone file. In the dot local file, you can have three octets and then you only need one octet in your zone file. If that doesn’t make sense post on Piazza, let us know. We can try and clarify that.\nOkay, so the second part is your DHCP server. And the DHCP server configuration is pretty simple. However, I’m going to call out one thing real quick. If I go to /etc/default, you’ll notice there’s a file in here that might exist. That’s, whoops. Helps if I go to the server. I go to the server. And then looking here, you’ll see there is a file called isc-dhcp-server. And so in this file, it has some entries. And depending on what guide you read, sometimes you will need to define an interface right here that your DHCP server could listen on. Mine I didn’t need to define that, it just worked. But in a lot of cases, you might need to define that. So this is where you would put that is that ens33 that we saw earlier.\nSo once you get your DHCP server running, you can start it, I’m just going to restart mine. So I restart that. And now your server should be running. If you want to check, a good way to check is you can do sudo cat /var/log/syslog. And we’ll look at the very end of our syslog. These last few entries. And we can see here that our, that our DHCP server started up; it read its config file. And then it will say things like listening and sending. And as long as you see that it’s listening on ens33. That means it’s working. If you get an error here that says it doesn’t, it’s not configured to listen on any interfaces, that’s where you might need to change that defaults file to listen on ens33 to work. So. That’s how you can tell your DHCP server started correctly.\nThere’s a couple other commands that you can use using the ss command to look at listening ports. So ss -l will show you all the listening ports on your system. However, it’s not very useful at all. So instead, we can try two different versions, I’m going to do ss -l, then I’m going to do t which will listen for TCP ports only. And then n which will show us the port numbers. So if I do ss -ltn, I will see really quickly that I have a DNS server listening right here on port 53. That’s what we’re looking for for DNS. DHCP, however, is not TCP is UDP. And so if I look for UDP, I’ll see a DHCP server listening here on every IP address on port 67. And so there is the entry for your DHCP server. And so if you’re not sure if your servers are running correctly, you can use ss -ltn or ss -lun and look for entries that end in 53 for your DNS server or 67, for your DHCP server.\nYou’ll also notice that there is 161 for your SNMP server if you have that running. So this is kind of a really useful thing. You can also see your SSH server running on 22222, and we’ll see our HTTP server Apache running on port 80. So a couple of quick commands.\nBut the really easy way to test your DHCP server is actually on Windows. And so on Windows, I have no static IP address set. If I do ipconfig /release, that will release all of our IP configurations. And then I can do ipconfig /renew to force it to renew that IP address. And so it will think for just a minute and then it will come back and it should give us an IP address that has something within our range. It has dot two as the gateway. But the big giveaway is you can find this connection-specific DNS suffix, and it will contain your eID. That’s the dead giveaway that you are getting a DHCP address from your DHCP server. And that’s why it’s actually really easy to test this on Windows.\nNow, if I do ipconfig /all, then it will show all the information. And you can see that it also is getting the DNS server for your network. And so it actually gets the right DNS server, it will also show you what your DHCP server is. So if you want to know where that IP address came from, you can find it here as well. And then, of course, we can use nslookup to look up different names on our network. So if I do nslookup, I can look that up. And if I do the reverse, I can get the reverse as well. So everything is looking really good here.\nThe other thing you can do, obviously, for testing is on your Ubuntu client, you can do sudo dhclient -r, which will run the DHCP client and refresh. If I do sudo dhclient -v, you’ll actually see it sending the DHCP discover. So it sends to discover, then we get an offer of this IP address from our server IP. And you can see that it works.\nSo those are a quick overview of what lab three should look like when it’s working correctly. The big things to do are get your DNS server up and running, and then test it here on your server using those dig commands with the at symbol on them. So if we go back through my history, we find some of these dig commands having the at symbol where we can query a particular DNS server and look up a particular address. Once you get that working, you can then test it from your client using that same syntax or you can just go ahead, get your DHCP server up and running, check the system log to make sure it’s running. And then go to your Windows client, reboot it and make sure that it’s getting an IP address from your system. And then you should be good to go.\nIf it’s not working, check your config files, check your firewall, make sure that it’s listening on the right ports, or feel free to reach out on Piazza and let us know. So I hope this video was helpful. If you have suggestions or questions or anything else I can go over please let me know.\nGood luck.\n",
    "description": "",
    "tags": null,
    "title": "Lab 3 Demo",
    "uri": "/3-core-networking-services/18-lab3-demo/index.html"
  },
  {
    "content": " YouTube Video Resources Slides Networking, Web \u0026 Email from Ubuntu Networking from Ubuntu Server Guide Internet and Networking from Ubuntu UFW (Uncomplicated Firewall) from Ubuntu GUFW (Graphical Interface for UFW) from Ubuntu Video Script In this last video on Ubuntu, we’ll discuss some important security and networking details needed to finish getting your VMs set up and configured properly.\nFirst, let’s talk about security. Whenever you install a new operating system, there are 4 major steps you should always perform before doing just about anything else on the system. Those steps are:\nConfigure User Accounts \u0026 Passwords - at the very least, make sure you have at least one user account with a secure password added to the sudo group to perform administrative tasks. Most other users should have standard accounts without sudo access. You may also add this system to an LDAP directory to get additional user accounts. We’ll discuss that in detail in Module 4. Configure the Firewall - Ubuntu ships with a firewall installed, but by default it is not configured or enabled. To configure the firewall, I recommend installing the gufw tool as a graphical interface for the Ubuntu Uncomplicated Firewall (UFW). Once that tool is installed, you can enable the firewall with a single click. Install Antivirus Software - All computers should still run some form of antivirus software. Ubuntu does not provide any by default, but it is very easy to install the ClamAV scanner. It is an open source antivirus scanner, and is recommended for use on all Linux systems. Install All System Updates - You should also install all available system updates for your operating system. This makes sure you have patches against the latest known flaws and attacks. Ubuntu has the ability to install updates automatically, but it must be configured. You can search for the Software \u0026 Updates program after clicking the Activities button. Once there, look at the Updates tab to configure automatic updates. You can always use Synaptic or apt to make sure all packages are the latest version. Let’s take a quick look at the firewall, since you’ll need to allow an application through the firewall. You can find it by searching for “firewall” once you have installed gufw. Lab 1 directs you to install the Apache 2 web server. You’ll need to allow it through the firewall somehow. I won’t show you how in this video, but I encourage you to review the links in the resources section below the video for documentation showing how to accomplish this task. There are several ways to do it.\nTo test your firewall configuration, you can use your Windows virtual machine created as part of Lab 1. First, make sure they are both on the same network segment in VMware by looking at the hardware configuration for each virtual machine. Then, you’ll need to get the IP address of the Ubuntu computer. There are several ways to do this, but one of the simplest is to go to the Settings application, then choosing Network from the menu on the left. In the Wired section, click the gear icon to see the details. Here you’ll find the IPv4 address, usually in form of four numbers separated by decimal points. We’ll spend most of Module 3 discussing networking, so I won’t go into too much detail here.\nOnce you have that IP address, switch to your Windows virtual machine, and open up the Firefox web browser. At the top in the address bar, simply input the IP address and press enter. If everything works correctly, you should be presented with the default Apache screen as seen here. If not, you’ll need to do some debugging to figure out what is missing.\nWith that, you should now have all the information you need to finish the Ubuntu portion of Lab 1. If you have issues, please feel free to post in the course discussion forums or chat with me. Good luck!\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu Security \u0026 Networking",
    "uri": "/1-secure-workstations/18-ubuntu-security-networking/index.html"
  },
  {
    "content": "Here are some helpful networking diagrams to better explain the desired state before and after the end of Lab 3. In all of these diagrams, I am using a sample network with the IP address 192.168.40.0. You will have to adapt the IPs to match your network configuration. In addition, each network address will need to be updated with your unique eID as directed in the lab assignment.\nBefore Lab 3 Once you’ve completed Task 1 of Lab 3, you should have three virtual machines connected to your network. Each of them will be configured to automatically get an IP address using DHCP, and they will also be configured to use the automatically provided DNS information from that DHCP server. By default, the NAT network in VMware is configured to use VMware’s built-in DHCP server to handle automatic IP addresses, and that DHCP server also configures each system to use VMware’s built-in DNS server. That server can be found at 192.168.40.2.\nThis is very similar to how most home networks are configured, where the wireless router acts as both a DHCP server and DNS server (in actuality, a DNS forwarder, since it doesn’t really resolve DNS entries itself).\nAfter Lab 3 Once you’ve completed all of Lab 3, your network diagram should resemble the one shown above. At this point, you have installed and configured both a DNS and a DHCP server on your Ubuntu VM labelled Server, and disabled the built-in DHCP server in VMware’s NAT network. By doing so, your Ubuntu VM labelled Client as well as your Windows 10 VM will now be getting automatic IP addresses from your Ubuntu server. In addition, the DHCP server will configure those systems to use your Ubuntu server as the primary DNS server. So, whenever you try to access any addresses in the cis527\u003cyour eID\u003e.cs.ksu.edu network on those systems, they will use your Ubuntu DNS server to look up the names.\nHowever, notice that your Ubuntu VM labelled Server still uses the VMware DNS server as its primary entry. By doing so, it will not be able to look up any addresses in the cis527\u003cyour eID\u003e.cs.ksu.edu network at this time. However, you can modify its DNS settings to include a secondary entry which points back to itself (it is important to use it’s external 192.168.40.41 IP address for this, and not the localhost address 127.0.0.1). By doing so, that will allow it to query itself for DNS entries it can’t find, but will still guarantee it can access the internet even if your internal DNS server is not configured properly.\n",
    "description": "",
    "tags": null,
    "title": "Lab 3 Networking Diagrams",
    "uri": "/3-core-networking-services/19-lab3-networking/index.html"
  },
  {
    "content": "Additional helpful material for system administrators.\n",
    "description": "",
    "tags": null,
    "title": "Extras",
    "uri": "/x-extras/index.html"
  },
  {
    "content": "Information for Current Students!\nPrevious Semesters Are Here ",
    "description": "",
    "tags": null,
    "title": "Announcements",
    "uri": "/y-announcements/index.html"
  },
  {
    "content": "Helpful pages for the instructor\n",
    "description": "",
    "tags": null,
    "title": "Instructor Resources",
    "uri": "/z-instructor-resources/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "content": "Welcome This website contains the course materials for CIS 527: Enterprise Systems Administration at Kansas State University.\nAnyone and everyone is welcome to review the contents on this webpage. However, if you plan on taking the course for credit, you should enroll in the course through K-State before starting, as the course itself includes appropriate assignments and assessments to ensure that you have learned the material.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nThe source materials for this website are available on the K-State CS GitLab server.\n",
    "description": "",
    "tags": null,
    "title": "CIS 527 Textbook",
    "uri": "/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
